## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/GRID/utils/fetch_output_onfailure.sh

**Start chunk id:** a6f74e6e5c6db01691d6d21f7f7c56356a36f34e1dded967c69e0aa86c01a97b

## Content

**Question:** What does the script do if the master job ID is not provided as the first argument?

**Answer:** The script will print the message "Please provide a master job id as first argument" and exit with a status of 1 if the master job ID is not provided as the first argument.

---

**Question:** What commands are used to filter and extract subjob IDs that failed from the master job logs?

**Answer:** The commands used to filter and extract subjob IDs that failed from the master job logs are:

1. `alien.py ps -a -m ${MY_JOBID} -f ERROR_ALL` - This command lists all subjobs of the specified master job and filters for those that have an error status.

2. `awk '/EE/{print $2}'` - This `awk` command filters the output to only include lines containing "EE", which typically denotes errors, and then prints the second field, which is the subjob ID.

3. `tr '\n' ' '` - This command replaces newline characters with spaces, effectively concatenating the subjob IDs into a single string separated by spaces.

---

**Question:** What specific command-line operations does the script perform to identify and fetch error files for failed subjobs of a given AliEn GRID masterjob, and how are these operations structured within the script?

**Answer:** The script identifies and fetches error files for failed subjobs of a given AliEn GRID masterjob through a series of command-line operations structured as follows:

1. The script starts by checking if a master job id is provided as the first argument. If not, it outputs a message requesting the job id and exits.

2. It then retrieves all subjob ids associated with the master job using `alien.py ps --trace ${MY_JOBID}`. The output of this command is piped to `awk` to filter lines containing "Subjob submitted", and `sed` to extract subjob ids, which are stored in `SUBJOBIDS`.

3. The script next identifies the failed subjobs by running `alien.py ps -a -m ${MY_JOBID} -f ERROR_ALL`. It filters the output for lines starting with "EE" using `awk` and extracts the subjob ids, storing them in `FAILEDSUBJOBIDS`.

4. Finally, the script sets the current directory to `${PWD}` and creates an output directory named `/tmp/AlienLogs_${MY_JOBID}` for storing error files.

This sequence of operations systematically isolates the failed subjobs and prepares a directory for their associated error files.

---

**Question:** What is the purpose of the first `if` statement in the script?

**Answer:** The first `if` statement in the script is used to check if the variable `RecycleBase` is empty. If `RecycleBase` is not set (empty), the script proceeds to extract the base path for recycle output directories by parsing the output of the `alien.py ps --trace` command for the specified job ID. This base path is then stored in `RecycleBase` without the job ID suffix, which is essential for subsequent operations related to registering and retrieving output files.

---

**Question:** What is the purpose of the `RecycleBase` variable and how is it determined in the script?

**Answer:** The `RecycleBase` variable is used to store the base path of the recycle directory where the failed job outputs are stored, without the job-specific identifier. It is determined by parsing the output of `alien.py ps --trace ${jobid}` for the line containing "Going to uploadOutputFiles". From this line, the script extracts the output directory using `awk` and `sed` commands. Then, it removes the job ID part using parameter expansion to isolate the base path, which is stored in `RecycleBase`.

---

**Question:** What specific command is used to register output for retrieval, and how does it handle the case when there is no existing recycle base path?

**Answer:** The specific command used to register output for retrieval is `alien.py registerOutput ${jobid}`. When there is no existing recycle base path, the script first retrieves the recycle output directory from the job trace using `alien.py ps --trace ${jobid}`. It then extracts the base path by removing the job ID from the end of this directory path with the line `RecycleBase=${RecycleOutputDir%-${jobid}}`. After establishing the base path, it uses the `alien.py registerOutput` command for each job ID to register the output, suppressing any error messages with `2> /dev/null`.

---

**Question:** What is the purpose of the `sleep 1` command in the document?

**Answer:** The purpose of the `sleep 1` command is to allow some time for the "registerOutput" process to complete before proceeding with the subsequent steps.

---

**Question:** What command is used to extract log files from the archive files in the specified directory?

**Answer:** The command used to extract log files from the archive files in the specified directory is:

unzip -q -o ${OutputDir}/${jobid}/log_archive.zip -d ${OutputDir}/${jobid}

---

**Question:** What specific actions are taken to handle and extract log files from failed jobs, and how does the script ensure that logs from outside of O2DPG tasks are also considered?

**Answer:** Specific actions taken to handle and extract log files from failed jobs include:

- Downloading output files using alien.py to copy files from the recycle directory to the output directory.
- Unzipping log_archive.zip files found in each job directory to extract relevant log files.
- Automating the extraction of logs for tasks that failed, as identified in the stdout, using the extractErroredLogFiles.sh script located in O2DPG_ROOT/GRID/utils/.
- Ensuring that logs from outside of O2DPG tasks are also considered by examining the log files generated during preprocessing and other processes that might have errors, not just within the O2DPG tasks.