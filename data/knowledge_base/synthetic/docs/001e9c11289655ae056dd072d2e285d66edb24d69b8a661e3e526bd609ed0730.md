## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/analysis_testing/o2dpg_analysis_test_workflow.py

**Start chunk id:** 001e9c11289655ae056dd072d2e285d66edb24d69b8a661e3e526bd609ed0730

## Content

DOCUMENT:
    def create_ana_task(name, cmd, output_dir, *, cpu=1, mem='2000', needs=None, extraarguments="-b", is_mc=False):
    """A handy function to generate an analysis task

    This function generates an analysis task using the provided arguments.

    """

---

if input_aod does not start with "alien://":
    input_aod = abspath(input_aod)
if input_aod ends with ".txt" and does not start with "@":
    input_aod = f"@{input_aod}"

additional_workflows = []
if autoset_converters:  # This is required to run with the latest O2Physics TAG and older data
    additional_workflows = get_additional_workflows(input_aod)

data_or_mc = ANALYSIS_VALID_MC if is_mc else ANALYSIS_VALID_DATA
collision_system = get_collision_system(collision_system)

---

if not split_analyses:
    # add the combined analysis
    analysis_pipes.append(merged_analysis_pipe)
    analysis_names.append(ANALYSIS_MERGED_ANALYSIS_NAME)
    # ensure at least the resources required for one analysis
    analysis_cpu_mem.append((max(1, merged_analysis_cpu_mem[0]), max(2000, merged_analysis_cpu_mem[1])))
    merged_analysis_expected_output = list(set(merged_analysis_expected_output))
    # configuration for the merged analysis. Since it wasn't previously defined, we create it here
    analyses_config.append({'name': ANALYSIS_MERGED_ANALYSIS_NAME,
                            'valid_mc': is_mc,
                            'valid_data': not is_mc,
                            'enabled': True,
                            'tasks': merged_analysis_pipe,
                            'expected_output': merged_analysis_expected_output})

---

def add_analysis_tasks(workflow, input_aod="./AO2D.root", output_dir="./Analysis", *, analyses_only=None, is_mc=True, collision_system=None, needs=None, autoset_converters=False, include_disabled_analyses=False, timeout=None, split_analyses=False):
    """Adds default analyses to the user's workflow

    Args:
        workflow: list
            a list of tasks to which the analyses will be added
        input_aod: str
            the path to the AOD file to be analyzed
        output_dir: str
            the top-level output directory where the analysis is executed and potential results are stored
    Keyword arguments:
        analyses_only: iter (optional)
            an iterable of analysis names to be used exclusively
        is_mc: bool
            indicates if MC is expected, with data being assumed otherwise
        needs: iter (optional)
            a list of tasks that need to be completed before this one
    """

---

#
# Example for data processing:
# 1. Use o2dpg_analysis_test_workflow.py -f </path/to/AO2D.root> to create all data analysis workflows.
# 2. Use o2dpg_analysis_test_workflow.py -f </path/to/AO2D.root> --with-qc-upload --pass-name <some-pass-name> --period-name <some-period-name> to also add tasks for uploading analysis results to the CCDB.
# 3. Use o2dpg_analysis_test_workflow.py -f </path/to/AO2D.root> --only-analyses MCHistograms EventTrackQA EventSelectionQA to filter only the desired analyses; note that MCHistograms would be excluded by the script since it's not compatible with data.
#
# You can combine the above arguments as needed.
#
# For MC data, include the flag --is-mc.
#
# To execute the workflow, run:
#
# ${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json
#
# If your analyses are part of a larger workflow (e.g., one created with o2dpg_sim_workflow), you can do so.

---

parser.add_argument('--condition-not-after', dest="condition_not_after", type=int, help="only consider CCDB objects not created after this timestamp (for TimeMachine)", default=3385078236000)
parser.add_argument('--split-analyses', dest='split_analyses', action='store_true', help='Divide into individual analyses pipelines for execution.')

---

workflow.append(create_ana_task(analysis_name, analysis_pipe_assembled, output_dir, cpu=analysis_res[0], mem=analysis_res[1], needs=needs, is_mc=is_mc))

def add_analysis_qc_upload_tasks(workflow, period_name, run_number, pass_name):
    """add o2-qc-upload-root-objects to specified analysis tasks

    The analysis name only needs to be present in the workflow for these upload tasks to work. Since it doesn't require specific knowledge of the analysis, it can be added to any analysis task.

    Args:
        workflow: list
            current list of tasks
        ana_tasks_expected_outputs: list of tuples
            [(AnalysisName_1, (expected_output_1_1, expected_output_1_2, ...)), ..., (AnalysisName_N, (expected_output_N_1, expected_output_N_2, ...)) ]
    """
    analyses_to_add_for = {}
    # gather analyses in the current workflow
    for task in workflow:
        if ANALYSIS_LABEL in task["labels"]:
            analyses_to_add_for[task["name"]] = task

---

if input_aod.endswith(".root"):
    from ROOT import TFile
    if input_aod.startswith("alien://"):
        from ROOT import TGrid
        TGrid.Connect("alien")
    froot = TFile.Open(input_aod, "READ")
    # Link of tables and converters
    o2_analysis_converters = {"O2collision_001": "o2-analysis-collision-converter --doNotSwap",
                              "O2zdc_001": "o2-analysis-zdc-converter",
                              "O2bc_001": "o2-analysis-bc-converter",
                              "O2v0_002": "o2-analysis-v0converter",
                              "O2trackextra_001": "o2-analysis-tracks-extra-converter",
                              "O2ft0corrected": "o2-analysis-ft0-corrected-table"}
    for i in froot.GetListOfKeys():
        if "DF_" not in i.GetName():
            continue
        df_dir = froot.Get(i.GetName())
        # print(i)
        for j in df_dir.GetListOfKeys():

---

# we must establish the output directory for the final configuration files
output_dir_config = join(output_dir, 'config')
if output_dir_config does not exist:
    makedirs(output_dir_config)

# save the analysis configuration here
with open(join(output_dir, 'analyses_config.json'), 'w') as f:
    json.dump({'analyses': analyses_config}, f, indent=2)

configuration = adjust_and_get_configuration_path(data_or_mc, collision_system, output_dir_config)

---

#
# Functionality of Analysis Task
#
# --> Integrating analysis tasks into an existing workflow <--
#
# Through another script, one can invoke (for instance, from o2dpg_sim_workflow) the command: add_analysis_tasks(workflow["stages"], needs=[AOD_merge_task["name"]], is_mc=True)
#
# --> Generating a standalone workflow file focused solely on analyses <--
#
# Usage message:
# usage: o2dpg_analysis_test_workflow.py [-h] -f INPUT_FILE [-a ANALYSIS_DIR] [-o OUTPUT] [--is-mc] [--with-qc-upload] [--run-number RUN_NUMBER] [--pass-name PASS_NAME] [--period-name PERIOD_NAME] [--config CONFIG] [--only-analyses [ONLY_ANALYSES [ONLY_ANALYSES ...]]]
#
# Function to create analysis test workflow
#
# Optional arguments:
#   -h, --help            display this help message and exit
#   -f INPUT_FILE, --input-file INPUT_FILE
#                         complete path to the AO2D input
#   -a ANALYSIS_DIR, --analysis-dir ANALYSIS_DIR
#                         the directory for analysis output and work
#   -o OUTPUT, --output OUTPUT

---

def run(args):
    """processing input from the command line"""
    if args.with_qc_upload and (not args.pass_name or not args.period_name):
        print("ERROR: QC upload requested, but --pass-name and --period-name are needed in this case")
        return 1

---

DOCUMENT:
    merged_analysis_pipe.extend(ana['tasks'])
        # Underestimate the resource requirements of a single analysis in the merged scenario.
        # Merging all tasks into one large pipeline does not proportionally scale resources!
        merged_analysis_cpu_mem[0] += 0.5
        merged_analysis_cpu_mem[1] += 700
        merged_analysis_expected_output.extend(ana['expected_output'])

---

parser.set_defaults(func=execute)
arguments = parser.parse_args()
return(arguments.func(arguments))

if __name__ == "__main__":
    sys.exit(run())

---

#                         managing the analysis output and directory structure
#   -o OUTPUT, --output OUTPUT
#                         specify the name of the workflow file
#   --is-mc               indicates if the input is from MC (assumed data otherwise)
#   --with-qc-upload
#   --run-number RUN_NUMBER
#                         enter the run number
#   --pass-name PASS_NAME
#                         define the pass name
#   --period-name PERIOD_NAME
#                         production tag
#   --config CONFIG       override the default configuration JSON by providing the path </path/to/file>, which will be automatically adjusted to json://
#   --only-analyses [ONLY_ANALYSES [ONLY_ANALYSES ...]]
#                         limit the analyses to these specific ones
#
# The -f/--input-file argument is mandatory in both MC and data scenarios
# If --with-upload-qc is enabled, --period-name must also be provided; for data, --pass-name is additionally required, while for MC it is set to passMC
#
# Example for data

---

# 1. ${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json --target-labels Analysis
# to execute all tasks marked with "Analysis" (which effectively covers all analyses)
# 2. ${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json -tt Analysis_<ana_name>
# to run just this particular analysis
#
import sys
import importlib.util
import argparse
from os import environ, makedirs
from os.path import join, exists, abspath, expanduser
import json

---

# ensure O2DPG and O2 are loaded
O2DPG_ROOT=environ.get('O2DPG_ROOT')

if O2DPG_ROOT is None:
    print('ERROR: O2DPG must be loaded')
    sys.exit(1)

# dynamically load needed utilities
module_name = "o2dpg_workflow_utils"
spec = importlib.util.spec_from_file_location(module_name, join(O2DPG_ROOT, "MC", "bin", "o2dpg_workflow_utils.py"))
o2dpg_workflow_utils = importlib.util.module_from_spec(spec)
sys.modules[module_name] = o2dpg_workflow_utils
spec.loader.exec_module(o2dpg_workflow_utils)
from o2dpg_workflow_utils import createTask, dump_workflow, createGlobalInitTask

module_name = "o2dpg_analysis_test_utils"
spec = importlib.util.spec_from_file_location(module_name, join(O2DPG_ROOT, "MC", "analysis_testing", "o2dpg_analysis_test_utils.py"))
o2dpg_analysis_test_utils = importlib.util.module_from_spec(spec)
sys.modules[module_name] = o2dpg_analysis_test_utils
spec.loader.exec_module(o2dpg_analysis_test_utils)
from o2dpg_analysis_test_utils import *

---

# collection of lists, each sub-list represents one analysis pipeline to be run
analysis_pipes = []
# gather names for each analysis pipeline
analysis_names = []
# specify CPU and memory for each task
analysis_cpu_mem = []
# compile all tasks into a single list
merged_analysis_pipe = additional_workflows.copy()
# determine CPU and memory requirements for merged analyses
merged_analysis_cpu_mem = [0, 0]
# define the expected output from merged analyses
merged_analysis_expected_output = []
# prepare configurations for analyses to be written
analyses_config = []

---

#!/usr/bin/env python3

BEGIN DOCUMENTATION:
    This is a Python script designed for use with the ALICE O2 simulation.
END DOCUMENTATION

---

# add upload task for each expected output file
task = createTask(name=f"{ANALYSIS_LABEL}_finalize_{ana_name_raw}_{rename_output}", cwd=cwd, lab=[f"{ANALYSIS_LABEL}Upload", ana_name_raw], cpu=1, mem='2000', needs=needs)
# Once the file is renamed for upload purposes, it should be renamed back to its original name afterward, as there's no need to rename it on disk just for a specific task. 
rename_cmd = f"mv {eo} {rename_output}"
rename_back_cmd = f"mv {rename_output} {eo}"
task["cmd"] = f"{rename_cmd} && o2-qc-upload-root-objects --input-file ./{rename_output} --qcdb-url ccdb-test.cern.ch:8080 --task-name Analysis{ana_name_raw} --detector-code AOD --provenance {provenance} --pass-name {pass_name} --period-name {period_name} --run-number {run_number} && {rename_back_cmd} "
workflow.append(task)

---

This function generates an analysis task based on given parameters.

Args:
    name: str
        the desired analysis name
    cmd: str
        the command line to execute
    output_dir: str
        the directory for analysis output and work

Keyword Args (optional):
    needs: tuple, list
        a list of tasks that need to be completed before this one
    extraarguments: str
        additional O2/DPL arguments to be included in the command

Return:
    dict: the task dictionary
    """
    # if another workflow wants to use it externally, allow dependencies to be added before running analyses
    if needs is None:
        # set to an empty list
        needs = []
    task = createTask(name=full_ana_name(name), cwd=join(output_dir, name), lab=[ANALYSIS_LABEL, name], cpu=cpu, mem=mem, needs=needs)
    if is_mc:
        task["labels"].append(ANALYSIS_LABEL_ON_MC)
    task['cmd'] = f"{cmd} {extraarguments}"
    return task

---

DOCUMENT:
    # print(i)
    for j in df_dir.GetListOfKeys():
        # print(j)
        if j.GetName() in o2_analysis_converters:
            o2_analysis_converters.pop(j.GetName())
    for i in o2_analysis_converters:
        additional_workflows.append(o2_analysis_converters[i])
    return additional_workflows

---

### setup global environment variables that apply to all tasks, set as the initial task
global_env = {"ALICEO2_CCDB_CONDITION_NOT_AFTER": args.condition_not_after} if args.condition_not_after else None
workflow = [createGlobalInitTask(global_env)]
add_analysis_tasks(workflow, args.input_file, expanduser(args.analysis_dir), is_mc=args.is_mc, analyses_only=args.only_analyses, autoset_converters=args.autoset_converters, include_disabled_analyses=args.include_disabled, timeout=args.timeout, collision_system=args.collision_system, split_analyses=args.split_analyses)
if args.with_qc_upload:
    add_analysis_qc_upload_tasks(workflow, args.period_name, args.run_number, args.pass_name)
if not workflow:
    print("WARNING: No tasks were added")
dump_workflow(workflow, args.output)
print("To execute the workflow, you can run: `${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f {args.output}`")
return 0

---

def main():
    """starting point for the script when executed directly from the command line"""
    parser = argparse.ArgumentParser(description='Set up the analysis test workflow')
    parser.add_argument("-f", "--input-file", dest="input_file", default="./AO2D.root", help="full path to the AO2D input", required=True)
    parser.add_argument("-a", "--analysis-dir", dest="analysis_dir", default="./Analysis", help="directory for analysis output and work")
    parser.add_argument("-o", "--output", default="workflow_analysis_test.json", help="name of the workflow file")
    parser.add_argument("--is-mc", dest="is_mc", action="store_true", help="indicates if the input is from MC, data is assumed otherwise")
    parser.add_argument("--with-qc-upload", dest="with_qc_upload", action="store_true")
    parser.add_argument("--run-number", dest="run_number", type=int, default=300000, help="the specified run number")
    parser.add_argument("--pass-name", dest="pass_name", help="name of the pass")

---

def get_additional_workflows(input_aod):
    additional_workflows = []

    # Handle the case where the input is a text file; use the first line in that scenario
    if input_aod.endswith(".txt"):
        if input_aod.startswith("@"):
            input_aod = input_aod[1:]
        with open(input_aod) as f:
            input_aod = f.readline().strip('\n')

---

FOR analysis_name, analysis_pipe, analysis_res IN zip(analysis_names, analysis_pipes, analysis_cpu_mem):
    # eliminate any duplicates in the pipe, particularly in the merged scenario
    analysis_pipe = list(set(analysis_pipe))
    analysis_pipe_assembled = []
    for executable_string in analysis_pipe:
        # the input executable may already have configurations, but the first token is the executable itself
        executable_string += f' --configuration json://{configuration}'
        analysis_pipe_assembled.append(executable_string)

    # compile the pipe, including AOD and timeout if necessary
    analysis_pipe_assembled = ' | '.join(analysis_pipe_assembled)
    analysis_pipe_assembled += f' --aod-file {input_aod} --shm-segment-size 3000000000 --readers 1 --aod-memory-rate-limit 500000000'
    if timeout is not None:
        analysis_pipe_assembled += f' --time-limit {timeout}'

---

for each_analysis in load_analyses(analyses_only, include_disabled_analyses=include_disabled_analyses):
    if is_mc and not each_analysis.get("valid_mc", False):
        print(f"INFO: Analysis {each_analysis['name']} not included as it is not valid in MC")
        continue
    if not is_mc and not each_analysis.get("valid_data", False):
        print(f"INFO: Analysis {each_analysis['name']} not included as it is not valid in data")
        continue
    if analyses_only and each_analysis['name'] not in analyses_only:
        # filter based on requested analyses
        continue

    if split_analyses:
        # only individual analyses, not merged ones
        analysis_pipes.append(each_analysis['tasks'])
        analysis_names.append(each_analysis['name'])
        analysis_cpu_mem.append((1, 2000))
        analyses_config.append(each_analysis)
        continue

---

FOR EACH ANALYSIS IN load_analyses(include_disabled_analyses=True):
    IF NOT ana["expected_output"]:
        CONTINUE
    ana_name_raw = ana["name"]
    ana_name = full_ana_name(ana_name_raw)
    IF ana_name NOT IN analyses_to_add_for:
        CONTINUE
    # check workflow stages for the requested analysis
    pot_ana = analyses_to_add_for[ana_name]
    cwd = pot_ana["cwd"]
    needs = [ana_name]
    provenance = "qc_mc" IF ANALYSIS_LABEL_ON_MC IN pot_ana["labels"] ELSE "qc"
    FOR eo IN ana["expected_output"]:
        # this step seems redundant but is included for backward compatibility
        rename_output = eo.strip(".root")
        rename_output = f"{rename_output}_{ana_name_raw}.root"
        # add upload task for each expected output file

---

def get_additional_workflows(input_aod):
    additional_workflows = []
    for wf in input_aod:
        if not wf.get("enabled", False) and not include_disabled_analyses:
            print(f"INFO: Workflow {wf['name']} not added since it is disabled")
            continue
        additional_workflows.append(wf)

    return additional_workflows

---

parser.add_argument("--pass-name", dest="pass_name", help="specify the pass name")
parser.add_argument("--period-name", dest="period_name", help="specify the period name")
parser.add_argument("--only-analyses", dest="only_analyses", nargs="*", help="filter analyses to only these")
parser.add_argument("--include-disabled", dest="include_disabled", action="store_true", help="run disabled analyses despite their status")
parser.add_argument("--autoset-converters", dest="autoset_converters", action="store_true", help="compatibility mode to automatically configure converters for analyses")
parser.add_argument("--timeout", type=int, default=None, help="set timeout for analysis tasks in seconds")
parser.add_argument("--collision-system", dest="collision_system", help="define the collision system. If not provided, it is inferred from ALIEN_JDL_LPMInterationType. Defaults to pp if not set")