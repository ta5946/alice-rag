## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/RelVal/o2dpg_release_validation.py

**Start chunk id:** 053338dae82395bd16565bcd8f20a896eef5d4cf73d5a5b1c746670df0398322

## Content

DOCUMENT:
    if not exists(output):
        makedirs(output)

    json_out = f"{prefix}_extracted.json" if prefix else "extracted.json"
    root_out = f"{prefix}_extracted.root" if prefix else "extracted.root"
    json_out = abspath(join(output, json_out))
    root_out = abspath(join(output, root_out))

    if not extract_and_flatten_impl(files, root_out, include_file_directories=include_directories, add_if_exists=add_if_exists, reference_extracted=reference_extracted, json_extracted=json_out):
        return None, None

    data = None
    with open(json_out, "r") as f:
        data = json.load(f)

    data["label"] = label

    with open(json_out, "w") as f:
        json.dump(data, f, indent=2)

    return json_out, data


def rel_val_root(d1, d2, metrics_enabled, metrics_disabled, output_dir):
    """
    RelVal for 2 ROOT files, a wrapper around the ReleaseValidation.C macro

    """

---

DOCUMENT:
    utils.print_summary(rel_val, variables.REL_VAL_SEVERITIES, long=args.print_long)

    if not args.no_plot:
        print("Proceeding to plot...")
        # create multiple figures for the user to review
        plot_pie_charts(rel_val, variables.REL_VAL_SEVERITIES, variables.REL_VAL_SEVERITY_COLOR_MAP, args.output)
        plot_compare_summaries((rel_val,), args.output)
        plot_summary_grid(rel_val, variables.REL_VAL_SEVERITIES, variables.REL_VAL_SEVERITY_COLOR_MAP, args.output)
        plot_value_histograms(rel_val, args.output)

        if is_inspect:
            if annotations_inspect := rel_val.annotations:
                annotations_inspect = annotations_inspect[0]
                dict_1 = load_from_meta_json(annotations_inspect["json_path_1"])
                dict_2 = load_from_meta_json(annotations_inspect["json_path_2"])

---

# common parser for handling metric options
COMMON_METRIC_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_METRIC_PARSER.add_argument("--enable-metric", dest="enable_metric", nargs="*")
COMMON_METRIC_PARSER.add_argument("--disable-metric", dest="disable_metric", nargs="*")

# common parser for managing object name patterns
COMMON_PATTERN_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_PATTERN_PARSER.add_argument("--include-patterns", dest="include_patterns", nargs="*", help="include objects that match at least one of the provided patterns (has higher priority)")
COMMON_PATTERN_PARSER.add_argument("--exclude-patterns", dest="exclude_patterns", nargs="*", help="exclude objects that match at least one of the provided patterns")

---

# common parser for handling input files
COMMON_FILE_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_FILE_PARSER.add_argument("-i", "--input1", nargs="*", help="EITHER a list of first input files for comparison OR the first input directory from the simulation for comparison", required=True)
COMMON_FILE_PARSER.add_argument("-j", "--input2", nargs="*", help="EITHER a list of second input files for comparison OR the second input directory from the simulation for comparison", required=True)
COMMON_FILE_PARSER.add_argument("--labels", nargs=2, help="labels to be used in the plot legends for overlay plots from inputs -i and -j, defaulting to 'batch_i' and 'batch_j'", default=("batch_i", "batch_j"))
COMMON_FILE_PARSER.add_argument("--no-extract", dest="no_extract", action="store_true", help="skip extraction and assume histograms are already present for comparison")

---

# each extraction will result in a JSON
    json_path_1, dict_1 = extract_and_flatten(args.input1, args.output, args.labels[0], args.include_dirs, args.add, prefix="1", reference_extracted=None)
    if json_path_1 is None:
        return 1
    json_path_2, dict_2 = extract_and_flatten(args.input2, args.output, args.labels[1], args.include_dirs, args.add, prefix="2", reference_extracted=dict_1["path"])
    if json_path_2 is None:
        return 1
    json_path = rel_val_root(dict_1, dict_2, args.enable_metric, args.disable_metric, args.output)
    if json_path is None:
        print("ERROR: Problem during RelVal")
        return 1
    annotations = {"json_path_1": json_path_1,
                   "json_path_2": json_path_2}

# subsequently, we load and create a RelVal object
rel_val = load_rel_val(json_path, include_patterns, exclude_patterns, args.enable_metric, args.disable_metric)

---

DOCUMENT:
    print("==> Process and compare 2 sets of files <==")

    file_1 = d1["path"]
    file_2 = d2["path"]

    # RelVal on flattened files
    metrics_enabled = ";".join(metrics_enabled) if metrics_enabled else ""
    metrics_disabled = ";".join(metrics_disabled) if metrics_disabled else ""

    command = f"\\\"{file_1}\\\",\\\"{file_2}\\\",\\\"{metrics_enabled}\\\",\\\"{metrics_disabled}\\\""
    command = f"root -l -b -q {ROOT_MACRO_RELVAL}{command}"
    output_directory = abspath(output_directory)
    log_file_rel_val = join(output_directory, "rel_val.log")
    print("Executing RelVal on extracted objects")
    result = utils.run_macro(command, log_file_rel_val, cwd=output_directory)

    # This is generated by the ROOT macro
    json_path = join(output_directory, "RelVal.json")

---

# identify the test and metric names that are shared between the two sets
test_names = np.intersect1d(rel_val1.known_test_names, rel_val2.known_test_names)
metric_names = np.intersect1d(rel_val1.known_metrics, rel_val2.known_metrics)

print("METRIC NAME, TEST NAME, INTERPRETATION, #IN COMMON, #ONLY IN FIRST, #ONLY IN SECOND")
for metric_name in metric_names:
    for test_name in test_names:
        object_names1, results1 = rel_val1.get_result_per_metric_and_test(metric_name, test_name)
        object_names2, results2 = rel_val2.get_result_per_metric_and_test(metric_name, test_name)

---

# set up for testing with mean and standard deviation
if regions_paths:
    regions = utils.get_paths_or_from_file(regions_paths)
    rel_val_regions = utils.RelVal()
    rel_val_regions.load(regions)
    utils.initialise_regions(evaluator, rel_val_regions)

# configure for threshold testing
thresholds_default = {metric_name: float(value) for metric_name, value in thresholds_default} if thresholds_default else None
rel_val_thresholds = None
if thresholds_paths:
    thresholds_margins = {metric_name: float(value) for metric_name, value in thresholds_margins} if thresholds_margins else None
    thresholds_paths = utils.get_paths_or_from_file(thresholds_paths)
    rel_val_thresholds = utils.RelVal()
    rel_val_thresholds.load(thresholds_paths)
utils.initialise_thresholds(evaluator, rel_val, rel_val_thresholds, thresholds_default, thresholds_margins, thresholds_combine)

evaluator.initialise()
return evaluator

---

DOCUMENT:
    Args:
        files1: iterable
            the first set of files to be compared
        files2: iterable
            the second set of files to be compared
        add_to_previous: bool
            determines whether extracted objects should be added to any existing objects in the file (if they exist)
        metrics_enabled: iterable or None
            names of metrics to be activated
        metrics_disabled: iterable or None
            names of metrics to be deactivated
        label1, label2: str
            labels for the overlay plots
        output_dir: str
            path to the output directory; the directory will be created if it does not already exist
        no_extract: bool
            if True, assumes files1 and files2 each contain a single file with pre-extracted objects. Extraction will be skipped, and the objects from these files will be directly compared.

---

# inspect
INSPECT_PARSER = SUB_PARSERS.add_parser("inspect", parents=[COMMON_THRESHOLD_PARSER, COMMON_METRIC_PARSER, COMMON_PATTERN_PARSER, COMMON_FLAGS_PARSER, COMMON_VERBOSITY_PARSER])
INSPECT_PARSER.add_argument("--path", dest="json_path", help="either the full file path to a Summary.json or a directory where such a file is anticipated to be located", required=True)
INSPECT_PARSER.add_argument("--output", "-o", help="the output directory", default="rel_val_inspect")
INSPECT_PARSER.set_defaults(func=rel_val)

---

#!/usr/bin/env python3
#
# This script enables users to compare:
# 1. Two corresponding ROOT files that contain histograms or QC Monitoring objects
# 2. Two corresponding simulation directories
#
# The RelVal suite can be executed using:
# o2dpg_release_validation.py rel-val -i <file-or-sim-dir1> -j <file-or-sim-dir2>
#

import sys
import argparse
import importlib.util
from os import environ, makedirs, remove
from os.path import join, abspath, exists, dirname, basename, isfile
import json

import numpy as np

# Ensure that O2DPG and O2 are loaded
O2DPG_ROOT=environ.get('O2DPG_ROOT')

if O2DPG_ROOT is None:
    print('ERROR: This requires O2DPG to be loaded')
    sys.exit(1)

---

# Utilize multiple sub-parsers
SUB_PARSERS = PARSER.add_subparsers(dest="command")

# rel-val
REL_VAL_PARSER = SUB_PARSERS.add_parser("rel-val", parents=[COMMON_FILE_PARSER, COMMON_METRIC_PARSER, COMMON_THRESHOLD_PARSER, COMMON_FLAGS_PARSER, COMMON_VERBOSITY_PARSER])
REL_VAL_PARSER.add_argument("--include-dirs", dest="include_dirs", nargs="*", help="restrict inclusion to specific directories within the ROOT file; each pattern is considered to start from the top-level directory (currently, no regex or wildcards are supported)")
REL_VAL_PARSER.add_argument("--add", action="store_true", help="when this flag is set and a RelVal already exists in the output directory, additional objects will be appended to the existing ones")
REL_VAL_PARSER.add_argument("--output", "-o", help="specify the output directory", default="rel_val")
REL_VAL_PARSER.set_defaults(func=rel_val)

---

if not exists(args.output):
    makedirs(args.output)

need_apply = False
is_inspect = False
dict_1 = None
dict_2 = None
if hasattr(args, "json_path"):
    # this is from the inspect command
    is_inspect = True
    json_path = utils.get_summary_path(args.json_path)
    annotations = None
    include_patterns, exclude_patterns = (args.include_patterns, args.exclude_patterns)
else:
    # here, new input ROOT files are provided, requiring all tests to be applied
    need_apply = True
    # include everything by default
    include_patterns, exclude_patterns = (None, None)
    if args.add:
        print(f"NOTE: Extracted objects will be added to existing ones if a RelVal already exists at {args.output}.\n")

---

```python
def initialise_evaluator(rel_val, thresholds_paths, thresholds_default, thresholds_margins, thresholds_combine, regions_paths):
    """
    Wrapper function to create an evaluator

    """
    rel_val.set_object_name_patterns(include_patterns, exclude_patterns)
    rel_val.enable_metrics(enable_metrics)
    rel_val.disable_metrics(disable_metrics)
    rel_val.load((json_path,))
    return rel_val
```

---

# always the same
row_tags = table_name + tags_out

with open(output_path, "w") as f:
    object_names, metric_names, result_names, results = rel_val.query_results()
    for i, (object_name, metric_name, result_name, result) in enumerate(zip(object_names, metric_names, result_names, results)):
        common_string = f"{row_tags},id={i},histogram_name={object_name},metric_name={metric_name},test_name={result_name} status={variables.REL_VAL_SEVERITY_MAP[result.interpretation]}"
        if result.value is not None:
            common_string += f",value={result.value}"
        if result.mean is not None:
            common_string += f",threshold={result.mean}"
        f.write(f"{common_string}\n")
return 0


def print_simple(args):
    """
    Simply prints line-by-line

    object names (--object-names)

    metric names (--metric-names)

    test names (--test-names)
    """

---

DOCUMENT:
    # Derived from the ROOT macro
    json_path = join(output_dir, "RelVal.json")

    if not exists(json_path) or ret > 0:
        # An error occurred
        print(f"ERROR: An issue occurred while calculating metrics, the log file at {log_file_rel_val} states")
        with open(log_file_rel_val, "r") as f:
            print(f.read())
        return None

    return json_path


def load_rel_val(json_path, include_patterns=None, exclude_patterns=None, enable_metrics=None, disable_metrics=None):
    """
    A wrapper to generate RelVal and configure certain properties

    """

---

# extract
EXTRACT_PARSER = SUB_PARSERS.add_parser("extract", parents=[COMMON_VERBOSITY_PARSER])
EXTRACT_PARSER.add_argument("--input", nargs="*", help="Set of input files to be processed", required=True)
EXTRACT_PARSER.add_argument("--output", "-o", help="output directory for the results", default="rel_val_extracted")
EXTRACT_PARSER.add_argument("--prefix", "-p", help="prefix to add to the output files")
EXTRACT_PARSER.add_argument("--label", "-l", help="label to be included", required=True)
EXTRACT_PARSER.add_argument("--reference", "-r", help="path to a reference file for extraction (useful to maintain consistent binning when TTrees are processed)")
EXTRACT_PARSER.set_defaults(func=only_extract)


def main():
    """entry point when the script is run directly from the command line"""
    args = PARSER.parse_args()
    if args.command != "print":
        print_header()
    return(args.func(args))

if __name__ == "__main__":
    sys.exit(main())

---

FOR EACH INTERPRETATION IN variables.REL_VAL_SEVERITIES:
    IF args.interpretations AND THE INTERPRETATION IS NOT IN args.interpretations, CONTINUE
    # OBJECT NAMES OF RESULTS MATCHING THE CURRENT INTERPRETATION
    object_names_interpretation1 = object_names1[utils.count_interpretations(results1, interpretation)]
    object_names_interpretation2 = object_names2[utils.count_interpretations(results2, interpretation)]
    # OBJECTS ONLY IN THE FIRST SET THAT ARE NOT IN THE SECOND SET
    only_in1 = np.setdiff1d(object_names_interpretation1, object_names_interpretation2)
    # OBJECTS ONLY IN THE SECOND SET THAT ARE NOT IN THE FIRST SET
    only_in2 = np.setdiff1d(object_names_interpretation2, object_names_interpretation1)
    # OBJECTS COMMON TO BOTH SETS
    in_common = np.intersect1d(object_names_interpretation1, object_names_interpretation2)

---

This can be accessed via the rel-val or inspect commands.
"""
def interpret_result(outcome, measure):
    """
    This function evaluates a result based on the associated metric and assigns an interpretation.
    """
    is_critical = not args.is_critical or measure.name in args.is_critical
    if not measure.comparable and is_critical:
        outcome.interpretation = variables.REL_VAL_INTERPRETATION_CRIT_NC
        return
    if not measure.comparable:
        outcome.interpretation = variables.REL_VAL_INTERPRETATION_NONCRIT_NC
        return
    if outcome.result_flag == utils.Result.FLAG_UNKNOWN:
        outcome.interpretation = variables.REL_VAL_INTERPRETATION_UNKNOWN
        return
    if outcome.result_flag == utils.Result.FLAG_PASSED:
        outcome.interpretation = variables.REL_VAL_INTERPRETATION_GOOD
        return
    if outcome.result_flag == utils.Result.FLAG_FAILED and is_critical:

---

# Generate an InfluxDB metrics file based on the provided arguments
if args.influx:
    rel_val = load_rel_val(utils.get_summary_path(args.path))
    create_influx_metrics_file(rel_val, args.path)

return 0

---

DOCUMENT:
    evaluator.initialise()
    return evaluator

###################################################################
# Functions Triggered Post Command Line Processing #
###################################################################


def only_extract(args):
    if not extract_and_flatten(args.input, args.output, None, args.label, args.reference)[0]:
        # checking one of the return values for None
        return 1
    return 0


def rel_val(args):
    """
    Entry point for ReleaseValidation

    """

---

DOCUMENT:
    if dict_1 and dict_2:
        overlay_plots_out = join(args.output, "overlayPlots")
        if not exists(overlay_plots_out):
            makedirs(overlay_plots_out)
        plot_overlays(rel_val, dict_1, dict_2, overlay_plots_out)

    return 0


def compare(args):
    """
    Compare two RelVal outputs against each other
    """
    if len(args.input1) > 1 or len(args.input2) > 1:
        print("ERROR: Currently, you can only compare one RelVal output with one other RelVal output.")
        return 1

    output_dir = args.output

    # load
    rel_val1 = load_rel_val(utils.get_summary_path(args.input1[0]), args.include_patterns, args.exclude_patterns, args.enable_metric, args.disable_metric)
    rel_val2 = load_rel_val(utils.get_summary_path(args.input2[0]), args.include_patterns, args.exclude_patterns, args.enable_metric, args.disable_metric)

---

# influx
INFLUX_PARSER = SUB_PARSERS.add_parser("influx")
INFLUX_PARSER.add_argument("--path", help="directory where the ReleaseValidation process was executed", required=True)
INFLUX_PARSER.add_argument("--tags", nargs="*", help="tags to include for influx, formatted as key=value pairs")
INFLUX_PARSER.add_argument("--table-suffix", dest="table_suffix", help="prefix for the table name")
INFLUX_PARSER.add_argument("--output", "-o", help="output directory; if not specified, a file named influxDB.dat is saved within the RelVal directory")
INFLUX_PARSER.set_defaults(func=influx)

---

DOCUMENT:
    output_path = args.path if the file exists at args.path else join(args.path, "influxDB.dat")
    table_name = "O2DPG_MC_ReleaseValidation"
    if args.table_suffix:
        table_name = f"{table_name}_{args.table_suffix}"
    tags_out = ""
    if args.tags:
        for t in args.tags:
            t_split = t.split("=")
            if len(t_split) != 2 or not t_split[0] or not t_split[1]:
                print(f"ERROR: Invalid format of tags {t} for InfluxDB")
                return 1
            # we dissect and reassemble it to ensure no extra spaces or other characters are present
            tags_out += f",{t_split[0].strip()}={t_split[1].strip()}"

    # always consistent
    row_tags = table_name + tags_out

---

DOCUMENT:
    if O2DPG_ROOT is None:
    print('ERROR: This requires O2DPG to be loaded')
    sys.exit(1)


O2DPG_ROOT = environ.get("O2DPG_ROOT")
spec = importlib.util.spec_from_file_location("o2dpg_release_validation_variables", join(O2DPG_ROOT, "RelVal", "utils", 'o2dpg_release_validation_variables.py'))
o2dpg_release_validation_variables = importlib.util.module_from_spec(spec)
spec.loader.exec_module(o2dpg_release_validation_variables)
sys.modules["o2dpg_release_validation_variables"] = o2dpg_release_validation_variables
import o2dpg_release_validation_variables as variables

spec = importlib.util.spec_from_file_location("o2dpg_release_validation_utils", join(O2DPG_ROOT, "RelVal", "utils", 'o2dpg_release_validation_utils.py'))
o2dpg_release_validation_utils = importlib.util.module_from_spec(spec)
spec.loader.exec_module(o2dpg_release_validation_utils)
sys.modules["o2dpg_release_validation_utils"] = o2dpg_release_validation_utils
import o2dpg_release_validation_utils as utils

---

ARGS:
    input_filenames: list
        a list of input filenames from which objects will be extracted
    target_filename: str
        the path to the file where the extracted objects will be saved
    include_file_directories: iterable or ""
        only include ROOT sub-directories that contain any of the specified strings
    add_if_exists: bool (default: False)
        if the target file already exists, update it instead of recreating it
    reference_extracted: str
        used for extracting TTrees, where the x-axis binning will be set according to this reference to ensure comparability
    json_extracted: str
        the path where the JSON file containing the details of "what has been extracted where" will be saved

---

DOCUMENT:
    print("Extraction of files")

    for f in input_filenames:
        f = abspath(f)
        print(f"  {f}")
        cmd = f"\\\"{f}\\\",\\\"{target_filename}\\\",\\\"{reference_extracted}\\\",\\\"{include_file_directories}\\\",\\\"{json_extracted}\\\""
        cmd = f"root -l -b -q {ROOT_MACRO_EXTRACT}{cmd}"
        ret = utils.run_macro(cmd, log_file_name, cwd)
        if ret != 0:
            print(f"ERROR: Extracting from file {f} failed. Please check logfile {abspath(join(cwd, log_file_name))}")
            return False

    return True


def extract_and_flatten(files, output, label, include_directories=None, add_if_exists=False, prefix=None, reference_extracted=""):
    """
    Extract from input files to a flat ROOT file

    Returns the path to a meta JSON and that JSON file loaded as a dictionary
    """

---

DOCUMENT:
    metric names (--metric-names)

    test names (--test-names)
    """

    if not args.path:
        if not args.metric_names:
            return 0
        return metrics_from_root()

    rel_val = load_rel_val(utils.get_summary_path(args.path), args.include_patterns, args.exclude_patterns, args.enable_metric, args.disable_metric)

    def filter_on_interpretations(result):
        # consider only results that match a user-requested flag
        return not args.interpretations or result.interpretation in args.interpretations

---

if the number of input_filenames is one and the first character of the first filename is "@":
    # assuming that this single filename contains the paths of the actual files to be extracted
    read_files_from = input_filenames[0][1:]
    input_filenames = get_files_from_list(read_files_from)
    if input_filenames is empty:
        print(f"ERROR: It seems that {read_files_from} does not contain any files to be extracted.")
        return False

if the target_filename exists and add_if_exists is False:
    # the file will be updated if it exists
    remove(target_filename)

# the ROOT macro is executed in the current working directory (cwd) and places the basename there
current_working_directory = dirname(target_filename)
target_filename = basename(target_filename)
log_file_name = join(current_working_directory, f"{target_filename}_extract_and_flatten.log")

print("Starting file extraction")

---

# common parser for interpreting options
COMMON_FLAGS_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_FLAGS_PARSER.add_argument("--interpretations", nargs="*", help="extract objects with at least one test having this severity flag", choices=list(variables.REL_VAL_SEVERITY_MAP.keys()))
COMMON_FLAGS_PARSER.add_argument("--is-critical", dest="is_critical", nargs="*", help="designate metrics as critical")

# common parser for managing verbosity
COMMON_VERBOSITY_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_VERBOSITY_PARSER.add_argument("--print-long", dest="print_long", action="store_true", help="increase verbosity")
COMMON_VERBOSITY_PARSER.add_argument("--no-plot", dest="no_plot", action="store_true", help="disable plotting")

# The primary parser
PARSER = argparse.ArgumentParser(description='Wrapper for ReleaseValidation macro')

# Utilize different sub-parsers
SUB_PARSERS = PARSER.add_subparsers(dest="command")

---

DOCUMENT:
    with open(json_path, "r") as f:
        try:
            return json.load(f)
        except (json.decoder.JSONDecodeError, UnicodeDecodeError):
            pass
    return None


def extract_and_flatten_impl(input_filenames, target_filename, include_file_directories=None, add_if_exists=False, reference_extracted="", json_extracted=""):
    """
    Handle the extraction of objects for comparison

    These objects will be extracted (from TH1, QC objects, TTree etc.), converted to TH1, and placed into a flat ROOT file structure.

    """

---

```python
def load_from_meta_json(json_path):
    """
    Load a meta JSON file and return a dictionary
    """
    if not exists(json_path):
        return None

    with open(json_path, "r") as f:
        import json
        data = json.load(f)
    return data
```

---

DOCUMENT:
    rel_val.filter_results(filter_on_interpretations)
    if args.metric_names:
        for metric_name in rel_val.known_metrics:
            print(metric_name)
    if args.test_names and rel_val.number_of_tests:
        for test_name in rel_val.known_test_names:
            print(test_name)
    if args.object_names:
        if rel_val.number_of_tests:
            # we have tests, so we retrieve object names with interpretations
            object_names, _ = rel_val.get_result_per_metric_and_test()
        else:
            object_names = rel_val.known_objects
        for object_name in np.unique(object_names):
            print(object_name)
    return 0


def print_header():
    print(f"\n{'#' * 25}\n#{' ' * 23}#\n# RUN ReleaseValidation #\n#{' ' * 23}#\n{'#' * 25}\n")


################################################################
# define the parser globally so that it can even be imported    #
################################################################

---

# common parser settings for handling thresholds
COMMON_THRESHOLD_PARSER = argparse.ArgumentParser(add_help=False)
COMMON_THRESHOLD_PARSER.add_argument("--regions", help="Apply calculated regions for status testing")
COMMON_THRESHOLD_PARSER.add_argument("--default-threshold", dest="default_threshold", action="append", nargs=2)
COMMON_THRESHOLD_PARSER.add_argument("--use-values-as-thresholds", nargs="*", dest="use_values_as_thresholds", help="Utilize values from another run as thresholds for the current run")
COMMON_THRESHOLD_PARSER.add_argument("--combine-thresholds", dest="combine_thresholds", choices=["mean", "extreme"], help="Select arithmetic mean or extreme value as the threshold", default="mean")
COMMON_THRESHOLD_PARSER.add_argument("--margin-threshold", dest="margin_threshold", action="append", nargs=2)

---

if need_apply or args.use_values_as_thresholds or args.default_threshold or args.regions:
    evaluator = initialise_evaluator(rel_val, args.use_values_as_thresholds, args.default_threshold, args.margin_threshold, args.combine_thresholds, args.regions)
    rel_val.apply_evaluator(evaluator)
    # assign interpretations to the results we obtained
    rel_val.interpret(interpret_results)

    def filter_on_interpretations(result):
        # only consider results that match the user-requested flags
        return not args.interpretations or result.interpretation in args.interpretations

---

# Filter the results based on their interpretation, adding an additional mask when necessary to ensure that
# object_names, metric_names, and results
# from RelVal align with the filter function's condition
rel_val.filter_results(filter_on_interpretations)
# If this is from inspecting, it will include the annotations from the rel-val before, so re-write it
rel_val.write(join(args.output, "Summary.json"), annotations=annotations or rel_val.annotations[0])

utils.print_summary(rel_val, variables.REL_VAL_SEVERITIES, long=args.print_long)

---

Potential past results will not be replaced; instead, new results will be saved in a new directory (as is standard). 
Returns:
    str or None
        On success, returns the path to the JSON containing the computed metrics.
        Returns None otherwise.
    """

---

DOCUMENT:
    Parameters:
        rel_val: RelVal
            the RelVal object to potentially test and serves as a basis for default thresholds
        thresholds_paths: iterable or None
            if not None, a collection of strings representing paths to RelVal JSON files
        thresholds_defaults: iterable of 2-tuples or None
            set a default threshold value (tuple element 1) for a metric name (tuple element 0)
        threshold_margins: iterable of 2-tuples or None
            apply a margin in percentage (tuple element 1) to a threshold value of a metric name (tuple element 0)
        thresholds_combine: str
            either "mean" or "extreme", specifying how threshold values from thresholds_paths should be combined
        regions_paths: iterable or None
            if not None, a collection of strings representing paths to RelVal JSON files
    Returns:
        Evaluator
    """
    evaluator = utils.Evaluator()

---

# compare
COMPARE_PARSER = SUB_PARSERS.add_parser("compare", parents=[COMMON_FILE_PARSER, COMMON_PATTERN_PARSER, COMMON_METRIC_PARSER, COMMON_VERBOSITY_PARSER, COMMON_FLAGS_PARSER])
COMPARE_PARSER.add_argument("--output", "-o", help="directory for output", default="rel_val_comparison")
COMPARE_PARSER.add_argument("--difference", action="store_true", help="plot histograms with varying severity levels")
COMPARE_PARSER.add_argument("--plot", action="store_true", help="generate plots for value and threshold comparisons from RelVals")
COMPARE_PARSER.set_defaults(func=compare)

---

DOCUMENT:
    spec = importlib.util.spec_from_file_location("o2dpg_release_validation_plot", join(O2DPG_ROOT, "RelVal", "utils", "o2dpg_release_validation_plot.py"))
o2dpg_release_validation_plot = importlib.util.module_from_spec(spec)
spec.loader.exec_module(o2dpg_release_validation_plot)
sys.modules["o2dpg_release_validation_plot"] = o2dpg_release_validation_plot
from o2dpg_release_validation_plot import plot_pie_charts, plot_summary_grid, plot_compare_summaries, plot_overlays, plot_value_histograms


ROOT_MACRO_EXTRACT = join(O2DPG_ROOT, "RelVal", "utils", "ExtractAndFlatten.C")
ROOT_MACRO_RELVAL = join(O2DPG_ROOT, "RelVal", "utils", "ReleaseValidation.C")
ROOT_MACRO_METRICS = join(O2DPG_ROOT, "RelVal", "utils", "ReleaseValidationMetrics.C")

from ROOT import gROOT

gROOT.SetBatch()


#############################################
# Helper functions exclusively used here      #
#############################################

---

Returns:
        bool: True if successful, False otherwise.
    """
    def extract_filenames_from_list(list_filename):
        """
        A quick helper function to retrieve filenames listed in a specified file.
        """
        collected_files = []
        with open(list_filename, "r") as file:
            for line in file:
                line = line.strip()
                if not line:
                    continue
                collected_files.append(line)
        return collected_files

    include_file_directories = ",".join(include_file_directories) if include_file_directories else ""

---

DOCUMENT:
    s = f"{metric_name}, {test_name}, {interpretation}, {len(in_common)}, {len(only_in1)}, {len(only_in2)}"
    if args.print_long:
        in_common = ";".join(in_common) if in_common else "NONE"
        only_in1 = ";".join(only_in1) if only_in1 else "NONE"
        only_in2 = ";".join(only_in2) if only_in2 else "NONE"
        s += f", {in_common}, {only_in1}, {only_in2}"
    print(s)

---

# print
PRINT_PARSER = SUB_PARSERS.add_parser("print", parents=[COMMON_METRIC_PARSER, COMMON_PATTERN_PARSER, COMMON_FLAGS_PARSER])
PRINT_PARSER.add_argument("--path", help="either the full file path to a Summary.json or a directory where such a file is anticipated")
PRINT_PARSER.add_argument("--metric-names", dest="metric_names", action="store_true")
PRINT_PARSER.add_argument("--test-names", dest="test_names", action="store_true")
PRINT_PARSER.add_argument("--object-names", dest="object_names", action="store_true")
PRINT_PARSER.set_defaults(func=print_simple)

---

return
    if result.result_flag == utils.Result.FLAG_FAILED and is_critical:
        result.interpretation = variables.REL_VAL_INTERPRETATION_BAD
        return
    result.interpretation = variables.REL_VAL_INTERPRETATION_WARNING