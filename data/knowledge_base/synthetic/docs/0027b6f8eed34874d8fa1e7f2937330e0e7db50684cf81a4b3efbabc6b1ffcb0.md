## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/RelVal/utils/o2dpg_release_validation_utils.py

**Start chunk id:** 0027b6f8eed34874d8fa1e7f2937330e0e7db50684cf81a4b3efbabc6b1ffcb0

## Content

# retrieve all tests associated with the specified arguments
results = []
return_metrics_idx = []

# likely there is an optimized approach
for idx, metric in enumerate(metrics):
    mask = (self.object_names == metric.object_name) & (self.metric_names == metric.name)
    if not np.any(mask):
        continue
    for t in self.tests[mask]:
        return_metrics_idx.append(idx)
        results.append(t.test(metric))

return np.array(return_metrics_idx, dtype=int), np.array(results, dtype=Result)

---

NOTE that currently, searches can only be performed using a single string for object or metric name. Matching multiple criteria is not supported at this time.
    
        mask = self.any_mask if object_name is None else np.isin(self.object_names, object_name)
        mask = mask & (self.any_mask if metric_name is None else np.isin(self.metric_names, metric_name))
        return self.object_names[mask], self.metric_names[mask], self.metrics[mask]

---

with open(filepath, "w") as f:
    json.dump(final_dict, f, indent=2)


def extract_paths_or_from_file(paths):
    """
    Either return the paths directly or extract them from a text file
    """
    if len(paths) == 1 and paths[0].startswith("@"):
        with open(paths[0][1:], "r") as f:
            return f.read().splitlines()
    return paths

---

DOCUMENT:
    def __eq__(self, other):
        return self.object_name == other.object_name and self.name == other.name

    def as_dict(self):
        return {"object_name": self.object_name,
                "metric_name": self.name,
                "value": self.value,
                "comparable": self.comparable,
                "proposed_threshold": self.proposed_threshold,
                "lower_is_better": self.lower_is_better,
                "non_comparable_note": self.non_comparable_note}

    def from_dict(self, in_dict):
        self.object_name = in_dict["object_name"]
        self.name = in_dict["metric_name"]
        self.value = in_dict["value"]
        self.comparable = in_dict["comparable"]
        self.proposed_threshold = in_dict["proposed_threshold"]
        self.lower_is_better = in_dict["lower_is_better"]
        self.non_comparable_note = in_dict["non_comparable_note"]

---

class Metric:
    def __init__(self, object_name=None, name=None, value=None, proposed_threshold=None, comparable=None, lower_is_better=None, non_comparable_note=None, in_dict=None):
        self.object_name = object_name
        self.name = name
        self.value = value
        self.comparable = comparable
        self.proposed_threshold = proposed_threshold
        self.lower_is_better = lower_is_better
        self.non_comparable_note = non_comparable_note
        if in_dict is not None:
            self.from_dict(in_dict)

    def from_dict(self, in_dict):
        self.name = in_dict["result_name"]
        self.value = in_dict["value"]
        self.result_flag = in_dict["result_flag"]
        self.n_sigmas = in_dict["n_sigmas"]
        self.mean = in_dict["mean"]
        self.interpretation = in_dict["interpretation"]
        self.non_comparable_note = in_dict["non_comparable_note"]

---

def consider_object(self, object_name):
    """
    evaluate a name against a set of regex to determine if it should be included
    """
    if not self.include_patterns and not self.exclude_patterns:
        return True

    if self.include_patterns:
        for pattern in self.include_patterns:
            if re.search(pattern, object_name):
                return True
        return False

    # we can only reach this point if there are no include_patterns
    # which implies the presence of exclude_patterns, as otherwise we would have returned earlier
    for pattern in self.exclude_patterns:
        if re.search(pattern, object_name):
            return False
    return True

---

DOCUMENT:
    def interpret(self, interpret_func):
        """
        Apply an interpretation to the Result objects using a user-defined function
        """
        for metric_idx, result in zip(self.results_to_metrics_idx, self.results):
            interpret_func(result, self.metrics[metric_idx])

    def filter_results(self, filter_func):
        """
        Create a mask to filter results without discarding any
        """
        if self.results is None:
            return
        self.result_filter_mask = [filter_func(result) for result in self.results]

    def query_results(self, query_func=None):
        """
        Examine Result objects using a function provided by the user

        """

---

DOCUMENT:
    class TestLimits:
    """
    Integrates features to manage limits, validate values, and generate Result objects
    """
    def __init__(self, name, mean=None, std=None, test_function=default_evaluation):
        self.name = name
        self.mean = mean
        self.std = std
        self.limits = compute_limits(mean, std)
        self.test_function = test_function(self.limits)

    def set_test_function(self, test_function):
        """
        Assigns a test function that, using the limits, returns a lambda function to assess pass/fail for a given value
        """
        self.test_function = test_function(self.limits)

    def test(self, metric):
        """
        Assess a value and return a Result object
        """
        value = metric.value

---

DOCUMENT:
    def write(self, filepath, annotations=None):
        """
        Save all data to a JSON file

        The structure matches what ROOT's RelVal produces, allowing for the recreation of a RelVal object
        """
        all_objects = []

        def make_dict_include_results(object_name, metric, result):
            return {RelVal.KEY_OBJECT_NAME: object_name} | metric.as_dict() | result.as_dict()

        def make_dict_exclude_results(object_name, metric, *args):
            return {RelVal.KEY_OBJECT_NAME: object_name} | metric.as_dict()

---

#!/usr/bin/env python3
#
# Definition of common functionality

import re
from os.path import join, exists, isdir, abspath
from os import makedirs, rename
from shutil import rmtree, copy
from itertools import product
from subprocess import Popen, PIPE, STDOUT
from shlex import split
import json
import numpy as np


def default_evaluation(limits):
    """
    Returns a lambda function f(value) -> bool

    This function determines if a given value passes or fails (True/False)
    """
    if limits[0] is None and limits[1] is None:
        return lambda x: None
    if limits[0] is not None and limits[1] is None:
        return lambda x: x >= limits[0]
    if limits[0] is None and limits[1] is not None:
        return lambda x: x <= limits[1]
    return lambda x: limits[0] <= x <= limits[1]

---

DOCUMENT:
    self.object_names = np.array(self.object_names, dtype=str)
    self.metric_names = np.array(self.metric_names, dtype=str)
    self.metrics = np.array(self.metrics, dtype=Metric)
    self.any_mask = np.full(self.object_names.shape, True)

    # currently, results are still in list form
    self.results_to_metrics_idx = np.array(self.results_to_metrics_idx, dtype=int) if self.results else None
    self.test_names_results = np.array([r.name for r in self.results]) if self.results else None
    self.known_test_names = np.unique(self.test_names_results) if self.results else None
    self.results = np.array(self.results, dtype=Result) if self.results else None
    self.result_filter_mask = np.full(self.results.shape, True) if self.results is not None else None

---

def get_result_per_metric_and_test(self, metric_index_or_name=None, test_index_or_name=None):
    """
    Return Result objects that match the specified metric or test
    """
    test_name = test_index_or_name if (isinstance(test_index_or_name, str) or test_index_or_name is None) else self.known_test_names[test_index_or_name]
    metric_name = metric_index_or_name if (isinstance(metric_index_or_name, str) or metric_index_or_name is None) else self.known_metrics[metric_index_or_name]
    metric_idx = np.argwhere(self.metric_names == metric_name) if metric_name is not None else self.results_to_metrics_idx
    mask = np.isin(self.results_to_metrics_idx, metric_idx)
    if self.result_filter_mask is not None:
        mask = mask & self.result_filter_mask
    if test_name is not None:
        mask = mask & (self.test_names_results == test_name)
    return np.take(self.object_names, self.results_to_metrics_idx[mask]), self.results[mask]

---

DOCUMENT:
    def __init__(self, name=None, value=None, result_flag=FLAG_UNKNOWN, n_sigmas=None, mean=None, interpretation=None, non_comparable_note=None, in_dict=None):
        self.name = name
        self.value = value
        self.result_flag = result_flag
        self.n_sigmas = n_sigmas
        self.mean = mean
        self.interpretation = interpretation
        self.non_comparable_note = non_comparable_note
        if in_dict is not None:
            self.from_dict(in_dict)

    def as_dict(self):
        return {"result_name": self.name,
                "value": self.value,
                "result_flag": self.result_flag,
                "n_sigmas": self.n_sigmas,
                "mean": self.mean,
                "interpretation": self.interpretation,
                "non_comparable_note": self.non_comparable_note}

---

FOR each object_name in rel_val_thresholds.known_objects:
    FOR each metric_name in rel_val_thresholds.known_metrics:
        # retrieve metric for specified objects by name
        _, _, metrics = rel_val_thresholds.get_metrics((object_name,), (metric_name,))

        IF no values in metrics:
            CONTINUE

        # gather all values from all metrics for this object
        values = [m.value for m in metrics if m.comparable]

        # determine if lower or higher values are preferable
        lower_is_better = metrics[0].lower_is_better
        factor = 1 if lower_is_better else -1

---

def execute_command(cmd, log_file, current_working_dir=None):
    """
    A function to execute a command line
    """
    process = Popen(split(cmd), cwd=current_working_dir, stdout=PIPE, stderr=STDOUT, universal_newlines=True)
    # open a log file and append lines to it
    log_file = open(log_file, 'a')
    for line in process.stdout:
        log_file.write(line)
    process.wait()
    # close the log file and return the command's exit code
    log_file.close()
    return process.returncode


def find_interpretations(results, target_interpretation):
    """
    Return indices where the results have a specific interpretation
    """
    return np.array([result.interpretation == target_interpretation for result in results], dtype=bool)

---

for v in values:
                diff = v - proposed_threshold
                if (diff < 0 and lower_is_better) or (diff > 0 and not lower_is_better):
                    # if the value is below and lower is better (or the opposite), then accept it as it is definitely better than the proposed threshold
                    values_central.append(v)
                    continue
                if diff != 0:
                    # check how much the calculated difference deviates from the proposed value
                    diff = abs(proposed_threshold / diff)
                    if diff < 0.1:
                        # this indicates an outlier, accepted up to an order of magnitude
                        values_outlier.append(v)
                        continue
                # if this is reached, the value is worse than the proposed threshold but by less than one order of magnitude
                values_central.append(v)

---

DOCUMENT:
    if value is None:
        return Result(self.name, non_comparable_note=metric.non_comparable_note)
    if not self.test_function or self.mean is None:
        return Result(self.name, value, non_comparable_note=metric.non_comparable_note)
    n_sigmas = self.std[int(value > self.mean)]
    if n_sigmas == 0:
        n_sigmas = None
    elif n_sigmas is not None:
        n_sigmas = abs(self.mean - value) / n_sigmas if n_sigmas != 0 else 0

    # NOTE We aim for the test_function to directly return the test flag
    test_flag = self.test_function(value)
    if test_flag:
        test_flag = Result.FLAG_PASSED
    elif test_flag is None:
        test_flag = Result.FLAG_UNKNOWN
    else:
        test_flag = Result.FLAG_FAILED
    return Result(self.name, value, test_flag, n_sigmas, self.mean, non_comparable_note=metric.non_comparable_note)


class Evaluator:

---

DOCUMENT:
    print("\n##########################\n")


def get_summary_path(path):
    """
    Retrieve the complete path to Summary.json

    If a directory is specified, search for the file within it.
    """
    if isdir(path):
        path = join(path, "Summary.json")
    if exists(path):
        return path
    print(f"ERROR: Unable to locate {path}.")
    return None


def copy_overlays(rel_val, input_dir, output_dir):
    """
    Transfer overlay plots from the summary in the input directory to the output directory.
    """
    input_dir = abspath(input_dir)
    output_dir = abspath(output_dir)

    if not exists(input_dir):
        print(f"ERROR: Input directory {input_dir} does not exist")
        return 1

    in_out_same = input_dir == output_dir

    input_dir_new = input_dir + "_tmp"
    if in_out_same:
        # rename input directory
        rename(input_dir, input_dir_new)
        input_dir = input_dir_new

    if not exists(output_dir):
        makedirs(output_dir)

---

def apply_evaluator(self, evaluator):
    """
    Apply loaded tests
    """
    # First, we need to eliminate duplicates from object_names and metric_names, and remove the corresponding duplicates from metrics.
    if self.results is not None:
        object_metric_names = np.vstack((self.object_names, self.metric_names)).T
        _, idx = np.unique(object_metric_names, return_index=True, axis=0)
        self.metrics = self.metrics[idx]
        self.object_names = self.object_names[idx]
        self.metric_names = self.metric_names[idx]
        self.any_mask = np.full(self.object_names.shape, True)

    self.results_to_metrics_idx, self.results = evaluator.test(self.metrics)
    self.test_names_results = np.array([r.name for r in self.results])
    self.known_test_names = np.unique(self.test_names_results)
    self.result_filter_mask = np.full(self.results.shape, True)

---

# calculate the mean and standard deviation for the central region, and if there are any outlier values, also calculate their mean and standard deviation
mean_central = np.mean(values_central)
std_central = np.std(values_central)
if values_outlier.any():
    mean_outlier = np.mean(values_outlier)
    std_outlier = np.std(values_outlier)
else:
    mean_outlier = None
    std_outlier = None
# use these mean and standard deviation values to define two different test limits
evaluator.add_limits(object_name, metric_name, TestLimits("regions_tight", mean_central, (std_central, std_central)))
evaluator.add_limits(object_name, metric_name, TestLimits("regions_loose", mean_outlier, (std_outlier, std_outlier)))

---

if not values:
    evaluator.add_limits(object_name, metric_name, TestLimits("threshold_user"))
    continue
if thresholds_combine == "mean":
    # combine the values, by default use the mean as the threshold
    mean_central = np.mean(values)
else:
    # otherwise use the extremum
    mean_central = factor * max([factor * v for v in values])

margin = thresholds_margin[metric_name] * mean_central if thresholds_margin and metric_name in thresholds_margin else 0
# combine the standard limits and add the TestLimits to the Evaluator
if factor > 0:
    low = None
    up = margin
else:
    up = None
    low = margin
evaluator.add_limits(object_name, metric_name, TestLimits("threshold_user", mean_central, (low, up)))

---

def generate_object_metrics_results():
    """
    Sequentially return metrics and results for each object
    """
    results = None
    if self.results is not None:
        if self.result_filter_mask is not None:
            mask = self.result_filter_mask
        else:
            mask = np.full(self.results.shape, True)
        idx = self.results_to_metrics_idx[mask]
        object_names = np.take(self.object_names, idx)
        metrics = np.take(self.metrics, idx)
        results = self.results[mask]
    else:
        object_names = self.object_names
        metrics = self.metrics

    for object_name in np.unique(object_names):
        mask = object_names == object_name
        yield_metrics = metrics[mask]
        yield_results = results[mask] if results is not None else np.array([None] * len(yield_metrics))
        yield object_name, yield_metrics, yield_results

---

if self.results is None:
    object_names = self.object_names
    metrics = self.metrics
    results = np.empty(metrics.shape, dtype=bool)
    make_dict = make_dict_exclude_results
else:
    object_names = np.take(self.object_names, self.results_to_metrics_idx)
    metrics = np.take(self.metrics, self.results_to_metrics_idx)
    results = self.results
    make_dict = make_dict_include_results

for object_name, metric, result in zip(object_names, metrics, results):
    all_objects.append(make_dict(object_name, metric, result))

final_dict = {RelVal.KEY_OBJECTS: all_objects,
              RelVal.KEY_ANNOTATIONS: annotations}

with open(filepath, "w") as f:
    json.dump(final_dict, f, indent=2)

---

if not exists(output_dir):
    makedirs(output_dir)

object_names, _ = rel_val.get_result_per_metric_and_test()
object_names = list(set(object_names))

ret = 0
for object_name in object_names:
    filename = join(input_dir, f"{object_name}.png")
    if exists(filename):
        copy(filename, output_dir)
    else:
        print(f"File {filename} not found.")
        ret = 1

if in_out_same:
    rmtree(input_dir)

return ret

---

DOCUMENT:
    def compute_limits(mean, std):
    """
    Calculate numerical boundaries based on the provided mean and standard deviation
    """
    if mean is None or std is None:
        return (None, None)
    low, high = std[0], std[1]
    if low is not None and high is None:
        return (mean - low, None)
    if low is None and high is not None:
        return (None, mean + high)
    return (mean - low, mean + high)


class Result:
    """
    Stores outcome values after assessing a metric against its defined limits

    This represents the most basic unit of results we will use in the end
    """
    FLAG_UNKNOWN = 0
    FLAG_PASSED = 1
    FLAG_FAILED = 2

---

if "result_name" in line:
    # include this result; it will be associated with the metric it is derived from using the index
    self.add_result(idx, Result(in_dict=line))

    # transform all previous lists into numpy objects
    self.to_numpy()

def get_metrics(self, object_name=None, metric_name=None):
    """
    retrieve all metrics that match a specified object_name or metric_name

    Args:
        object_name: str or None
            the object name to search for; if None, any object name will be considered
        metric_name: str or None
            the metric name to search for; if None, any metric name will be considered

---

def load(self, summaries_to_test):
    """
    This method loads and populates the object using a dictionary
    """
    self.annotations = []
    self.object_names = []
    self.metric_names = []
    self.metrics = []
    self.results_to_metrics_idx = []
    self.results = []

---

@staticmethod
def read(path_or_dict):
    """
    Convenient function to load metrics/results from a JSON file or a dictionary
    """
    if isinstance(path_or_dict, dict):
        return path_or_dict
    with open(path_or_dict, "r") as f:
        return json.load(f)

def add_metric(self, metric):
    """
    Adds a metric
    """
    object_name = metric.object_name
    if not self.consider_object(object_name) or not self.consider_metric(metric.name):
        return False
    self.object_names.append(object_name)
    self.metric_names.append(metric.name)
    self.metrics.append(metric)
    return True

---

def print_summary(rel_val, interpretations, long=False):
    """
    Verify if any two histograms have a specific severity level following RelVal
    """
    print("\n##### RELVAL SUMMARY #####\n")
    for metric_name in rel_val.known_metrics:
        for test_name in rel_val.known_test_names:
            object_names, results = rel_val.get_result_per_metric_and_test(metric_name, test_name)
            print(f"METRIC: {metric_name}, TEST: {test_name}")
            for interpretation in interpretations:
                object_names_interpretation = object_names[count_interpretations(results, interpretation)]
                percent = len(object_names_interpretation) / rel_val.number_of_objects
                print(f"  {interpretation}: {len(object_names_interpretation)} ({percent * 100:.2f}%)")
                if long:
                    for object_name in object_names_interpretation:
                        print(f"    {object_name}")

    print("\n##########################\n")

---

def setup_regions(evaluator, rel_val_regions):
    """
    Incorporate regions into the Evaluator for test cases
    """
    # Iterate through all elements
    for object_name in rel_val_regions.known_objects:
        for metric_name in rel_val_regions.known_metrics:
            _, _, metrics = rel_val_regions.get_metrics((object_name,), (metric_name,))
            # Collect all metric values for a specific object and metric
            values = [m.value for m in metrics if m.comparable]
            # Gather necessary properties of the metrics
            proposed_threshold = metrics[0].proposed_threshold
            lower_is_better = metrics[0].lower_is_better
            # Separate metric values into central and outlier groups
            values_central = []
            values_outlier = []
            for v in values:
                diff = v - proposed_threshold

---

def add_result(self, metric_idx, result):
    metric = self.metrics[metric_idx]
    object_name = metric.object_name
    if not self.consider_object(object_name) or not self.consider_metric(metric.name):
        return
    self.results_to_metrics_idx.append(metric_idx)
    self.results.append(result)

def get_metric_checking_dict(self, in_dict):
    """
    Verify if this metric is already present
    """
    if self.metrics is None:
        return None, None

    metric = Metric(in_dict=in_dict)

    for idx, search_metric in enumerate(self.metrics):
        if metric == search_metric:
            return idx, search_metric

    return None, metric

def to_numpy(self):
    """
    Transform all lists into numpy arrays for quicker access later
    """
    self.known_objects = np.unique(self.object_names)
    self.known_metrics = np.unique(self.metric_names)

---

# unique list of test names
self.known_test_names = None

# to store some annotations
self.annotations = None

def enable_metrics(self, metric_names):
    """
    Activate a list of metrics by their names
    """
    if not metric_names:
        return
    for metric in metric_names:
        if metric in self.include_metrics:
            continue
        self.include_metrics.append(metric)

def disable_metrics(self, metric_names):
    """
    Deactivate a list of metrics by their names
    """
    if not metric_names:
        return
    for metric in metric_names:
        if metric in self.exclude_metrics:
            continue
        self.exclude_metrics.append(metric)

---

def __init__(self):
    # metric names to be included (leave empty for all)
    self.include_metrics = []
    # metric names to be excluded, overrides self.include_metrics
    self.exclude_metrics = []
    # regex lists to include/exclude objects based on their names
    self.include_patterns = None
    self.exclude_patterns = None

    # collecting all available data; the following three members will have the same length
    self.object_names = None
    self.metric_names = None
    # metric objects
    self.metrics = None

    # unique object and metric names
    self.known_objects = None
    self.known_metrics = None

    # collecting all results; the following three members will have the same length
    self.results = None
    # each index corresponds to the object in self.object_names, self.metric_names and self.metrics
    self.results_to_metrics_idx = None

---

DOCUMENT:
    class Evaluator:

    def __init__(self):
        self.object_names = []
        self.metric_names = []
        self.test_names = []
        self.tests = []
        self.mask_any = None

    def add_limits(self, object_name, metric_name, test_limits):
        self.object_names.append(object_name)
        self.metric_names.append(metric_name)
        self.test_names.append(test_limits.name)
        self.tests.append(test_limits)

    def initialise(self):
        self.object_names = np.array(self.object_names, dtype=str)
        self.metric_names = np.array(self.metric_names, dtype=str)
        self.test_names = np.array(self.test_names, dtype=str)
        self.tests = np.array(self.tests, dtype=TestLimits)
        self.mask_any = np.full(self.test_names.shape, True)

    def evaluate(self, metrics):
        """
        All arguments must have the same length and cannot be None.
        """

---

DOCUMENT:
    def initialise_thresholds(evaluator, rel_val, rel_val_thresholds, thresholds_default, thresholds_margin, thresholds_combine="mean"):
    """
    Add thresholds to the Evaluator for a single test case.
    """
    # Default thresholds will be generated and applied to all objects and metrics identified in the RelVal.
    _, _, metrics = rel_val.get_metrics()
    for metric in metrics:
        # Retrieve the default threshold for each metric.
        proposed_threshold = thresholds_default[metric.name] if thresholds_default else metric.proposed_threshold
        # Adjust the standard deviation boundaries based on whether a lower or higher value is better.
        std = (0, None) if metric.lower_is_better else (None, 0)
        evaluator.add_limits(metric.object_name, metric.name, TestLimits("threshold_default", proposed_threshold, std))

    if not rel_val_thresholds:
        # No further action needed if there are no user-defined thresholds.
        return

---

def get_result_matrix_objects_metrics(self, test_index):
    """
    Return a matrix of Result objects.

    The vertical axis represents object names, while the horizontal axis represents metric names.

    Additionally, return the metric and object names to inform the user about the content.
    """
    mask = self.test_names_results == (self.known_test_names[test_index])
    idx = self.results_to_metrics_idx[mask]
    results = self.results[mask]
    object_names = np.take(self.object_names, idx)
    metric_names = np.take(self.metric_names, idx)

    idx = np.lexsort((metric_names, object_names))

    object_names = np.sort(np.unique(object_names))
    metric_names = np.sort(np.unique(metric_names))

    return metric_names, object_names, np.reshape(results[idx], (len(object_names), len(metric_names)))

---

DOCUMENT:
    def consider_metric(self, metric_name):
        """
        Determines if a specific metric should be considered.
        """
        if self.exclude_metrics and metric_name in self.exclude_metrics:
            return False
        if not self.include_metrics or metric_name in self.include_metrics:
            return True
        return False

    def set_object_name_patterns(self, include_patterns, exclude_patterns):
        """
        Load patterns to be used for regex matching.
        """
        def load_this_patterns(patterns):
            if not patterns or not patterns[0].startswith("@"):
                return patterns

---

DOCUMENT:
    Return matching Result objects along with their names
        """
        mask = np.array([query_func is None or query_func(result) for result, _ in enumerate(self.results)])
        if self.result_filter_mask is not None:
            mask = mask & self.result_filter_mask
        idx = self.results_to_metrics_idx[mask]
        return np.take(self.object_names, idx), np.take(self.metric_names, idx), self.test_names_results[idx], self.results[idx]

    @property
    def number_of_tests(self):
        return len(self.known_test_names) if self.results is not None else 0

    @property
    def number_of_metrics(self):
        return len(self.known_metrics)

    @property
    def number_of_objects(self):
        return len(self.known_objects)

    def get_test_name(self, idx):
        return self.known_test_names[idx]

    def get_metric_name(self, idx):
        return self.known_metrics[idx]

---

for summary_to_test in summaries_to_test:
    summary_to_test = self.read(summary_to_test)
    if annotations := summary_to_test.get(RelVal.KEY_ANNOTATIONS, None):
        self.annotations.append(annotations)
    for line in summary_to_test[RelVal.KEY_OBJECTS]:
        # each list object corresponds to an object with a specific test result
        # we first verify if this metric has already been loaded
        idx, metric = self.get_metric_checking_dict(line)
        if idx is None:
            # in this case, this metric is new
            idx = len(self.metrics)
            if not self.add_metric(metric):
                # only proceed with addition if the metric is not already present
                continue

---

DOCUMENT:
    patterns_from_file = []
    filename = patterns[0][1:]
    if not os.path.exists(filename):
        print(f"WARNING: Pattern file {filename} does not exist, no patterns will be extracted!")
        return
    with open(filename, "r") as f:
        for line in f:
            line = line.strip()
            # remove all comments; supports inline comments or entire comment lines), then take the first token
            line = line.split("#")[0].strip()
            if not line:
                continue
            patterns_from_file.append(line)
    return patterns_from_file

    self.include_patterns = load_this_patterns(include_patterns)
    self.exclude_patterns = load_this_patterns(exclude_patterns)