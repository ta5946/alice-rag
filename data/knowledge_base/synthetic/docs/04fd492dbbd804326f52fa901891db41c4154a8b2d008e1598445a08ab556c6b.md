## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/analysis_testing/README.md

**Start chunk id:** 04fd492dbbd804326f52fa901891db41c4154a8b2d008e1598445a08ab556c6b

## Content

ANALYSES ARE CONFIGURED IN A GLOBAL [CONFIGURATION](../config/analysis_testing/json/analyses_config.json) FILE. HERE IS AN EXAMPLE:
```json
{
    "name": "EventTrackQA",
    "enabled": true,
    "expected_output": ["AnalysisResults.root"],
    "valid_mc": true,
    "valid_data": true,
    "tasks": ["o2-analysis-timestamp",
              "o2-analysis-track-propagation",
              "o2-analysis-trackselection",
              "o2-analysis-event-selection",
              "o2-analysis-qa-event-track"]
}
```
It is crucial to list the tasks from `O2Physics` in a `tasks` array, as this array will be used to build the command-line pipeline. You can specify if an analysis is valid for mc or data by setting `valid_mc` or `valid_data`. To enable your analysis, set `enabled` to `true`. Additionally, providing a concise yet descriptive `name` for the analysis is essential.

---

No, you can start your analysis with the command:
```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow_analysis_test.json -tt Analysis_<analysis-name>
```
The `-tt` option allows you to define a specific task to be executed. If omitted, all tasks within the workflow will be processed.

---

## Additional Options and Capabilities

After you executed the analysis workflow as detailed [previously](#testing-an-analysis-on-some-aod), you can verify its technical success (excluding any physics output validations). To perform this check, run:
```bash
${O2DPG_ROOT}/MC/analysis_testing/o2dpg_analysis_test_config.py validate-output --tasks <analysis-name1> [<analysis-name2> [...]] [-d <output-directory>]
```
This command checks if the analysis completed successfully and ensures the expected outputs are present. By default, the output directory is `Analysis`, but you can specify a different directory using the `-d <output-directory>` option.

For more sub-commands and options, use:
```bash
${O2DPG_ROOT}/MC/analysis_testing/o2dpg_analysis_test_config.py --help # OR
${O2DPG_ROOT}/MC/analysis_testing/o2dpg_analysis_test_config.py <sub-command> --help
```

---

## AnalysisQC

"AnalysisQC" relies on existing tools and is primarily used in MC GRID productions and data reconstruction processes. To run a specific analysis, simply add the analysis definition as described [above](#definition-of-analyses), making sure the `enabled` flag is set to `true`. Only analyses with this flag activated will be executed. Given that the defined analyses will run automatically, it's important to discuss potential runtime and resource requirements before approving any related requests or pull requests (PRs). The AnalysisQC should not significantly impact the efficiency of MC productions or data reconstruction.

## Further options and possibilities

### Verify Analysis Success

Check if an analysis was successful.

---

Certainly, it is crucial to provide an analysis with a concise yet descriptive `name`.
To facilitate potential automated post-processing, the anticipated output must be defined in the `expected_output` list.

---

### Configuration for Analysis JSON

If a specific configuration for your analysis is not found, the default settings from [here](../config/analysis_testing/json/default/) will be applied.
It is recommended, however, to include specific configurations for your analysis to avoid conflicts with other configurations that may not meet your needs.
These specific configurations should be placed in a sub-directory that exactly matches the name of your analysis, as demonstrated [here](../config/analysis_testing/json/EventSelectionQA/).
Each of these directories will contain a further sub-directory indicating the collision system. Within this, the files must be named either `analysis-testing-mc.json` or `analysis-testing-data.json`.
Please note that, if no specific configuration is found, the default settings are used based on the collision system and whether the data or MC is being used.

## Testing an Analysis on Some AOD

(Note: The second part of the original text, about testing an analysis on some AOD, seems to be a heading and not a content to paraphrase. Thus, it is not included in the paraphrased version.)

---

## Executing an Analysis on Some AOD

Start by setting up the workflow to perform the analysis. This can be achieved using
```bash
${O2DPG_ROOT}/MC/analysis_testing/o2dpg_analysis_test_workflow.py -f <path-to-aod> [--is-mc] [-a <output-directory>] [--include-disabled]
```
For a list of all available options, execute
```bash
${O2DPG_ROOT}/MC/analysis_testing/o2dpg_analysis_test_workflow.py --help
```
By default, the system assumes data analysis. To specify that the analysis should be conducted on MC, include the flag `--is-mc`.

The outcomes of your analysis will be saved in `<output-directory>/<analysis-name>`. If no directory is specified, the default location is `Analysis/<analysis-name>`.

It's important to note that disabled analyses (indicated by `"enabled": false`, see [above](#definition-of-analyses)) won't be included in the workflow. To include them anyway, use the flag `--include-disabled`.

The workflow by default is stored in `workflow_analysis_test.json`. This can be altered by using the `-o <workflow-filename>` option.

---

# Analysis Testing (aka AnalysisQC)

A variety of analyses are maintained in this section. It is designed for testing purposes and **is not** intended to substitute production analysis procedures.

Technically, it comprises a small suite of tools and configurations.

## Definition of Analyses