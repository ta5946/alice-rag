## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2dpg_sim_workflow_anchored.py

**Start chunk id:** 011ef379cb47483b106986fa11a51fd18a4cc245ceb7cc0f1f9695059f1ccc4b

## Content

# First, determine the trigger efficiency
effTrigger = trig_eff_arg
if effTrigger < 0:
  # Verify if ColSystem is listed in trigger_effs
  if ColSystem in trigger_effs:
    if ColSystem == "pp":
      if eCM < 1000:
        effTrigger = trigger_effs["pp"]["1000"]
      elif eCM < 6000:
        effTrigger = trigger_effs["pp"]["6000"]
      else:
        effTrigger = trigger_effs["pp"]["default"]
    else:
      effTrigger = trigger_effs[ColSystem]["default"]
  else:
    effTrigger = 0.759  # The simulation will fail later if the collision system is not specified

---

def fetch_CTPScalers(ccdbreader, run_number, timestamp=None):
    """
    fetches the CTP scalers object for a specified timestamp and run number
    and computes the interaction rate for Monte Carlo digitizers
    """
    path = "CTP/Calib/Scalers/runNumber=" + str(run_number)
    _, ctpscaler = ccdbreader.fetch(path, "o2::ctp::CTPRunScalers", timestamp=timestamp)
    if ctpscaler is not None:
      ctpscaler.convertRawToO2()
      return ctpscaler
    return None

---

DOCUMENT:
    def retrieve_params_fromGRPECS_and_OrbitReset(ccdbreader, run_number, run_start, run_end):
    """
    Fetches the start of run (sor), end of run (eor), and other global parameters from the GRPECS object using a run number. 
    To achieve this, we must locate the appropriate object through a browsing request and metadata filtering.
    Optionally, you can provide an existing result from RCT/Info/RunInformation for consistency checks. 
    If inconsistencies are detected, we will use the time from RCT and issue a warning message.

    NOTE: This function is deprecated; it should no longer be used and may be removed soon.
    """

    # perform a simple HTTP request to the "browsing" endpoint
    url="http://alice-ccdb.cern.ch/browse/GLO/Config/GRPECS/runNumber="+str(run_number)
    ansobject=requests.get(url)
    tokens=ansobject.text.split("\n")

---

print("Collision system ", ColSystem)

# potentially override orbitsPerTF with external choices
if args.orbitsPerTF!="":
   # we need the interaction rate for this calculation
   # let's use the one from IR.txt (async reco) as a quick way to make the decision
   run_rate, _ = retrieve_MinBias_CTPScaler_Rate(ctp_scalers, mid_run_timestamp/1000., args.trig_eff, grplhcif.getBunchFilling().getNBunches(), ColSystem, eCM)
   determined_orbits = parse_orbits_per_tf(args.orbitsPerTF, run_rate)
   if determined_orbits != -1:
      print("Adjusting orbitsPerTF from " + str(GLOparams["OrbitsPerTF"]) + " to " + str(determined_orbits))
      GLOparams["OrbitsPerTF"] = determined_orbits

# determine the timestamp and production offset for the final MC job
timestamp, prod_offset = determine_timestamp(run_start, run_end, [args.split_id - 1, args.prod_split], args.cycle, args.tf, GLOparams["OrbitsPerTF"])

---

def fetch_Aggregated_RunInfos(run_number):
    """
    Obtains start of run (sor), end of run (eor), and other global parameters using the
    AggregatedRunInfo object in O2, which ensures consistent information building.
    This method is recommended over the older function "retrieve_params_fromGRPECS_and_OrbitReset".
    """

    runInfo = o2.parameters.AggregatedRunInfo.buildAggregatedRunInfo(o2.ccdb.BasicCCDBManager.instance(), run_number)
    detectorList = o2.detectors.DetID.getNames(runInfo.grpECS.getDetsReadOut())
    assert (run_number == runInfo.runNumber)
    assert (run_number == runInfo.grpECS.getRun())
    return {"SOR" : runInfo.sor,
            "EOR" : runInfo.eor,
            "FirstOrbit" : runInfo.orbitSOR,
            "LastOrbit" : runInfo.orbitEOR,
            "OrbitsPerTF" : int(runInfo.orbitsPerTF),
            "detList" : detectorList}

---

print("The exclusion list contains " + str(len(exclude_list)) + " entries")
if len(exclude_list) == 0:
    return False

---

# retrieve orbit reset for determining orbitFirst
_, oreset = ccdbreader.fetch("CTP/Calib/OrbitReset", "vector<Long64_t>", timestamp=run_start)
print("All orbit resets")
for i in range(len(oreset)):
    print(" oreset " + str(i) + " " + str(oreset[i]))

print("OrbitReset:", int(oreset[0]))
orbitFirst = int((1000*run_start - oreset[0])//LHCOrbitMUS)  # calculation in microseconds
orbitLast = int((1000*run_end - oreset[0])//LHCOrbitMUS)
print("OrbitFirst", orbitFirst) # first orbit of the run
print("LastOrbit of run", orbitLast)

# Retrieve the detector list
print("DetsReadout-Mask: ", grp["mDetsReadout"]['v'])
detList = o2.detectors.DetID.getNames(grp["mDetsReadout"]['v'])
print("Detector list is ", detList)

# orbitReset.get(run_number)
return {"FirstOrbit" : orbitFirst, "LastOrbit" : orbitLast, "OrbitsPerTF" : int(grp["mNHBFPerTF"]), "detList" : detList}

---

# determine the number of timeframes that fit within the run duration
# calculate the number of orbits per timeframe and then multiply by the orbit duration to find the number of timeframes
ntimeframes = time_length_inmus / (HBF_per_timeframe * LHCOrbitMUS)
# also compute the total number of orbits in the run duration
print (f"This run accommodates {ntimeframes} timeframes")

# ascertain the maximum number of timeframes a single job can handle
maxtimeframesperjob = ntimeframes / totaljobs
print (f"Each job can process up to {maxtimeframesperjob} timeframes at a production split of {totaljobs}")
print (f"Given that each job processes {ntf} timeframes, this translates to a filling rate of {ntf / maxtimeframesperjob}")
# ensure the filling rate does not exceed 100%
assert(ntf <= maxtimeframesperjob)

---

# Example:
#  ${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow_anchored.py -tf 500 --split-id ${s} --cycle ${cycle} --prod-split 100 --run-number 505600         \
#                                                       -- -gen pythia8 -eCM 900 -col pp -gen pythia8 -proc inel                             \
#                                                           -ns 22 -e TGeant4                                                                \
#                                                           -j 8 -interactionRate 2000                                                       \
#                                                           -field +2                                                                        \
#                                                           -confKey "Diamond.width[2]=6"
# (the initial set of parameters is used to define the anchoring point, while the subsequent parameters are passed directly to the workflow creation process)

---

# we proceed to the unanchored MC workflow creation
# TODO: this should be implemented in a more pythonic manner clearly
# NOTE: forwardargs can - in principle - include some of the arguments added here. However, the last argument takes precedence, overwriting any previous ones.
energyarg = (" -eCM " + str(eCM)) if A1 == A2 else (" -eA " + str(eA) + " -eB " + str(eB))
forwardargs += " -tf " + str(args.tf) + " --sor " + str(run_start) + " --timestamp " + str(timestamp) + " --production-offset " + str(prod_offset) + " -run " + str(args.run_number) + " --run-anchored --first-orbit "       \
               + str(GLOparams["FirstOrbit"]) + " -field ccdb -bcPatternFile ccdb" + " --orbitsPerTF " + str(GLOparams["OrbitsPerTF"]) + " -col " + str(ColSystem) + str(energyarg)
if '--readoutDets' not in forwardargs:
   forwardargs += ' --readoutDets ' + GLOparams['detList']
print ("forward args ", forwardargs)

---

def fetch_GRP(ccdbreader, timestamp):
    """
    fetches the GRP for a given time stamp
    """
    grp_path = "GLO/GRP/GRP"
    header = ccdbreader.fetch_header(grp_path, timestamp)
    if not header:
        print(f"WARNING: Failed to download GRP object for timestamp {timestamp}")
        return None
    ts, grp = ccdbreader.fetch(grp_path, "o2::parameters::GRPObject", timestamp = timestamp)
    return grp

def fetch_GRPLHCIF(ccdbreader, timestamp):
    """
    fetches the GRPLHCIF object for a given time stamp
    """
    _, grplhcif = ccdbreader.fetch("GLO/Config/GRPLHCIF", "o2::parameters::GRPLHCIFData", timestamp = timestamp)
    return grplhcif

---

# this is PyROOT; enables reading ROOT C++ objects
from ROOT import o2, TFile, TString, TBufferJSON, TClass, std

# some global constants
# these values should be derived from the C++ code (via PyROOT and library access)
LHCMaxBunches = 3564;                           # maximum number of bunches
LHCRFFreq = 400.789e6;                          # LHC RF frequency in Hz
LHCBunchSpacingNS = 10 * 1.e9 / LHCRFFreq;      # bunch spacing in ns (10 RF buckets)
LHCOrbitNS = LHCMaxBunches * LHCBunchSpacingNS; # orbit duration in ns
LHCOrbitMUS = LHCOrbitNS * 1e-3;                # orbit duration in microseconds
LHCBunchSpacingMUS = LHCBunchSpacingNS * 1e-3   # bunch spacing in microseconds

# these should be part of a module or support layer
class CCDBAccessor:
    def __init__(self, url):
        # This is utilized for specific operations
        self.api = o2.ccdb.CcdbApi()
        self.api.init(url)

---

# identify the orbit associated with the timestamp (primarily utilized in exclude_timestamp function)
orbit = GLOparams["FirstOrbit"] + int((timestamp - GLOparams["SOR"]) / ( LHCOrbitMUS / 1000))

# these values are based on
print ("Detected start-of-run as: ", run_start)
print ("Detected end-of-run as: ", run_end)
print ("Detected timestamp as: ", timestamp)
print ("Detected offset as: ", prod_offset)
print ("SOR: ", GLOparams["SOR"])
print ("EOR: ", GLOparams["EOR"])
print ("TIM: ", timestamp) # this specific timestamp
print ("OS: ", GLOparams["FirstOrbit"])
print ("OE: ", GLOparams["LastOrbit"])
print ("TO: ", orbit) # this particular orbit

# evaluate if the timestamp should be excluded
job_is_excluded = exclude_timestamp(timestamp, orbit, args.run_number, args.run_span_file, GLOparams)
# potentially reverse the selection
if args.invert_irframe_selection:
   job_is_excluded = not job_is_excluded

---

# this is used for the actual fetching for now
o2.ccdb.BasicCCDBManager.instance().setURL(url)
# we allow nullptr responses and will handle them ourselves
o2.ccdb.BasicCCDBManager.instance().setFatalWhenNull(False)

def fetch(self, path, obj_type, timestamp=None, meta_info=None):
    """
    TODO We could use CcdbApi::snapshot at some point, needs revision
    """

    if not timestamp:
        timestamp = o2.ccdb.BasicCCDBManager.instance().getTimestamp()
    else:
        o2.ccdb.BasicCCDBManager.instance().setTimestamp(timestamp)

    if not meta_info:
        obj = o2.ccdb.BasicCCDBManager.instance().get(obj_type)(path)
    else:
        obj = o2.ccdb.BasicCCDBManager.instance().get(obj_type)(path, meta_info)

    return timestamp, obj

def get_run_duration(self, run_number):
    return o2.ccdb.BasicCCDBManager.instance().getRunDuration(run_number)

---

timeframelength_intime = global_run_params["EOR"] - global_run_params["SOR"]
timeframelength_inorbits = global_run_params["LastOrbit"] - global_run_params["FirstOrbit"]
total_excluded_fraction = 0
excluded = False
for exclusion_entry in exclude_list:
    data_is_in_orbits = exclusion_entry[0] < 1514761200000
    print("Checking data", exclusion_entry)
    if data_is_in_orbits:
        total_excluded_fraction += (exclusion_entry[1] - exclusion_entry[0]) / timeframelength_inorbits
        if exclusion_entry[0] <= orbit and orbit <= exclusion_entry[1]:
            print("Excluding orbit ", str(orbit))
            excluded = True
    else:
        total_excluded_fraction += (exclusion_entry[1] - exclusion_entry[0]) / timeframelength_intime
        if exclusion_entry[0] <= ts and ts <= exclusion_entry[1]:
            print("Excluding timestamp ", str(ts))

---

print ("forward arguments ", forwardargs)
cmd = "${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py " + forwardargs

---

EXCEPT=subprocess.CalledProcessError AS e:
    PRINT(f"Command failed with return code {e.returncode}")
    PRINT("Output:")
    PRINT(e.output)
    RETURN {}, {}

IF __name__ == "__main__":
    sys.exit(main())

---

#!/usr/bin/env python3

import sys
import time
import argparse
from os import environ, makedirs
from os.path import join, expanduser, exists, dirname
from os.path import split as ossplit
from copy import deepcopy
import array as arr
import os
import requests
import re
import json
import math
import pandas as pd
import subprocess
import shlex

# a workaround to locate the script for metadata upload
o2dpg_root = os.environ.get("O2DPG_ROOT")
if o2dpg_root is None:
  raise EnvironmentError("O2DPG_ROOT is not configured in the environment.")
mc_prodinfo_path = os.path.abspath(os.path.join(o2dpg_root, "MC", "prodinfo"))
sys.path.append(mc_prodinfo_path)
from mcprodinfo_ccdb_upload import MCProdInfo, upload_mcprodinfo_meta, query_mcprodinfo
import dataclasses

# generates a time-stamped MC workflow; integrated into a specified run number (depending on production scale, etc.)

---

FUNCTION parse_orbits_per_tf(orbitsPerTF, intRate):
"""
Function to calculate the number of orbits per TF based on the interaction rate.
"""
if intRate is None or intRate < 0:
    return -1

# If the argument is a single integer, we use it directly
if orbitsPerTF.isdigit():
    return int(orbitsPerTF)

# Otherwise, we interpret the argument as a string in the format
# a1:b1:o1,a2:b2:o2,...
# where we apply orbit o2 if the intRate is within the range a2 <= intRate < b2.
ranges = orbitsPerTF.split(',')
for entry in ranges:
    try:
        a, b, x = map(int, entry.split(':'))
        if a <= intRate < b:
            return x
    except ValueError:
        raise ValueError(f"Invalid format in entry: {entry}")

# If no valid range is found, we return -1, indicating
# that the orbit number will be determined from GRPECS
return -1

---

# Each cycle gradually fills the run range. The maximum number of cycles needed to fully populate the run is:
maxcycles = maxtimeframesperjob / ntf
print (f"We can complete this many cycle iterations to achieve 100%: {maxcycles}")

# In total, we have maxcycles * totaljobs slots to fill the run range with ntf timeframes per slot.
# Determine the slot for simulation.
production_offset = int(thisjobID * maxcycles) + cycle
# Add the time difference of this slot to the start-of-run to obtain the final timestamp.
timestamp_of_production = sor + production_offset * ntf * HBF_per_timeframe * LHCOrbitMUS / 1000
# This is a closure test. With perfect floating point precision everywhere, it wouldn't fail.
# However, since we lack that and have some integer conversions, it's better to verify again.
assert (timestamp_of_production >= sor)
assert (timestamp_of_production <= eor)
return int(timestamp_of_production), production_offset

---

def determine_timestamp(sor, eor, splitinfo, cycle, ntf, HBF_per_timeframe = 256):
    """
    Calculates the timestamp and production offset based on the production's global properties
    (such as MC splitting) and the run's characteristics. The parameter ntf represents the number of timeframes per MC job.
    Args:
        sor: int
            start-of-run in milliseconds since epoch
        eor: int
            end-of-run in milliseconds since epoch
        splitinfo: tuple (int, int)
            splitinfo[0]: the split ID of this job
            splitinfo[1]: the total number of jobs to split into
        cycle: int
            cycle number for this production. A run is usually not completely filled by a single simulation
            but only a fraction of events is simulated. With each additional cycle, the data run is covered more comprehensively.
        ntf: int
            number of timeframes
        HBF_per_timeframe: int
            number of orbits per timeframe
    Returns:

    """

---

# calculate essential physics parameters
eCM = grplhcif.getSqrtS()
eA = grplhcif.getBeamEnergyPerNucleonInGeV(o2.constants.lhc.BeamDirection.BeamC)
eB = grplhcif.getBeamEnergyPerNucleonInGeV(o2.constants.lhc.BeamDirection.BeamA)
A1 = grplhcif.getAtomicNumberB1()
A2 = grplhcif.getAtomicNumberB2()

---

def exclude_timestamp(ts, orbit, run, filename, global_run_params):
    """
    Determines if the timestamp ts (or orbit) is within a period of bad data.
    Returns true if the timestamp should be excluded; false otherwise.
    
    The timestamp ts should be in milliseconds.
    Orbit refers to an orbit after the orbitreset of the run.
    """
    if len(filename) == 0:
       return False

    if not os.path.isfile(filename):
       return False

    def parse_file(filename):
      parsed_data = []
      with open(filename, 'r') as file:
        for line in file:
            # Split the line into exactly 4 parts (first three numbers + comment)
            columns = re.split(r'[,\s;\t]+', line.strip(), maxsplit=3)

            if len(columns) < 3:
                continue  # Skip lines with insufficient columns

---

# identify the collision system and energy
print("Identified eCM ", eCM)
print("Identified eA ", eA)
print("Identified eB ", eB)
print("Identified atomic number A1 ", A1)
print("Identified atomic number A2 ", A2)
ColSystem = ""
col_systems = {
    "pp": (1, 1),
    "pO": (1, 8),
    "Op": (8, 1),
    "OO": (8, 8),
    "NeNe": (10, 10),
    "PbPb": (82, 82)
}
# verify if the collision system is known
for system, (a1, a2) in col_systems.items():
    if A1 == a1 and A2 == a2:
        ColSystem = system
        break
if ColSystem == "":
    print(f"ERROR: Unknown collision system for A1={A1}, A2={A2}. Check the GRPLHCIF object.")
    exit(1)

print("Collision system ", ColSystem)

---

def fetch_header(self, path, timestamp=None):
    meta_info = std.map["std::string", "std::string"]()
    if timestamp is None:
        timestamp = -1
    header = self.api.retrieveHeaders(path, meta_info, timestamp)
    return header


def retrieve_CCDBObject_asJSON(ccdbreader, path, timestamp, objtype_external=None):
    """
    Fetches a CCDB object as a JSON/dictionary.
    The object type does not need to be known beforehand.
    """
    header = ccdbreader.fetch_header(path, timestamp)
    if not header:
        print(f"WARNING: Could not get header for path {path} and timestamp {timestamp}")
        return None
    objtype = header["ObjectType"]
    if objtype is None:
        objtype = objtype_external
    if objtype is None:
        return None

    ts, obj = ccdbreader.fetch(path, objtype, timestamp)
    # convert object to json
    jsonTString = TBufferJSON.ConvertToJSON(obj, TClass.GetClass(objtype))
    return json.loads(jsonTString.Data())

---

# The split ID must be less than or equal to the production ID.
assert(args.split_id <= args.prod_split)

# Create a CCDB accessor object.
ccdbreader = CCDBAccessor(args.ccdb_url)

# Obtain the EOR, SOR, and initial run parameters.
GLOparams = retrieve_Aggregated_RunInfos(args.run_number)
run_start = GLOparams["SOR"]
run_end = GLOparams["EOR"]

mid_run_timestamp = (run_start + run_end) // 2

# --------
# Retrieve additional crucial global properties required later.
# --------
ctp_scalers = retrieve_CTPScalers(ccdbreader, args.run_number, timestamp=mid_run_timestamp)
if ctp_scalers is None:
   print(f"ERROR: Unable to retrieve scalers for run number {args.run_number}")
   exit(1)

# Fetch the GRPHCIF object using the mid-run timestamp.
grplhcif = retrieve_GRPLHCIF(ccdbreader, int(mid_run_timestamp))

---

# here, we can share details about the production (if desired)
    if args.publish_mcprodinfo == True or os.getenv("PUBLISH_MCPRODINFO") != None:
        prod_tag = os.getenv("ALIEN_JDL_LPMPRODUCTIONTAG")
        grid_user_name = os.getenv("JALIEN_USER")
        mcprod_ccdb_server = os.getenv("PUBLISH_MCPRODINFO_CCDBSERVER")
        if mcprod_ccdb_server == None:
            mcprod_ccdb_server = "https://alice-ccdb.cern.ch"
        if prod_tag != None and grid_user_name != None:
            info = MCProdInfo(LPMProductionTag = prod_tag,
                              Col = ColSystem,
                              IntRate = rate,
                              RunNumber = args.run_number,
                              OrbitsPerTF = GLOparams["OrbitsPerTF"])
            publish_MCProdInfo(info, username = grid_user_name, ccdb_url = mcprod_ccdb_server)
        else:
            print("No production tag or GRID user name available. Not publishing MCProdInfo")

---

parser.add_argument("--run-number", type=int, help="Specify the run number to anchor the simulation to", required=True)
parser.add_argument("--ccdb-url", dest="ccdb_url", help="Provide the URL for accessing the CCDB", default="http://alice-ccdb.cern.ch")
parser.add_argument("--prod-split", type=int, help="Set the number of MC jobs to sample from the given time range", default=1)
parser.add_argument("--cycle", type=int, help="Define the MC cycle, which determines the sampling offset", default=0)
parser.add_argument("--split-id", type=int, help="Identify the split id of this job within the entire production (--prod-split)", default=0)
parser.add_argument("-tf", type=int, help="Specify the number of timeframes per job", default=1)
parser.add_argument("--ccdb-IRate", type=bool, help="Indicate whether to attempt fetching IRate from CCDB/CTP", default=True)
parser.add_argument("--trig-eff", type=float, dest="trig_eff", help="Set the trigger efficiency needed for IR (default is automatic mode)", default=-1.0)

---

def fetch_MinBias_CTPScaler_Rate(ctpscaler, finaltime, trig_eff_arg, NBunches, ColSystem, eCM):
    """
    fetches the CTP scaler object for a specified timestamp
    and determines the interaction rate for Monte Carlo digitizers.
    Utilizes trig_eff_arg if it is positive, otherwise computes the effTrigger.
    """
    trigger_efficiencies = {
        "pp": {
            "1000": 0.68,
            "6000": 0.737,
            "default": 0.759
        },
        "pO": {
            "default": 0.8222
        },
        "Op": {
            "default": 0.8222
        },
        "OO": {
            "default": 0.8677
        },
        "NeNe": {
            "default": 0.9147
        },
        "PbPb": {
            "default": 28.0  # this is ZDC
        }
    }

---

if job_is_exluded:
  print ("TIMESTAMP IS EXCLUDED IN RUN")
else:
  print ("Creating time-anchored workflow...")
  print ("Executing: " + cmd)
  try:
    cmd_list = shlex.split(os.path.expandvars(cmd))
    output = subprocess.check_output(cmd_list, text=True, stdin=subprocess.DEVNULL, timeout=120)
    print (output)

---

print(f"This run represents globally {total_excluded_fraction} of its data marked for exclusion.")
return excluded

def publish_MCProdInfo(mc_prod_info, ccdb_url="https://alice-ccdb.cern.ch", username="aliprod", include_meta_into_aod=False):
   print("Publishing MCProdInfo")

   # check if metadata is already uploaded, otherwise do nothing
   mc_prod_info_q = query_mcprodinfo(ccdb_url, username, mc_prod_info.RunNumber, mc_prod_info.LPMProductionTag)
   if mc_prod_info_q is None:
    # could extend this to depend on hash values in the future
    upload_mcprodinfo_meta(ccdb_url, username, mc_prod_info.RunNumber, mc_prod_info.LPMProductionTag, dataclasses.asdict(mc_prod_info))

def main():
    parser = argparse.ArgumentParser(description='Generates an O2DPG simulation workflow, anchored to a specified LHC run. The workflows are synchronized at regular intervals within a run based on production size, split-id, and cycle.')

---

parser.add_argument("--run-time-span-file", type=str, dest="run_span_file", help="A file specifying run-time spans for excluding timestamps (bad data periods, etc.)", default="")
parser.add_argument("--invert-irframe-selection", action='store_true', help="Reverses the logic applied by --run-time-span-file")
parser.add_argument("--orbitsPerTF", type=str, help="Forces a specific number of orbits per timeframe; defaults to auto-fetching from CCDB if not specified.", default="")
parser.add_argument('--publish-mcprodinfo', action='store_true', default=False, help="Publishes MCProdInfo metadata to CCDB")
parser.add_argument('forward', nargs=argparse.REMAINDER) # forwards any remaining arguments to the actual workflow creation
args = parser.parse_args()
print(args)

---

if the number of columns is less than 3:
                continue  # Skip lines with insufficient columns

            try:
                # Extract the first three columns as numbers
                num1, num2, num3 = map(int, columns[:3])  # Assuming integers in the data
                comment = columns[3] if len(columns) > 3 else ""
                parsed_data.append({"Run": num1, "From": num2, "To": num3, "Message": comment})
            except ValueError:
                continue  # Skip lines where the first three columns are not numeric
      return parsed_data

    data = parse_file(filename)
    # print (data)
    df = pd.DataFrame(data) # convert to data frame for easy handling

    # extract data for this run number
    filtered = df[df['Run'] == run]

    # now extract from and to lists
    exclude_list =  list(zip(filtered["From"].to_list() , filtered["To"].to_list()))

---

# search for the initial ID and validity token
ID=None
VALIDITY=None
for t in tokens:
  if "ID:" in t:
    ID=t.split(":")[1]
  if "Validity:" in t:
    VALIDITY=t.split(":")[1]
  if ID is not None and VALIDITY is not None:
    break

assert(ID is not None)
assert(VALIDITY is not None)

match_object=re.match("\s*([0-9]*)\s*-\s*([0-9]*)\s*.*", VALIDITY)
SOV = -1  # start of object validity, not necessarily coinciding with the actual run start
if match_object:
  SOV=match_object.group(1)

# generate an appropriate request at the start time, which provides us with the actual object, allowing us to determine its end time as well
grp=retrieve_CCDBObject_asJSON(ccdbreader, "/GLO/Config/GRPECS" + "/runNumber=" + str(run_number) + "/", int(SOV))

# ensure that this object corresponds to the intended run based on the run number
assert(int(grp["mRun"]) == int(run_number))

---

print("Excluding the timestamp ", str(ts))
excluded = True

---

DOCUMENT:
    forwardargs = " ".join([arg for arg in args.forward if arg != '--'])
    # retrieve the interaction rate
    rate = None
    ctp_local_rate_raw = None

    if args.ccdb_IRate:
       rate, ctp_local_rate_raw = retrieve_MinBias_CTPScaler_Rate(ctp_scalers, timestamp/1000., args.trig_eff, grplhcif.getBunchFilling().getNBunches(), ColSystem, eCM)

       if rate is not None:
         # if the rate was successfully calculated, we use it; otherwise, we use a rate from args.forward
         # a regular expression to match "interactionRate" followed by an integer
         pattern = r"-interactionRate\s+\d+"
         # use re.sub() to remove the pattern
         forwardargs = re.sub(pattern, " ", forwardargs)
         forwardargs += ' -interactionRate ' + str(int(rate))
       if ctp_local_rate_raw is not None:
         forwardargs += ' --ctp-scaler ' + str(ctp_local_rate_raw)

---

# this is the default setting for pp collisions
ctpclass = 0 # <---- we use the scaler from FT0
ctptype = 1
# this is the default for PbPb collisions
if ColSystem == "PbPb":
  ctpclass = 25  # <--- we use scalers from ZDC
  ctptype = 7
print(f"Fetching rate at time {finaltime} with class {ctpclass} and type {ctptype}")
rate = ctpscaler.getRateGivenT(finaltime, ctpclass, ctptype)

print(f"Global rate: {rate.first}, local rate: {rate.second}")
ctp_local_rate_raw = None
if rate.second >= 0:
  ctp_local_rate_raw = rate.second
if rate.first >= 0:
  # calculate the true rate (based on input from Chiara Zampolli) using the number of bunches
  coll_bunches = NBunches
  mu = - math.log(1. - rate.second / 11245 / coll_bunches) / effTrigger
  finalRate = coll_bunches * mu * 11245
  return finalRate, ctp_local_rate_raw

print(f"[ERROR]: Could not determine interaction rate; using some (external) default values")
return None, None

---

HBF_per_timeframe: int
            number of orbits per timeframe
    Returns:
        int: timestamp in milliseconds
        int: production offset, also known as "which timeslot in this production to simulate"
    """
    totaljobs = splitinfo[1]
    thisjobID = splitinfo[0]
    # duration of this run in microseconds, as we utilize the orbit duration in microseconds
    time_length_inmus = 1000 * (eor - sor)