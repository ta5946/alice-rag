## Metadata

**Document link:** https://github.com/AliceO2Group/simulation/blob/main/docs/o2dpgworkflow/README.md

**Start chunk id:** 0b7f1d2ad3c5eab10abe387878765f0783fba8170c728f7e7fb7cab6cb6c2b9c

## Content

1. If corrupt CCDB objects are an issue, it may be a real problem, but a hickup could lead to a corrupted object being saved in the local CCDB snapshot, typically located in a hidden `.ccdb` directory.
1. Try deleting the problematic snapshot and then rerunning the process from your current position.
1. Completely remove all files, including the `.ccdb` directory, as simply using `rm -r *` will not delete the hidden directory.
1. If the problem continues and you believe it to be genuine, please contact support.

### When a workflow run hangs

1. Ensure O2 and O2DPG are compatible versions. Although O2DPG consists primarily of scripts that utilize or execute O2 code, certain O2 features (like specific arguments for certain executables) might be incompatible between the versions of O2 and O2DPG you are using.
1. Terminate the hung task and review the log files (refer to [above](#what-are-the-interesting-log-files) for more details).

---

BY DEFAULT, THE SIMULATION WORKFLOW DESCRIPTION IS STORED IN `workflow.json`.

### Event Pools

To create a workflow for event pools, use the `--make-evtpool` flag.
```bash
${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -gen <generator> -eCM <emc energy [GeV]> -tf <nTFs> --ns <nEvents> --make-evtpool
```
This command will bypass all steps following signal generation (no transport), set the beam-spot vertex to kNoVertex, and include a final step named `poolmerge` to merge all `Kine.root` files produced for the `nTFs` timeframes into a single `evtpool.root` file.

---

## Workflow Execution

The basic command to run the workflow is
```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json
```
However, there are several useful command-line options available:
- `-tt <regex>` to specify the "target task" until which the workflow should be executed,
- `--mem-limit <mem limit [MB]` to ensure the memory limit is not exceeded by only running tasks that fit within the limit,
- `--cpu-limit <number of CPUs>` to limit the CPU usage by only running tasks that fit within the CPU limit,
- `--rerun-from <regex>` to instruct the runner to start from tasks whose names match the `<regex>` pattern,
- `--target-labels <list> <of> <target> <labels>` to run all tasks that have at least one of the specified labels.

The runner is designed to execute as many tasks in parallel as possible. Initially, the transport process may take some time, but soon your terminal will display numerous tasks running on top of the simulation results.

---

There is a specific scenario where the detector transport fails, and in that case, the log file reported would be `tf<i>/sgnsim_<i>.log` or `bkgsim.log`. Given that the detector simulation runs multiple workers, there are extra log files where the workers' outputs are directed. These include:
1. `tf<i>/sgn_<i>_workerlog0`,
2. `tf<i>/sgn_<i>_serverlog`,
3. `tf<i>/sgn_<i>_mergerlog`.

### In the Event of a Workflow Crash

---

## Troubleshooting

### Identifying Relevant Log Files

When a specific stage in the workflow encounters an error and the workflow runner exits with a non-zero code, as in the example:
```bash
command ${O2_ROOT}/bin/o2-sim-digitizer-workflow -b --run --condition-not-after 3385078236000 -n 1 --sims sgn_1 --onlyDet FT0,FV0,CTP --interactionRate 50000 --incontext collisioncontext.root --disable-write-ini --configKeyValues "HBFUtils.orbitFirstSampled=0;HBFUtils.nHBFPerTF=128;HBFUtils.orbitFirst=0;HBFUtils.runNumber=310000;HBFUtils.startTime=1550600800000" --combine-devices had a non-zero exit code 1
Stop on failure  True
setting up ROOT system
ft0fv0ctp_digi_1 failed ... checking retry
```
The `<i>` in the suffix `_<i>` indicates the timeframe of the issue, so the log file to focus on would be `tf1/ft0fv0ctp_digi_1.log`.

---

- Use -gen extkinO2 with an interactionRate of 500000
# Then run
${O2DPG_ROOT}/MC/bin/o2dpg_workflow_runner.py -f workflow.json -tt aod
```
To disable event randomisation, the user can edit the JSON file manually. A complete example of event pools is available in the [${O2DPG_ROOT}/MC/run/examples/event_pool.sh](https://github.com/AliceO2Group/O2DPG/blob/master/MC/run/examples/event_pool.sh) script (refer to the `--help` flag or the source code for instructions).
```

---

### When a workflow run encounters issues

1. Verify that O2 and O2DPG are compatible. Even though O2DPG includes mostly scripts that **run** O2 code, some O2 features (like specific arguments for certain executables) might be incompatible between the versions of O2 and O2DPG you are using.
1. If you have a custom local installation of the software but still expect the workflow to run successfully, then
    1. ensure the integrity of your installation,
    1. possibly update your installation (not just the development packages, but also `alidist`),
    1. execute the workflow using a version from `cvmfs` (available on `lxplus`),
    1. try a different machine or working environment (for instance, `lxplus` could be suitable if the workflow is not resource-intensive).

---

When working with event pools, it's important to adhere to certain guidelines, especially when creating an event cache. The workflow runner needs to be configured to reach the poolmerge step, as in:
```bash
${O2DPG_ROOT}/MC/bin/o2dpg_workflow_runner.py -f workflow.json -tt pool
```
Alternatively, when incorporating the pool into an O2DPG workflow (using `extkinO2` as the generator), users should be aware that by default, events will be randomized (using the same seed within each timeframe), but phi randomization is not active by default.
To run a workflow that includes phi angle random rotation, the command should be:
```bash
${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -eCM <emc energy  [GeV]> -gen extkinO2 -tf <nTFs> --ns <nEvents>
                                           -confKey "GeneratorFromO2Kine.randomphi=true;GeneratorFromO2Kine.fileName=/path/to/file/filename.root"
                                           -gen extkinO2 -interactionRate 500000
```

---

The initial script generates a workflow tree in JSON format, detailing the steps and dependencies from simulation through to the final AOD creation. The second component is the runtime engine, which handles the execution of this workflow on a compute node.

Examples can be accessed at <https://gitlab.cern.ch/aliperf/alibibenchtasks>. **N.B.** These tasks are executed nightly on a test machine, ensuring these scripts remain continuously updated.

For best results, it is recommended to build and load the `O2sim` environment via `alienv`.

## Prerequisites

While not all use cases can be covered here, a few key points and useful pointers are summarized below.

---

---
sort: 4
title: Run3 production setup - O2DPG MC Workflows
---

# Run3 production setup - O2DPG MC Workflows

The [O2DPG repository](https://github.com/AliceO2Group/O2DPG) supplies the definitive setup for official MC productions for ALICE-Run3, integrating all necessary simulation tasks into a unified and coherent framework. This framework delivers a comprehensive simulation pipeline, encompassing event generation, Geant transport, reconstruction, AOD creation, and running QC or analysis tasks.

Within O2DPG, the logic and configuration of a MC job are decoupled from the runtime engine responsible for executing the job on a compute node.

For this purpose, O2DPG offers two crucial scripts:
1. [o2dpg_sim_workflow.py](https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2dpg_sim_workflow.py),
2. [o2_dpg_workflow_runner.py](https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2_dpg_workflow_runner.py).

---

In addition, there is a small collection of analyses that can be incorporated into the workflow. These analyses are intended for testing and their configuration is not optimized for generating top-tier analysis outcomes. However, they can be used to verify that the AODs are generally in a sane state. To set up a workflow, use the `--include-analysis` option and then run the following command:

```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json --target-labels Analysis
```

## Modifying Resources

If you encounter the runner terminating with the message:
```bash
runtime error: Not able to make progress: Nothing scheduled although non-zero candidate set

**** Pipeline done with failures (global_runtime : 250.285s) *****
```

and your system has `16 GB` of RAM, you might need to artificially increase the memory limit for the runner by using the following command:

```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json -tt aod --mem-limit 18000
```

## Resolving Issues

### Identifying Useful Log Files

---

If you utilize `pythia8` as your generator **and do not** specify a process with `-proc <process>`, the simulation will fail due to a lack of necessary configuration (refer to [here](../generators/generatorso2.md#pythia8) for more details). Therefore, a valid configuration must be passed, which should be done as follows:
```bash
${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -gen pythia8 -eCM <emc energy [GeV]> -confKey "GeneratorPythia8.config=<path/to/config>"
```

Simulations executed via a workflow operate in timeframes, and the number of timeframes to simulate is defined with `-tf <nTFs>`. The number of events generated per timeframe can be specified with `-ns <nEvents>` (this is optional as it has a default value).

By default, the workflow description is saved to `workflow.json`.

### Event pools

---

### Embedding

The process can also be configured to simulate signal type events embedded within a specified type of background events. To activate this option, use the flag `--embedding`. For the background events, the following parameters are available:
* `-nb <nEvents>`: specifies the number of background events to simulate,
* `-genBkg <generator>`: identifies the generator used for background event generation,
* `-colBkg <system>`: defines the collision system for the background events,
* `-procBkg <proc>`: specifies the process for background generation (only applicable if the background generator is Pythia8),
* `-confKeyBkg <confKeyValuePairs>`: allows additional key-value pairs to be passed to the background generation and transport steps.

---

{% include list.liquid all=true %}

---

The workflows demand a minimum of `16 GB` of RAM and an `8`-core processor. If your machine has precisely `16 GB` of RAM, consult [these guidelines](#adjusting-resources). To access CCDB objects/alien, you must have a valid GRID token. For setting this up, refer to https://alice-doc.github.io/alice-analysis-tutorial/start/cert.html.

## Workflow Setup

To run a workflow, use the following minimal command:
```bash
${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -gen <generator> -eCM <collision energy [GeV]>
# OR
${O2DPG_ROOT}/MC/bin/o2dpg_sim_workflow.py -gen <generator> -eA <energy of first beam [GeV]> -eB <energy of second beam [GeV]>
```
This command requires at least the beam energies and the generator to be specified.

---

Sometimes, you may wish to execute the entire chain to generate the final `AO2D.root` file. For this purpose, and to avoid running unnecessary tasks, use the `-tt aod` option. 

### Utilization of Event Pools

---

### Regarding Completed Tasks

After a task completes, it won't be rerun unless you delete the `<task_name>_log_done` file in the directory. If you delete that file and rerun, ensure all dependent tasks are correctly re-executed by using
```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json --rerun-from <task_name> -tt <your_target_task>
```
and you do not need to remove `<task_name>_log_done`.

Alternatively, you can rerun from a specific task and set it as the target task without deleting any files. Just run
```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json --rerun-from <task_name> -tt <task_name>
```

## Quality Control and Extra Steps

To include standard Quality Control (QC) steps in the workflow, use the `--include-qc` flag. To run all QC tasks, execute
```bash
${O2DPG_ROOT}/MC/bin/o2_dpg_workflow_runner.py -f workflow.json --target-labels QC
```