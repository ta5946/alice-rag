## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/utils/o2dpg_get_resource_estimates.py

**Start chunk id:** 4f88788cc7ecf1fa71186ba7678f89d4f4faefe27e3147df6a9d8dd6a89f5dac

## Content

DOCUMENT:
    estimate = finalize(resource_accum)
    print(estimate)

    # finally save to JSON
    with open(args.output, "w") as f:
        json.dump(estimate, f, indent=2)


def main():
    parser = argparse.ArgumentParser(description="Create a O2DPG workflow resource file from existing time logs")
    parser.add_argument('-o','--output', help='Filename of output metric json file', default='learned_O2DPG_resource_metrics.json')
    parser.add_argument('-p','--path', help='Path to O2DPG workspace', default="./")
    args = parser.parse_args()
    process(args)

if __name__ == "__main__":
    sys.exit(main())

---

DOCUMENT:
    resource_accum = {}  # collects resources based on task names
    for f in files:
        # extract task name from the time log file
        name = f.split("/")[-1]
        name = re.sub("\.log_time$", "", name)
        name_notf = name.split("_")[0]
        resources = extract_time_single(f)
        resources["name"] = name
        resource_accum[name_notf] = resource_accum.get(name_notf, [])
        resource_accum[name_notf].append(resources)

---

# This function retrieves essential metrics from a single resource file (generated by O2 jobutils/taskwrapper).
def extract_time_single(path):
  r = {}
  with open(path, "r") as f:
    for line in f:
      if "walltime" in line:
        r["walltime"] = float(line.strip().split()[-1])
      elif "CPU" in line:
        r["cpu"] = float(line.strip().split()[-1].split('%')[0])
      elif "mem" in line:
        r["mem"] = float(line.strip().split()[-1])
  return r

def process(args):
  pipeline_dir = dirname(args.path)
  files = find_files(pipeline_dir, "*.log_time", 1)
  if not files:
      print(f"WARNING: Unable to locate time logs in {pipeline_dir}.")
      return

---

# finalizes metrics; average for CPU; maximum for MEM; average for walltime
# returns final metric result suitable for output to json
def finalize(resource_list):
    """
    input is a map of lists of resource dictionaries
    output is a map of final resource estimates
    """
    result = {}
    for task in resource_list:
        finalr = {"walltime": 0, "cpu": 0, "mem": 0}
        for r in resource_list[task]:  # r is a resource estimate
            finalr["walltime"] += r["walltime"]
            finalr["mem"] = max(r["mem"], finalr["mem"])
            finalr["cpu"] += r["cpu"]
        finalr["walltime"] /= len(resource_list[task])
        finalr["cpu"] /= 100 * len(resource_list[task])  # we use the number of CPUs, not the percentage
        finalr["mem"] /= 1024  # we prefer the value in MB
        finalr["mem"] = math.ceil(finalr["mem"])
        result[task] = finalr
    return result

estimate = finalize(resource_accum)
print(estimate)

---

#!/usr/bin/env python3

import sys
from os.path import dirname
import argparse
import re
from glob import glob
import json
import math

################################################################
#                                                              #
# Script designed to extract CPU/MEM resource estimates from   #
# the log_time files generated by individual tasks.            #
#                                                              #
# Outputs a JSON file for use in dynamic scheduling decisions. #
################################################################

# helper function for locating files
def locate_files(path, search, depth=0):
  files = []
  for d in range(depth + 1):
    wildcards = "/*" * d
    path_search = path + wildcards + f"/{search}"
    files.extend(glob(path_search))
  return files