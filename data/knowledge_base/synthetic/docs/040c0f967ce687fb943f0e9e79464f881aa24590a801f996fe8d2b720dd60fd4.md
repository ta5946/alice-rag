## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2_dpg_workflow_runner.py

**Start chunk id:** 040c0f967ce687fb943f0e9e79464f881aa24590a801f996fe8d2b720dd60fd4

## Content

DOCUMENT:
    self.resources.append(resources)
        # perform these steps to ensure the same Semaphore object is used for all matching TaskResources, thereby avoiding a lookup
        if semaphore_string:
            if semaphore_string not in self.semaphore_dict:
                self.semaphore_dict[semaphore_string] = Semaphore()
            resources.semaphore = self.semaphore_dict[semaphore_string]

---

DOCUMENT:
    taskenv = os.environ.copy()
      # apply specific (non-default) software version, if any
      # (this was set previously)
      alternative_env = self.alternative_envs.get(tid, None)
      if alternative_env is not None and len(alternative_env) > 0:
          actionlogger.info('Applying an alternative software environment to task ' + self.idtotask[tid])
          if alternative_env.get('TERM') is not None:
              # the environment is a complete environment
              taskenv = {}
              taskenv = alternative_env
          else:
            for entry in alternative_env:
              # overwrite existing entries
              taskenv[entry] = alternative_env[entry]

      # include task-specific environment
      if self.workflowspec['stages'][tid].get('env') != None:
          taskenv.update(self.workflowspec['stages'][tid]['env'])

---

class TaskResources:
    """
    Container holding the resources of a single task
    """
    def __init__(self, tid, name, cpu, cpu_relative, mem, resource_boundaries):
        # the task ID associated with these resources
        self.tid = tid
        self.name = name
        # original CPU and memory assigned (permanent)
        self.cpu_assigned_original = cpu
        self.mem_assigned_original = mem
        # relative CPU multiplier, to be applied to sampled CPU; set by the user, for instance, to allow backfilling tasks
        # only applies when sampling resources; permanent
        self.cpu_relative = cpu_relative if cpu_relative else 1
        # CPU and memory assigned (temporary)
        self.cpu_assigned = cpu
        self.mem_assigned = mem
        # global resource configuration
        self.resource_boundaries = resource_boundaries
        # sampled resources of this task
        self.cpu_sampled = None
        self.mem_sampled = None

---

for tid in taskids:
             taskspec = self.workflowspec['stages'][tid]
             name = taskspec['name']
             readmefile.write('A checkpoint was created due to a failure in task ' + name + '\n')
             readmefile.write('To reproduce this checkpoint, follow these steps:\n')
             readmefile.write('a) Set up the necessary O2sim environment using alienv\n')
             readmefile.write('b) Execute: $O2DPG_ROOT/MC/bin/o2_dpg_workflow_runner.py -f workflow.json -tt ' + name + '$ --retry-on-failure 0\n')
           readmefile.close()

           # Firstly, the base directory
           os.system(tarcommand)

---

# Identifying New Candidates
for each tid in finished tasks:
    if there is a potential next task for tid:
        potential candidates are listed from self.possiblenexttask[tid]
        for each candidate:
            # verify if it's a valid candidate:
            if the candidate is suitable (using self.is_good_candidate) and it's not already in the candidates list:
                add the candidate to the list

actionlogger.debug("New candidates identified: " + str(candidates))
send_webhook(self.args.webhook, "New candidates identified: " + str(candidates))

---

# navigate back
    lines.append('cd $OLDPWD\n')


# generate a standalone bash script for running the workflow
    def produce_script(self, filename):
        # select one of the correct task sequences
        taskorder = self.topological_orderings[0]
        outF = open(filename, "w")

        lines=[]
        # header
        lines.append('#!/usr/bin/env bash\n')
        lines.append('#THIS FILE IS AUTOGENERATED\n')
        lines.append('export JOBUTILS_SKIPDONE=ON\n')

        # record the overall environment configuration
        # especially to capture the workflow's initialization
        lines.append('#-- GLOBAL INIT SECTION FROM WORKFLOW --\n')
        for e in self.globalinit['env']:
            lines.append('export ' + str(e) + '=' + str(self.globalinit['env'][e]) + '\n')
        lines.append('#-- TASKS FROM WORKFLOW --\n')
        for tid in taskorder:
            print ('Processing task ' + self.idtotask[tid])
            self.emit_code_for_task(tid, lines)

---

for ok_to_submit_impl, should_break in ((ok_to_submit_default, True), (ok_to_submit_backfill, False)):
            tid_index = 0
            while tid_index < len(tids_copy):
                tid = tids_copy[tid_index]
                res = self.resources[tid]

                actionlogger.info("Prepare resources for task %s, cpu: %f, mem: %f", res.name, res.cpu_assigned, res.mem_assigned)
                tid_index += 1

                if (res.semaphore is not None and res.semaphore.locked) or res.booked:
                    continue

                nice_value = ok_to_submit_impl(res)
                if nice_value is not None:
                    # if a non-None nice value is obtained, it indicates that this task is ready to submit
                    res.nice_value = nice_value
                    # yield the tid along with its assigned nice value
                    yield tid, nice_value

---

if args.cgroup is not None:
    myPID = os.getpid()
    # cgroups like /sys/fs/cgroup/cpuset/<cgroup-name>/tasks
    # or /sys/fs/cgroup/cpu/<cgroup-name>/tasks
    command = "echo " + str(myPID) + " > " + args.cgroup
    actionlogger.info(f"Attempting to run within cgroup {args.cgroup}")
    waitstatus = os.system(command)
    if code := os.waitstatus_to_exitcode(waitstatus):
        actionlogger.error(f"Failed to apply cgroup")
        exit(code)
    actionlogger.info("Running within cgroup")

executor = WorkflowExecutor(args.workflowfile, jmax=int(args.maxjobs), args=args)
exit(executor.execute())

---

DOCUMENT:
    okcache = {}
    # create complete target list
    full_target_list = [ t for t in workflowspec['stages'] if task_matches(t['name']) and task_matches_labels(t) and canBeDone(t,okcache) ]
    full_target_name_list = [ t['name'] for t in full_target_list ]

    # construct full dependency list for a task t
    def getallrequirements(t):
        _l = []
        for r in t['needs']:
            fulltask = workflowspec['stages'][tasknametoid[r]]
            _l.append(fulltask)
            _l += getallrequirements(fulltask)
        return _l

    full_requirements_list = [ getallrequirements(t) for t in full_target_list ]

    # flatten and retrieve names only
    full_requirements_name_list = list(set([ item['name'] for sublist in full_requirements_list for item in sublist ]))

---

def emit_code_for_task(self, tid, lines):
    actionlogger.debug("Submitting task " + str(self.idtotask[tid]))
    taskspec = self.workflowspec['stages'][tid]
    command = taskspec['cmd']
    working_directory = taskspec['cwd']
    environment = taskspec.get('env')
    # generally:
    # try to create directory
    lines.append('[ ! -d ' + working_directory + ' ] && mkdir ' + working_directory + '\n')
    # change directory
    lines.append('cd ' + working_directory + '\n')
    # set local environment variables
    if environment:
        for key, value in environment.items():
            lines.append('export ' + key + '=' + str(value) + '\n')
    # execute command
    lines.append(command + '\n')
    # unset local environment variables
    if environment:
        for key, value in environment.items():
            lines.append('unset ' + key + '\n')

    # return to previous directory
    lines.append('cd $OLDPWD\n')

---

# second file logger setup
metriclogger = setup_logger('pipeline_metric_logger', ('pipeline_metric_' + str(os.getpid()) + '.log', args.action_logfile)[args.action_logfile is not None])

# Log immediately the enforced memory and CPU limits along with additional relevant metadata
_ , meta = read_workflow(args.workflowfile)
meta["cpu_limit"] = args.cpu_limit
meta["mem_limit"] = args.mem_limit
meta["workflow_file"] = os.path.abspath(args.workflowfile)
meta["target_task"] = args.target_tasks
meta["rerun_from"] = args.rerun_from
meta["target_labels"] = args.target_labels
metriclogger.info(meta)

# For debugging without terminal access
# TODO: integrate into standard logger
def send_webhook(hook, t):
    if hook != None:
      command="curl -X POST -H 'Content-type: application/json' --data '{\"text\":\" " + str(t) + "\"}' " + str(hook) + " &> /dev/null"
      os.system(command)

---

# An alternative method to retrieve all child processes
# when psutil encounters issues (like PermissionError).
# This function provides a list identical to what psutil.children(recursive=True) would return.
def getChildProcs(basepid):
  cmd='''
  childprocs() {
  local parent=$1
  if [ ! "$2" ]; then
    child_pid_list=""
  fi
  if [ "$parent" ] ; then
    child_pid_list="$child_pid_list $parent"
    for childpid in $(pgrep -P ${parent}); do
      childprocs $childpid "nottoplevel"
    done;
  fi
  # return via a string list (only if toplevel)
  if [ ! "$2" ]; then
    echo "${child_pid_list}"
  fi
  }
  '''
  cmd += '\n' + 'childprocs ' + str(basepid)
  output = subprocess.check_output(cmd, shell=True)
  plist = []
  for p in output.strip().split():
     try:
         proc=psutil.Process(int(p))
     except psutil.NoSuchProcess:
         continue

     plist.append(proc)
  return plist

---

if os.environ.get('ROOT_LDSYSPATH') != None and os.environ.get('ROOT_CPPSYSINCL') != None:
                  # do nothing if already set
                  return

               # a) the PATH for system libraries
               # search adapted from ROOT TUnixSystem
               cmd='LD_DEBUG=libs LD_PRELOAD=DOESNOTEXIST ls /tmp/DOESNOTEXIST 2>&1 | grep -m 1 "system search path" | sed \'s/.*=//g\' | awk \'//{print $1}\''
               proc = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
               libpath, err = proc.communicate()
               if not (args.no_rootinit_speedup == True):
                  print ("initializing ROOT system")
                  os.environ['ROOT_LDSYSPATH'] = libpath.decode()

---

# This sequence of operations is structured and functional.
# However, it creates lookups used elsewhere, so some CPU time might be conserved by sharing some structures between functions or reducing the number of data passes.

# helper lookup
tasknametoid = { t['name']:i for i, t in enumerate(workflowspec['stages'],0) }

# verify if a task can be executed
# or if it cannot due to missing prerequisites
def canBeDone(t, cache={}):
    ok = True
    c = cache.get(t['name'])
    if c is not None:
        return c
    for r in t['needs']:
        taskid = tasknametoid.get(r)
        if taskid is not None:
            if not canBeDone(workflowspec['stages'][taskid], cache):
                ok = False
                break
        else:
            ok = False
            break
    cache[t['name']] = ok
    return ok

---

if globalPSS exceeds self.resource_manager.resource_boundaries.mem_limit:
            metriclogger.info('*** MEMORY LIMIT EXCEEDED !! ***')
            # --> This could be utilized for corrective actions like terminating ongoing jobs
            # (or more appropriately, hibernating them)

    def waitforany(self, process_list, finished, failingtasks):
       """
       Cycle through all submitted tasks to check their completion status

       1. If the process is still running, take no action
       2. If the process has completed, retrieve its return value, update the finished and failingtasks lists
       2.1 release resources
       2.2 reclaim utilized resources and hand them over to the ResourceManager
       """
       failuredetected = False
       failingpids = []
       if len(process_list) == 0:
           return False

---

Explanation: This often occurs when the **PREDICTED** resource needs for certain tasks in the workflow surpass the available CPU cores or memory (as explicitly or implicitly set via the --cpu-limit and --mem-limit options). It can be particularly common on laptops with <=16GB of RAM when one of the tasks requires around 16GB. To address this, one can instruct the scheduler to use a slightly higher memory limit with an explicit --mem-limit option (for example, `--mem-limit 20000` to set it to 20GB). This could be effective if the **ACTUAL** resource usage is lower than anticipated (as only small test cases are being run).

---

if mem_sampled > self.resource_boundaries.mem_limit:
        actionlogger.warning("Sampled memory (%.2f) exceeds the assigned memory limit (%.2f)", mem_sampled, self.resource_boundaries.mem_limit)
elif mem_sampled <= 0:
    actionlogger.debug("Sampled memory for %s is %.2f, which is less than or equal to zero; setting it to the previously assigned value %.2f", self.name, mem_sampled, self.mem_assigned)
    mem_sampled = self.mem_assigned

for resource in self.related_tasks:
    if resource.is_done or resource.booked:
        continue
    resource.cpu_assigned = cpu_sampled * resource.cpu_relative
    resource.mem_assigned = mem_sampled
    # If this task has been run before, maintain an optimistic approach and limit the resources if the sampled values exceed the limits
    resource.limit_resources()


class ResourceManager:
    """
    Central class for managing resources

    - CPU limits
    - Memory limits
    - Semaphores

    Provides an entry point for setting and querying resources to be updated.

    """

---

# accumulate total metrics (CPU, memory)
totalCPU = 0.
totalPSS = 0.
totalSWAP = 0.
totalUSS = 0.
for p in psutilProcs:
    """
    try:
        for f in p.open_files():
            self.pid_to_files[pid].add(str(f.path)+'_'+str(f.mode))
        for f in p.connections(kind="all"):
            remote=f.raddr
            if remote==None:
                remote='none'
            self.pid_to_connections[pid].add(str(f.type)+"_"+str(f.laddr)+"_"+str(remote))
    except Exception:
        pass
    """
    thispss=0
    thisuss=0
    # MEMORY part
    try:
        fullmem=p.memory_full_info()
        thispss=getattr(fullmem,'pss',0) #<-- pss not available on MacOS

---

DOCUMENT:
    def limit_resources(self, cpu_limit=None, mem_limit=None):
        """
        Set resource limits for this task
        """
        if cpu_limit is None:
            cpu_limit = self.resource_boundaries.cpu_limit
        if mem_limit is None:
            mem_limit = self.resource_boundaries.mem_limit
        self.cpu_assigned = min(self.cpu_assigned, cpu_limit)
        self.mem_assigned = min(self.mem_assigned, mem_limit)

    def add(self, time_passed, cpu, mem):
        """
        Interface for adding measured resource data after a certain time
        """
        self.time_collect.append(time_passed)
        self.cpu_collect.append(cpu)
        self.mem_collect.append(mem)

    def sample_resources(self):
        """
        If the task is complete, sample CPU and MEM usage for tasks that haven't started yet
        """
        if not self.is_done:
            return

---

def ok_to_submit_backfill(res, backfill_cpu_factor=1.5, backfill_mem_factor=1.5):
    """
    Return the backfill nice value if the conditions are satisfied, otherwise return None.
    """
    if self.n_procs_backfill >= args.n_backfill:
        return None

    if res.cpu_assigned > 0.9 * self.resource_boundaries.cpu_limit or res.mem_assigned / self.resource_boundaries.cpu_limit >= 1900:
        return None

---

# remove the original file
os.remove(logf)
os.remove(donef)
os.remove(timef)

# output an error message if no progress can be made
def noprogress_errormsg(self):
    # TODO: instead of repeating this, link to the documentation for further details
    msg = """Scheduler runtime error: The scheduler cannot proceed given a non-empty candidate set.

"""

---

if the length of self.time_collect is less than 3:
    # Need at least 3 points to sample resources
    self.cpu_sampled = self.cpu_assigned
    self.mem_sampled = self.mem_assigned
    actionlogger.debug("Task %s does not have enough points (< 3) to sample resources, setting to previously assigned values.", self.name)
else:
    # Calculate time deltas and exclude the first CPU measurement, which may be less meaningful,
    # particularly if it comes from psutil.Proc.cpu_percent(interval=None)
    time_deltas = [self.time_collect[i+1] - self.time_collect[i] for i in range(len(self.time_collect) - 1)]
    cpu = sum([cpu * time_delta for cpu, time_delta in zip(self.cpu_collect[1:], time_deltas) if cpu >= 0])
    self.cpu_sampled = cpu / sum(time_deltas)
    self.mem_sampled = max(self.mem_collect)

---

@[property]
def is_complete(self):
    return self.time_collection and not self.booked

def validate_resource_limits(self):
    """
    Verify that allocated resources stay within set constraints
    """
    cpu_limit_adhered = True
    mem_limit_adhered = True
    if self.cpu_assigned > self.resource_boundaries.cpu_limit:
        cpu_limit_adhered = False
        actionlogger.warning("CPU usage for task %s surpasses limit: %d > %d", self.name, self.cpu_assigned, self.resource_boundaries.cpu_limit)
    if self.cpu_assigned > self.resource_boundaries.mem_limit:
        mem_limit_adhered = False
        actionlogger.warning("Memory usage for task %s surpasses limit: %d > %d", self.name, self.cpu_assigned, self.resource_boundaries.cpu_limit)
    return cpu_limit_adhered and mem_limit_adhered

---

# build the DAG and calculate task weights
    workflow = build_dag_properties(self.workflowspec)
    if args.visualize_workflow:
        draw_workflow(self.workflowspec)
    self.possiblenexttask = workflow['nexttasks']
    self.taskweights = workflow['weights']
    self.topological_orderings = workflow['topological_ordering']
    self.taskuniverse = [ l['name'] for l in self.workflowspec['stages'] ]
    # create a task ID to task name lookup
    self.idtotask = [ 0 for _ in self.taskuniverse ]
    self.tasktoid = {}
    for i, name in enumerate(self.taskuniverse):
        self.tasktoid[name]=i
        self.idtotask[i]=name

    if args.update_resources:
        update_resource_estimates(self.workflowspec, args.update_resources)

---

# the trivial cases do nothing
if packagestring is None or packagestring == "" or packagestring == "None":
    return {}

def transform_env_file(env_file):
    """Convert an environment file created using 'export > env.txt' into a Python dictionary."""
    env_vars = {}
    with open(env_file, "r") as f:
        for line in f:
            line = line.strip()

            # Ignore empty lines or comments
            if not line or line.startswith("#"):
                continue

            # Remove 'declare -x ' if it exists
            if line.startswith("declare -x "):
                line = line.replace("declare -x ", "", 1)

            # Handle cases where "FOO" equals nothing (assign an empty string)
            if "=" not in line:
                key, value = line.strip(), ""
            else:
                key, value = line.split("=", 1)
                value = value.strip('"')  # Remove any surrounding quotes if present

---

#!/usr/bin/env python3

# Initiated in February 2021, by sandro.wenzel@cern.ch

import re
import subprocess
import time
import json
import logging
import os
import signal
import socket
import sys
import traceback
import platform
import tarfile
try:
    from graphviz import Digraph
    havegraphviz=True
except ImportError:
    havegraphviz=False

formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')

sys.setrecursionlimit(100000)

import argparse
import psutil
max_system_mem=psutil.virtual_memory().total

sys.path.append(os.path.join(os.path.dirname(__file__), '.', 'o2dpg_workflow_utils'))
from o2dpg_workflow_utils import read_workflow

# specifying command line options
parser = argparse.ArgumentParser(description='Parallel execution of an (O2-DPG) Directed Acyclic Graph data/job pipeline under resource constraints.',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)

---

# envfilename = "taskenv_" + str(tid) + ".json"
      # with open(envfilename, "w") as file:
      #    json.dump(taskenv, file, indent=2)

      p = psutil.Popen(['/bin/bash','-c',c], cwd=workdir, env=taskenv)
      try:
          p.nice(nice)
      except (psutil.NoSuchProcess, psutil.AccessDenied):
          actionlogger.error('Failed to set nice value of ' + str(p.pid) + ' to ' + str(nice))

      return p

    def ok_to_skip(self, tid):
        """
        Determine if a task can be skipped based on the existence of <task>.log_done
        """
        done_filename = self.get_done_filename(tid)
        if os.path.exists(done_filename) and os.path.isfile(done_filename):
          return True
        return False

    def try_job_from_candidates(self, taskcandidates, finished):
       """
       Attempt to schedule the next tasks
       """

---

else:
    failuredetected = True
    failingpids.append(pid)
    failingtasks.append(tid)

if failuredetected and self.stoponfailure:
    actionlogger.info('Stopping pipeline due to failure in stages with PID ' + str(failingpids))
    # self.analyse_files_and_connections()
    if self.args.stdout_on_failure:
        self.cat_logfiles_tostdout(failingtasks)
    self.send_checkpoint(failingtasks, self.args.checkpoint_on_failure)
    self.stop_pipeline_and_exit(process_list)

# empty finished means we need to wait more
return len(finished)==0

---

_, alive = psutil.wait_procs(procs, timeout=3)
for p in alive:
    try:
        actionlogger.info("Terminating " + str(p))
        p.terminate()
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

exit(1)

def extract_global_environment(self, workflowspec):
    """
    Verifies if the workflow includes a specific initialization task that sets a global environment.
    Extract the relevant environment information and remove it from workflowspec.
    """
    init_index = 0 # this must be the first task in the workflow
    globalenv = {}
    initcmd = None
    if workflowspec['stages'][init_index]['name'] == '__global_init_task__':
        env = workflowspec['stages'][init_index].get('env', None)
        if env is not None:
            globalenv = { e : env[e] for e in env }
        cmd = workflowspec['stages'][init_index].get('cmd', None)
        if cmd != 'NO-COMMAND':
            initcmd = cmd

---

# Analyze CPU
    okcpu = (self.cpu_booked_backfill + res.cpu_assigned <= self.resource_boundaries.cpu_limit)
    okcpu = okcpu and (self.cpu_booked + self.cpu_booked_backfill + res.cpu_assigned <= backfill_cpu_factor * self.resource_boundaries.cpu_limit)
    # Analyze MEM
    okmem = (self.mem_booked + self.mem_booked_backfill + res.mem_assigned <= backfill_mem_factor * self.resource_boundaries.mem_limit)
    actionlogger.debug ('Condition check --backfill-- for  ' + str(res.tid) + ':' + res.name + ' CPU ' + str(okcpu) + ' MEM ' + str(okmem))

    return self.nice_backfill if (okcpu and okmem) else None

if self.n_procs + self.n_procs_backfill >= self.procs_parallel_max:
    # In this scenario, no action can be taken
    return

---

for tid, proc in process_list:

    # proc is a Popen object
    pid = proc.pid
    if self.pid_to_files.get(pid) is None:
        self.pid_to_files[pid] = set()
        self.pid_to_connections[pid] = set()
    try:
        psutilProcs = [proc]
        # use psutil for CPU measurement
        psutilProcs += proc.children(recursive=True)
    except psutil.NoSuchProcess:
        continue

    except (psutil.AccessDenied, PermissionError):
        psutilProcs += getChildProcs(pid)

---

DOCUMENT:
    # resets the done status for tasks that need to be executed again
    def reset_done_status(self, listoftaskids):
       """
       Reset the <task>.log_done files for the specified task IDs
       """
       for tid in listoftaskids:
          done_filename = self.get_done_filename(tid)
          task_name=self.workflowspec['stages'][tid]['name']
          if args.dry_run:
              print ("Would reset task " + task_name + " to be done again")
          else:
              print ("Resetting task " + task_name + " to be done again")
              if os.path.exists(done_filename) and os.path.isfile(done_filename):
                  os.remove(done_filename)

    # launches a task as a subprocess and keeps track of the Popen instance
    def execute(self, tid, nice):
      """
      Execute a task

---

DOCUMENT:
    if (len(inters)>0):
        print ('FILE Intersection ' + str(p1) + ' ' + str(p2) + ' ' + str(inters))
    # check for intersections
    for p1, s1 in self.pid_to_connections.items():
        for p2, s2 in self.pid_to_connections.items():
            if p1!=p2:
                if type(s1) is set and type(s2) is set:
                    if len(s1)>0 and len(s2)>0:
                        try:
                            inters = s1.intersection(s2)
                        except Exception:
                            print ('Exception during intersect inner')
                            pass
                        if (len(inters)>0):
                            print ('CON Intersection ' + str(p1) + ' ' ' + str(p2) + ' ' + str(inters))

---

# To ensure each TaskResources has a list of related tasks, thereby eliminating the need for an additional lookup:
    if related_tasks_name exists:
        if related_tasks_name is not already in self.resources_related_tasks_dict:
            # Initialize the assigned list as [valid to use, list of CPU, list of MEM, list of walltimes, list of parallel processes, list of used CPUs, list of assigned CPUs, list of tasks finished in the meantime]
            self.resources_related_tasks_dict[related_tasks_name] = []
        append resources to self.resources_related_tasks_dict[related_tasks_name]
        set resources.related_tasks to self.resources_related_tasks_dict[related_tasks_name]

def add_monitored_resources(self, tid, time_delta_since_start, cpu, mem):
    self.resources[tid].add(time_delta_since_start, cpu, mem)

def book(self, tid, nice_value):
    """
    Reserve the resources for this task with the specified nice value

---

# If a task was flagged for "retry", we reintegrate it into the candidate pool.
                if len(self.tids_marked_toretry) > 0:
                    # First, we need to remove these tasks from the list of finished tasks.
                    for t in self.tids_marked_toretry:
                        finished = [ x for x in finished if x != t ]
                        finishedtasks = [ x for x in finishedtasks if x != t ]

                    candidates += self.tids_marked_toretry
                    self.tids_marked_toretry = []

---

```python
Args:
    taskcandidates: list
       list of possible tasks that can be submitted
    finished: list
       empty list that will be filled with IDs of tasks that were completed in the meantime
    """
    self.scheduling_iteration = self.scheduling_iteration + 1

    # immediately remove "completed / skippable" tasks
    for tid in taskcandidates.copy():  # <--- the copy is crucial !! otherwise this loop might not function as intended
       if self.ok_to_skip(tid):
           finished.append(tid)
           taskcandidates.remove(tid)
           actionlogger.info("Skipping task " + str(self.idtotask[tid]))
```

---

def process_files_and_connections(self):
    for p, s in self.pid_to_files.items():
        for f in s:
            print("F" + str(f) + " : " + str(p))
    for p, s in self.pid_to_connections.items():
        for c in s:
            print("C" + str(c) + " : " + str(p))
        #print(str(p) + " CONS " + str(c))
    try:
        # check for overlaps
        for p1, s1 in self.pid_to_files.items():
            for p2, s2 in self.pid_to_files.items():
                if p1 != p2:
                    if isinstance(s1, set) and isinstance(s2, set):
                        if len(s1) > 0 and len(s2) > 0:
                            try:
                                inters = s1.intersection(s2)
                            except Exception:
                                print('Exception during intersection check')
                                pass
                            if len(inters) > 0:

---

DOCUMENT:
    outF.writelines(lines)
    outF.close()

    def production_endoftask_hook(self, tid):
        # Executes an end-of-task hook after a successful task, intended for use in GRID productions.
        # Currently, it handles archiving log files and done + time files from jobutils.
        # In the future, this may expand to include more generic tasks like dynamic cleanup of intermediate files.
        # Special attention is needed for the continue feature as `_done` files are now stored elsewhere.
        actionlogger.info("Cleaning up log files for task " + str(tid))
        logf = self.get_logfile(tid)
        donef = self.get_done_filename(tid)
        timef = logf + "_time"

        # Add to the tar file archive
        tf = tarfile.open(name="pipeline_log_archive.log.tar", mode='a')
        if tf is not None:
            tf.add(logf)
            tf.add(donef)
            tf.add(timef)
            tf.close()

---

The final nice value is set during the final submission and may differ. This discrepancy can occur if the nice value was intended to be updated but the system does not allow it.
```python
res = self.resources[tid]
# retrieve the previously set nice value from the last resource check
previous_nice_value = res.nice_value

if previous_nice_value is None:
    # this task was never checked for resources before, so treat it as backfill
    actionlogger.warning("Task ID %d has never been checked for resources. Treating as backfill.", tid)
    nice_value = self.nice_backfill
elif res.nice_value != nice_value:
    actionlogger.warning("Task ID %d was last checked with a different nice value (%d) but is now being submitted with (%d).", tid, res.nice_value, nice_value)
```

---

The processes must exist and the tasks file must be writable by the current user.

---

DOCUMENT:
    plist.append(proc)
    return plist

#
# Code segment to identify all topological orderings
# of a Directed Acyclic Graph (DAG). This helps in determining
# when we can schedule tasks in parallel.
# Source: https://www.geeksforgeeks.org/all-topological-sorts-of-a-directed-acyclic-graph/

# A class to represent a graph
class Graph:

    # Constructor
    def __init__(self, edges, N):

        # An adjacency list to represent the graph
        self.adjList = [[] for _ in range(N)]

        # A list to store the in-degree of each vertex
        # Initialize the in-degree of each vertex to 0
        self.indegree = [0] * N

        # Add edges to the graph
        for (src, dest) in edges:

            # Add an edge from the source to the destination
            self.adjList[src].append(dest)

            # Increase the in-degree of the destination vertex by 1
            self.indegree[dest] = self.indegree[dest] + 1

---

def create_tar_command(dir='./', flags='cf', findtype='f', filename='checkpoint.tar'):
    return 'find ' + str(dir) + ' -maxdepth 1 -type ' + str(findtype) + ' -print0 | xargs -0 tar ' + str(flags) + ' ' + str(filename)

if location is not None:
    print('Creating a failure checkpoint')
    # let's determine a filename using ALIEN_PROC_ID, hostname, and PID

    aliprocid = os.environ.get('ALIEN_PROC_ID')
    if aliprocid is None:
        aliprocid = 0

    fn = 'pipeline_checkpoint_ALIENPROC' + str(aliprocid) + '_PID' + str(os.getpid()) + '_HOST' + socket.gethostname() + '.tar'
    actionlogger.info("Checkpointing to file " + fn)
    tarcommand = create_tar_command(filename=fn)
    actionlogger.info("Creating tar with command " + tarcommand)

    # generating a README file with instructions on how to use the checkpoint
    readmefile = open('README_CHECKPOINT_PID' + str(os.getpid()) + '.txt', 'w')

---

# cpu
    if newcpu is not None:
        oldcpu = task["resources"]["cpu"]
        rel_cpu = task["resources"]["relative_cpu"]
        if rel_cpu is not None:
           # adhere to the relative CPU settings
           # By default, the CPU value in the workflow is scaled if relative_cpu is specified.
           # The new estimate, however, is not scaled, so it must be adjusted here.
           newcpu *= rel_cpu
        actionlogger.info("Modifying cpu estimate for " + task["name"] + " from " + str(oldcpu) + " to " + str(newcpu))
        task["resources"]["cpu"] = newcpu

# a function to convert a software environment defined by alienv into
# a python dictionary
def get_alienv_software_environment(packagestring):
    """
    packagestring can be a string like O2::v202298081-1,O2Physics::xxx representing packages
    published on CVMFS ... or ... a file containing the software environment to apply
    """

---

if self.is_worth_retrying(tid) and ((self.retry_counter[tid] < int(args.retry_on_failure)) or (self.retry_counter[tid] < int(self.task_retries[tid]))):
    print (str(self.idtotask[tid]) + ' to be retried')
    actionlogger.info ('Task ' + str(self.idtotask[tid]) + ' failed but set to be retried ')
    self.tids_marked_toretry.append(tid)
    self.retry_counter[tid] += 1

---

DOCUMENT:
    del workflowspec['stages'][init_index]

    return {"env" : globalenv, "cmd" : initcmd }

    def execute_globalinit_cmd(self, cmd):
        actionlogger.info(f"Executing global setup command: {cmd}")
        # execute the global initialization command (consider any cleanup or setup tasks that need to be done)
        p = subprocess.Popen(['/bin/bash','-c', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = p.communicate()

        # verify if the command executed successfully (return code 0)
        if p.returncode == 0:
            actionlogger.info(stdout.decode())
        else:
            # this should be treated as an error
            actionlogger.error("Error executing global initialization function")
            return False
        return True

    def get_global_task_name(self, name):
        """
        Retrieve the global task name
        """

---

if len(candidates)==0 and len(self.process_list)==0:
                   break
        except Exception as e:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            print(exc_type, fname, exc_tb.tb_lineno)
            traceback.print_exc()
            print ('Cleaning up ')

            self.SIGHandler(0,0)

        endtime = time.perf_counter()
        statusmsg = "success"
        if errorencountered:
           statusmsg = "with failures"

        print ('\n**** Pipeline done ' + statusmsg + ' (global runtime : {:.3f}s) *****\n'.format(endtime-self.start_time))
        actionlogger.debug("global runtime : {:.3f}s".format(endtime-self.start_time))
        return errorencountered

---

# check for intersections
            #for p1, s1 in self.pid_to_files.items():
            #    for p2, s2 in self.pid_to_files.items():
            #        if p1 != p2 and len(s1.intersection(s2)) != 0:
            #            print('Intersection found between files ' + str(p1) + ' and ' + str(p2) + ' with common elements ' + str(s1.intersection(s2)))
        except Exception as e:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            print(exc_type, fname, exc_tb.tb_lineno)
            print('Exception during intersect outer')
            pass

    def is_good_candidate(self, candid, finishedtasks):
        if self.procstatus[candid] != 'ToDo':
            return False
        required_tasks = set([self.tasktoid[t] for t in self.taskneeds[self.idtotask[candid]]])
        if set(finishedtasks).intersection(required_tasks) == required_tasks:
            return True
        return False

---

DOCUMENT:
    env_vars[key.strip()] = value
        return env_vars

    # check if the provided string is a file
    if os.path.exists(packagestring) and os.path.isfile(packagestring):
       actionlogger.info("Using software environment from file " + packagestring)
       return load_env_file(packagestring)

    # execute alienv printenv packagestring to get a dictionary
    # currently this works only with CVMFS
    cmd="/cvmfs/alice.cern.ch/bin/alienv printenv " + packagestring
    proc = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)

    envstring, err = proc.communicate()
    # ensure the printenv command executed successfully
    if len(err.decode()) > 0:
       print (err.decode())
       raise Exception

---

# resources sampled by this
    self.cpu_sampled = None
    self.mem_sampled = None
    # set after a task completes to estimate resources for related tasks
    self.walltime = None
    self.cpu_taken = None
    self.mem_taken = None
    # gathered during monitoring
    self.time_collect = []
    self.cpu_collect = []
    self.mem_collect = []
    # linked to other tasks of the same type
    self.related_tasks = None
    # semaphore can be assigned
    self.semaphore = None
    # the task's priority value
    self.nice_value = None
    # indicates if the task's resources are currently reserved
    self.booked = False

---

DOCUMENT:
    self.scheduling_iteration = 0 # track the number of attempts to schedule new tasks
    self.process_list = []  # list of tasks with normal priority currently scheduled
    self.backfill_process_list = [] # list of tasks with low backfill priority that are currently scheduled (not sure if necessary)
    self.pid_to_psutilsproc = {}  # cache of psutil processes for resource monitoring
    self.pid_to_files = {} # can automatically detect which files are produced by which task (at least partially)
    self.pid_to_connections = {} # can automatically detect which connections are opened by which task (at least partially)
    signal.signal(signal.SIGINT, self.SIGHandler)
    signal.siginterrupt(signal.SIGINT, False)
    self.internalmonitorcounter = 0 # internal counter
    self.internalmonitorid = 0 # internal identifier
    self.tids_marked_toretry = [] # sometimes we might want to retry a failed task (simply because it was "unlucky") and we keep them in this list

---

# execute the user-specified global initialization command for this workflow
globalinitcmd = self.globalinit.get("cmd", None)
if globalinitcmd is not None:
    if not self.execute_globalinit_cmd(globalinitcmd):
        exit(1)

if args.rerun_from:
    reruntaskfound = False
    for task in self.workflowspec['stages']:
        taskname = task['name']
        if re.match(args.rerun_from, taskname):
            reruntaskfound = True
            taskid = self.tasktoid[taskname]
            self.remove_done_flag(find_all_dependent_tasks(self.possiblenexttask, taskid))
    if not reruntaskfound:
        print('No task matching ' + args.rerun_from + ' found; refusing to proceed ')
        exit(1)

# **************************
# primary control loop
# **************************
candidates = [ tid for tid in self.possiblenexttask[-1] ]

---

def update_resource_estimates(workflow, resource_json):
    resource_dict = load_json(resource_json)
    stages = workflow["stages"]

    for task in stages:
        if task["timeframe"] >= 1:
            name = "_".join(task["name"].split("_")[:-1])
        else:
            name = task["name"]

        if name not in resource_dict:
            continue

        new_resources = resource_dict[name]

        # memory
        newmem = new_resources.get("mem", None)
        if newmem is not None:
            oldmem = task["resources"]["mem"]
            actionlogger.info("Updating the memory estimate for " + task["name"] + " from " + str(oldmem) + " to " + str(newmem))
            task["resources"]["mem"] = newmem
        newcpu = new_resources.get("cpu", None)

---

# create the object responsible for managing resources...
  self.resource_manager = ResourceManager(args.cpu_limit, args.mem_limit, args.maxjobs, args.dynamic_resources, args.optimistic_resources)
  for task in self.workflowspec['stages']:
      # ...and include all initial resource estimations
      global_task_name = self.get_global_task_name(task["name"])
      try:
          cpu_relative = float(task["resources"]["relative_cpu"])
      except TypeError:
          cpu_relative = 1
      self.resource_manager.add_task_resources(task["name"], global_task_name, float(task["resources"]["cpu"]), cpu_relative, float(task["resources"]["mem"]), task.get("semaphore"))

  self.procstatus = { tid:'ToDo' for tid in range(len(self.workflowspec['stages'])) }
  self.taskneeds= { t:set(self.getallrequirements(t)) for t in self.taskuniverse }
  self.stoponfailure = not args.keep_going
  print ("Stop on failure ",self.stoponfailure)

---

# retain only the tasks that are essential to run according to the user's criteria
      self.workflowspec = filter_workflow(self.workflowspec, args.target_tasks, args.target_labels)

      if not self.workflowspec['stages']:
          if args.target_tasks:
              print ('It seems that some of the selected target tasks are missing from the workflow')
              exit (0)
          print ('The workflow is empty; there is nothing to process')
          exit (0)

---

if self.is_productionmode:
    # we can perform some general cleanup of completed tasks in non-interactive/GRID mode
    # TODO: this operation can run asynchronously
    for _t in finished_from_started:
        self.production_endoftask_hook(_t)

    # if a task was flagged as "failed" and we proceed here (due to using --keep-going)...
    # we must remove the pid from the finished list to prevent processing their children
    if len(failing) > 0:
        # remove these from the list of finished tasks to avoid continuing with their children
        errorencountered = True
        for t in failing:
            finished = [ x for x in finished if x != t ]
            finishedtasks = [ x for x in finishedtasks if x != t ]

---

ENTRYPoint for managing and querying resources for updates.

Can determine if a specific task can be executed based on the current resource usage.
Reserve and release resources as needed.
"""
def __init__(self, cpu_limit, mem_limit, procs_parallel_max=100, dynamic_resources=False, optimistic_resources=False):
    """
    Initialize members with default values
    """
    # store TaskResources for all tasks
    self.resources = []

    # helper dictionaries for common objects that will be distributed to individual TaskResources objects
    # to prevent further lookups and to share the same common objects
    self.resources_related_tasks_dict = {}
    self.semaphore_dict = {}

    # one common object holding global resource settings like CPU and MEM limits
    self.resource_boundaries = ResourceBoundaries(cpu_limit, mem_limit, dynamic_resources, optimistic_resources)

---

DOCUMENT:
    def get_logfile(self, tid):
        """
        The O2 taskwrapper records task stdout and stderr in a logfile named <task>.log.
        Retrieve the precise path of this logfile using the task ID.
        """
        # calculates the name of the logfile for this task
        name = self.workflowspec['stages'][tid]['name']
        workdir = self.workflowspec['stages'][tid]['cwd']
        return os.path.join(workdir, f"{name}.log")

    def get_done_filename(self, tid):
        """
        After a task successfully completes, the O2 taskwrapper creates a <task>.log_done file.
        Obtain the exact path of this file based on the task ID.
        """
        return f"{self.get_logfile(tid)}_done"

    def get_resources_filename(self, tid):
        """
        Upon task completion, the O2 taskwrapper generates a <task>.log_time file.
        Retrieve the precise path of this file using the task ID.
        """
        return f"{self.get_logfile(tid)}_time"

---

DOCUMENT:
    finished = [] # --> to account for finished tasks that are already done or skipped
                actionlogger.debug('Sorted current candidates: ' + str([(c,self.idtotask[c]) for c in candidates]))
                self.try_job_from_candidates(candidates, finished)
                if len(candidates) > 0 and len(self.process_list) == 0:
                    self.noprogress_errormsg()
                    send_webhook(self.args.webhook,"Unable to make further progress: Quitting")
                    errorencountered = True
                    break

---

# b) the PATH for compiler includes needed by Cling
cmd = "LC_ALL=C c++ -xc++ -E -v /dev/null 2>&1 | sed -n '/^#include/,${/^ \\/.*++/{p}}'"
proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
incpath, err = proc.communicate()
incpaths = [ line.lstrip() for line in incpath.decode().splitlines() ]
joined = ':'.join(incpaths)
if args.no_rootinit_speedup != True:
    actionlogger.info("Determined ROOT_CPPSYSINCL=" + joined)
    os.environ['ROOT_CPPSYSINCL'] = joined

speedup_ROOT_Init()

---

# track resources reserved with the default priority
self.cpu_booked = 0
self.mem_booked = 0
# count the tasks reserved with the default priority
self.n_procs = 0

# track resources reserved with the higher priority
self.cpu_booked_backfill = 0
self.mem_booked_backfill = 0
# count the tasks reserved with the higher priority
self.n_procs_backfill = 0

# define the maximum concurrent tasks
self.procs_parallel_max = procs_parallel_max

# determine the default priority of this script
self.nice_default = os.nice(0)
# add 19 to set the priority for low-priority tasks
self.nice_backfill = self.nice_default + 19

---

class ResourceBoundaries:
    """
    Holds global resource properties
    """
    def __init__(self, cpu_limit, mem_limit, dynamic_resources=False, optimistic_resources=False):
        self.cpu_limit = cpu_limit
        self.mem_limit = mem_limit
        self.dynamic_resources = dynamic_resources
        # if enabled, tasks exceeding resource limits will still be attempted to run
        self.optimistic_resources = optimistic_resources

---

In addition, it could be beneficial to run the workflow without the resource-aware, dynamic scheduler.
This can be achieved by converting the JSON workflow into a linear shell script and directly executing it.
To do this, use the `--produce-script myscript.sh` option.
"""
        print (msg, file=sys.stderr)

    def execute(self):
        self.start_time = time.perf_counter()
        psutil.cpu_percent(interval=None)
        os.environ['JOBUTILS_SKIPDONE'] = "ON"
        errorencountered = False

        def speedup_ROOT_Init():
               """initialize some environment variables that speed up ROOT initialization
               and prevent ROOT from launching many short-lived child processes"""

               # only perform this on Linux
               if platform.system() != 'Linux':
                  return

---

# if tasks_skipped:
   #  return  # ---> we return early to maintain order (the next candidate should be the offspring of the skipped jobs)
   # retrieve task ID and proposed niceness from the generator
   for (tid, nice_value) in self.resource_manager.ok_to_submit(taskcandidates):
      actionlogger.debug ("attempting to submit " + str(tid) + ':' + str(self.idtotask[tid]))
      if p := self.submit(tid, nice_value):
         # set the niceness explicitly here from the process again as submit might not have changed it
         # inform the ResourceManager of the final niceness
         self.resource_manager.book(tid, p.nice())
         self.process_list.append((tid,p))
         taskcandidates.remove(tid)
         # short delay
         time.sleep(0.1)

---

parser.add_argument('--produce-script', help='Generates a shell script to execute the workflow sequentially and exits.')
parser.add_argument('--rerun-from', help='Re-executes the workflow starting from the specified task (or pattern), rerunning all dependent jobs.')
parser.add_argument('--list-tasks', help='Lists all tasks by name and exits.', action='store_true')

---

# Recursive function to find all topological orderings of a directed acyclic graph (DAG)
def findAllTopologicalOrders(graph, path, discovered, N, allpaths, maxnumber=1):
    if len(allpaths) >= maxnumber:
        return

    # process each vertex
    for v in range(N):

        # proceed only if the in-degree of the current vertex is 0 and the vertex has not been processed yet
        if graph.indegree[v] == 0 and not discovered[v]:

            # for every adjacent vertex u of v, decrease the in-degree of u by 1
            for u in graph.adjList[v]:
                graph.indegree[u] -= 1

            # add the current vertex to the path and mark it as discovered
            path.append(v)
            discovered[v] = True

            # recurse
            findAllTopologicalOrders(graph, path, discovered, N, allpaths)

            # backtrack: restore in-degree information for the current vertex
            for u in graph.adjList[v]:
                graph.indegree[u] += 1

---

DOCUMENT:
    # the location should be an alien path such as alien:///foo/bar/
           copycommand='alien.py cp ' + fn + ' ' + str(location) + '@disk:1'
           actionlogger.info("Copying to alien " + copycommand)
           os.system(copycommand)

    def init_alternative_software_environments(self):
        """
        Sets up alternative software environments for particular tasks, if the workflow specification includes a relevant annotation.
        """

        environment_cache = {}
        # iterate over all tasks and configure the environment
        for taskid in range(len(self.workflowspec['stages'])):
          packagestr = self.workflowspec['stages'][taskid].get("alternative_alienv_package")
          if packagestr is None:
             continue

          if environment_cache.get(packagestr) is None:
             environment_cache[packagestr] = get_alienv_software_environment(packagestr)

          self.alternative_envs[taskid] = environment_cache[packagestr]

---

# empty finished means we need to wait longer
    return len(finished)==0

def is_worth_retrying(self, tid):
    # This function examines certain patterns in logfiles to determine if retrying the task could be beneficial.
    # Ideally, this should be customizable by the user, allowing them to define a custom lambda or regular expression.
    # For now, we simply retry the task a few times.
    logfile = self.get_logfile(tid)

    return True #! --> currently, we retry tasks multiple times

    # 1) ZMQ_EVENT + interrupted system calls (DPL bug during shutdown)
    # Uncertain if using 'grep' is more efficient than native Python text search...
    # status = os.system('grep "failed setting ZMQ_EVENTS" ' + logfile + ' &> /dev/null')
    # if os.WEXITSTATUS(status) == 0:
    #   return True

    # return False

---

parser.add_argument('-f','--workflowfile', help='Name of the input workflow file', required=True)
parser.add_argument('-jmax','--maxjobs', type=int, help='Maximum number of parallel tasks.', default=100)
parser.add_argument('-k','--keep-going', action='store_true', help='Continue executing the pipeline even if an error occurs.')
parser.add_argument('--dry-run', action='store_true', help='Display the actions that would be performed without executing them.')
parser.add_argument('--visualize-workflow', action='store_true', help='Generates a visual graph of the workflow.')
parser.add_argument('--target-labels', nargs='+', help='Runs the pipeline based on target labels (e.g., "TPC" or "DIGI"). This condition is combined logically AND with --target-tasks.', default=[])
parser.add_argument('-tt','--target-tasks', nargs='+', help='Runs the pipeline based on target tasks (e.g., "tpcdigi"). By default, all tasks in the graph are executed. Supports regular expressions.', default=["*"])

---

elif should_break:
                    # terminate execution if the resources required for the next task are insufficient
                    break


class WorkflowExecutor:
    # Constructor
    def __init__(self, workflowfile, args, jmax=100):
      self.args = args
      self.is_productionmode = args.production_mode == True or os.getenv("ALIEN_PROC_ID") is not None
      self.workflowfile = workflowfile
      self.workflowspec = load_json(workflowfile)
      self.globalinit = self.extract_global_environment(self.workflowspec) # initialize global environment settings
      for e in self.globalinit['env']:
        if os.environ.get(e, None) is None:
           value = self.globalinit['env'][e]
           actionlogger.info("Applying global environment from init section " + str(e) + " : " + str(value))
           os.environ[e] = str(value)

---

DOCUMENT:
    res.nice_value = nice_value
    res.booked = True
    if res.semaphore is not None:
        res.semaphore.lock()
    if nice_value != self.nice_default:
        self.n_procs_backfill += 1
        self.cpu_booked_backfill += res.cpu_assigned
        self.mem_booked_backfill += res.mem_assigned
        return
    self.n_procs += 1
    self.cpu_booked += res.cpu_assigned
    self.mem_booked += res.mem_assigned

---

# candidate list trivial
nextjobtrivial = { n:[] for n in nodes }
# startnodes
nextjobtrivial[-1] = nodes
for e in edges:
    nextjobtrivial[e[0]].append(e[1])
    if nextjobtrivial[-1].count(e[1]):
        nextjobtrivial[-1].remove(e[1])

# determine topological orderings of the graph
# construct a graph from the edges
graph = Graph(edges, N)
orderings = printAllTopologicalOrders(graph)

return (orderings, nextjobtrivial)


def draw_workflow(workflowspec):
    if not havegraphviz:
        print('graphviz not installed, unable to draw workflow')
        return

    dot = Digraph(comment='MC workflow')
    nametoindex={}
    index=0
    # nodes
    for node in workflowspec['stages']:
        name=node['name']
        nametoindex[name]=index
        dot.node(str(index), name)
        index=index+1

---

# backtrack: remove the current node from the path and
            # mark it as undiscovered
            path.pop()
            discovered[v] = False

# document the valid sequence
if len(path) == N:
    allpaths.append(path.copy())


# generate all topological orderings of a given directed acyclic graph (DAG) as a list
def printAllTopologicalOrders(graph, maxnumber=1):
    # determine the number of nodes in the graph
    N = len(graph.adjList)

    # initialize an auxiliary array to track the discovery status of each vertex
    discovered = [False] * N

    # initialize the path and all possible paths
    path = []
    allpaths = []
    # identify all topological orderings and record them
    findAllTopologicalOrders(graph, path, discovered, N, allpaths, maxnumber=maxnumber)
    return allpaths

# <--- end code section for topological sorts

---

# run control, webhooks
parser.add_argument('--stdout-on-failure', action='store_true', help='Display log files of failing tasks on standard output,')
parser.add_argument('--webhook', help=argparse.SUPPRESS) # log some information to this webhook channel
parser.add_argument('--checkpoint-on-failure', help=argparse.SUPPRESS) # a debugging option that creates a debug tarball and sends it to a specified address
                                                                       # the argument should be an alien path
parser.add_argument('--retry-on-failure', help=argparse.SUPPRESS, default=0) # specifies the number of times a failing task should be retried
parser.add_argument('--no-rootinit-speedup', help=argparse.SUPPRESS, action='store_true') # disables the initialization of ROOT environment variables to speed up initialization/startup

---

DOCUMENT:
    def get_global_task_name(self, name):
        """
        Retrieve the global task name

        Tasks are considered related if only the suffix _<i> differs
        """
        tokens = name.split("_")
        try:
            int(tokens[-1])
            return "_".join(tokens[:-1])
        except ValueError:
            pass
        return name

    def getallrequirements(self, task_name):
        """
        Obtain all requirements of a task by its name
        """
        l = []
        for required_task_name in self.workflowspec['stages'][self.tasktoid[task_name]]['needs']:
            l.append(required_task_name)
            l += self.getallrequirements(required_task_name)
        return l

---

1. if necessary, create the working directory if it doesn't already exist
2. mark the task as running by updating the lookup structures
3. configure the specific environment if required for the task
4. initialize psutil.Process from the command line
4.1 modify the niceness of this process if needed
5. return the psutil.Process object
"""
actionlogger.debug("Submitting task " + str(self.idtotask[tid]) + " with nice value " + str(nice))
command = self.workflowspec['stages'][tid]['cmd']
working_directory = self.workflowspec['stages'][tid]['cwd']
if working_directory:
    if os.path.exists(working_directory) and not os.path.isdir(working_directory):
        actionlogger.error('Cannot create working directory ... another resource is already present')
        return None

    if not os.path.isdir(working_directory):
        os.makedirs(working_directory)

---

mem_sampled = 0
        cpu_sampled = []
        for res in self.related_tasks:
            if res.is_done:
                mem_sampled = max(mem_sampled, res.mem_sampled)
                cpu_sampled.append(res.cpu_sampled)
        cpu_sampled = sum(cpu_sampled) / len(cpu_sampled)

        # If this task has already run with the assigned resources, set it to the limit
        if cpu_sampled > self.resource_boundaries.cpu_limit:
            actionlogger.warning("Sampled CPU (%.2f) exceeds assigned CPU limit (%.2f)", cpu_sampled, self.resource_boundaries.cpu_limit)
        elif cpu_sampled < 0:
            actionlogger.debug("Sampled CPU for %s is %.2f < 0, setting to previously assigned value %.2f", self.name, cpu_sampled, self.cpu_assigned)
            cpu_sampled = self.cpu_assigned

---

if the directory does not exist at workdir:
                  create it using os.makedirs()

      update self.procstatus[tid] to 'Running'
      if args.dry_run is True:
          construct drycommand as "echo \' " + str(self.scheduling_iteration) + " : would do " + str(self.workflowspec['stages'][tid]['name']) + "\'"
          return subprocess.Popen(['/bin/bash','-c',drycommand], cwd=workdir)

---

DOCUMENT:
    def stop_pipeline_and_exit(self, process_list):
        # terminate all remaining processes
        for p in process_list:
           p[1].terminate()

        exit(1)


    def monitor(self, process_list):
        """
        Iterate through all active tasks and collect their current resource usage.

        Resources are aggregated for each task and its descendants.

        Pass the CPU, PSS, USS, niceness, and current timestamp to the metriclogger.

        Issue a warning if the total PSS surpasses the allocated memory limit.
        """
        self.internalmonitorcounter+=1
        if self.internalmonitorcounter % 5 != 0:
            return

        self.internalmonitorid+=1

        globalCPU=0.
        globalPSS=0.
        resources_per_task = {}

        for tid, proc in process_list:

---

DOCUMENT:
    def ok_to_submit(self, tids):
        """
        This generator yields the tid and nice value tuple from the list of task ids to be verified
        """
        tids_copy = tids.copy()

        def ok_to_submit_default(res):
            """
            Return the default nice value if the conditions are satisfied, otherwise return None
            """
            # evaluate CPU usage
            okcpu = (self.cpu_booked + res.cpu_assigned <= self.resource_boundaries.cpu_limit)
            # evaluate MEM usage
            okmem = (self.mem_booked + res.mem_assigned <= self.resource_boundaries.mem_limit)
            actionlogger.debug ('Condition check --normal-- for  ' + str(res.tid) + ':' + res.name + ' CPU ' + str(okcpu) + ' MEM ' + str(okmem))
            return self.nice_default if (okcpu and okmem) else None

---

DOCUMENT:
    thispss=getattr(fullmem,'pss',0) #<-- pss not available on MacOS
    totalPSS=totalPSS + thispss
    totalSWAP=totalSWAP + fullmem.swap
    thisuss=fullmem.uss
    totalUSS=totalUSS + thisuss
except (psutil.NoSuchProcess, psutil.AccessDenied):
    pass

---

self.alternative_envs = {} # mapping from taskid to alternative software environments (to be applied per task)
      # initialize alternative software environments
      self.init_alternative_software_environments()

    def SIGHandler(self, signum, frame):
       """
       essentially forcing the termination of all child processes
       """
       actionlogger.info("Signal " + str(signum) + " received")
       try:
           procs = psutil.Process().children(recursive=True)
       except (psutil.NoSuchProcess):
           pass
       except (psutil.AccessDenied, PermissionError):
           procs = getChildProcs(os.getpid())

       for p in procs:
           actionlogger.info("Terminating " + str(p))
           try:
             p.terminate()
           except (psutil.NoSuchProcess, psutil.AccessDenied):
             pass

---

# <--- end code section for topological sorts

# identify all tasks that are dependent on a specified task (id); if a cache
# dictionary is provided, we can populate the entire graph in a single pass...
def identify_all_dependent_tasks(possiblenexttask, tid, cache=None):
    c = cache.get(tid) if cache else None
    if c is not None:
        return c

    daughterlist = [tid]
    # potentially recurse
    for n in possiblenexttask[tid]:
        c = cache.get(n) if cache else None
        if c is None:
            c = identify_all_dependent_tasks(possiblenexttask, n, cache)
        daughterlist += c
        if cache is not None:
            cache[n] = c

    if cache is not None:
        cache[tid] = daughterlist
    return list(set(daughterlist))


# function to process given edges, build the graph, and derive all topological orderings along with auxiliary data structures
def analyze_graph(edges, nodes):
    # total number of nodes in the graph
    N = len(nodes)

---

time_delta = int((time.perf_counter() - self.start_time) * 1000)
totalUSS /= 1024 / 1024
totalPSS /= 1024 / 1024
nice_value = proc.nice()
resources_per_task[tid] = {'iter': self.internalmonitorid, 'name': self.idtotask[tid], 'cpu': totalCPU, 'uss': totalUSS, 'pss': totalPSS, 'nice': nice_value, 'swap': totalSWAP, 'label': self.workflowspec['stages'][tid]['labels']}
self.resource_manager.add_monitored_resources(tid, time_delta, totalCPU / 100, totalPSS)
if nice_value == self.resource_manager.nice_default:
    globalCPU += totalCPU
    globalPSS += totalPSS

metriclogger.info(resources_per_task[tid])
send_webhook(self.args.webhook, resources_per_task)

---

# inner "lambda" helper function to determine if a task "name" is required by the specified targets
def needed_by_targets(name):
    if name in full_target_name_list:
        return True
    if name in full_requirements_name_list:
        return True
    return False

# finally, we copy over everything that matches the targets as well as all their requirements
transformedworkflowspec['stages'] = [ l for l in workflowspec['stages'] if needed_by_targets(l['name']) ]
return transformedworkflowspec


# constructs topological orderings for each timeframe
def build_dag_properties(workflowspec):
    globaltaskuniverse = [ (l, i) for i, l in enumerate(workflowspec['stages'], 1) ]
    timeframeset = set(l['timeframe'] for l in workflowspec['stages'])

    edges, nodes = build_graph(globaltaskuniverse, workflowspec)
    tup = analyseGraph(edges, nodes.copy())
    global_next_tasks = tup[1]

---

# edges
for node in workflowspec['stages']:
    to_index = nametoindex[node['name']]
    for requirement in node['needs']:
        from_index = nametoindex[requirement]
        dot.edge(str(from_index), str(to_index))

dot.render('workflow.gv')

# constructs the graph using a "taskuniverse" list and creates associated tasktoid and idtotask structures
def build_graph(taskuniverse, workflowspec):
    tasktoid = { t[0]['name']:i for i, t in enumerate(taskuniverse, 0) }
    # print (tasktoid)

    nodes = []
    edges = []
    for task in taskuniverse:
        nodes.append(tasktoid[task[0]['name']])
        for need in task[0]['needs']:
            edges.append((tasktoid[need], tasktoid[task[0]['name']]))

    return (edges, nodes)


# reads a JSON file and converts it into a dictionary, suitable for a workflow specification
def load_json(workflowfile):
    with open(workflowfile) as fp:
        workflowspec = json.load(fp)
    return workflowspec

---

# Resources
parser.add_argument('--update-resources', dest="update_resources", help='Read resource estimates from a JSON and apply them where feasible.')
parser.add_argument("--dynamic-resources", dest="dynamic_resources", action="store_true", help="Adjust resource estimates of tasks based on completed related tasks.") # dynamically derive resources
parser.add_argument('--optimistic-resources', dest="optimistic_resources", action="store_true", help="Attempt to run the workflow despite resource limits potentially underestimating some tasks' needs.")
parser.add_argument("--n-backfill", dest="n_backfill", type=int, default=1)
parser.add_argument('--mem-limit', help='Set a memory limit for scheduling (in MB)', default=0.9*max_system_mem/1024./1024, type=float)
parser.add_argument('--cpu-limit', help='Set a CPU limit (number of cores)', default=8, type=float)
parser.add_argument('--cgroup', help='Run the pipeline under a specified cgroup (e.g., 8coregrid) to simulate resource constraints.')

---

def unbook(self, tid):
    """
    Release the resources allocated to this task
    """
    res = self.resources[tid]
    res.booked = False
    if self.resource_boundaries.dynamic_resources:
        res.sample_resources()
    if res.semaphore is not None:
        res.semaphore.unlock()
    if res.nice_value != self.nice_default:
        self.cpu_booked_backfill -= res.cpu_assigned
        self.mem_booked_backfill -= res.mem_assigned
        self.n_procs_backfill -= 1
        if self.n_procs_backfill <= 0:
            self.cpu_booked_backfill = 0
            self.mem_booked_backfill = 0
        return
    self.n_procs -= 1
    self.cpu_booked -= res.cpu_assigned
    self.mem_booked -= res.mem_assigned
    if self.n_procs <= 0:
        self.cpu_booked = 0
        self.mem_booked = 0

---

DOCUMENT:
    self.process_list=[] # a list of tuples containing node IDs and Popen subprocess instances

        finishedtasks=[] # a global list of completed tasks

        try:

            while True:
                # sort the candidate list based on task weights
                candidates = [ (tid, self.taskweights[tid]) for tid in candidates ]
                candidates.sort(key=lambda tup: (tup[1][0],-tup[1][1])) # prioritize smaller tasks and those within the same timeframe, then prioritize important tasks within the same timeframe
                # remove weights
                candidates = [ tid for tid,_ in candidates ]

---

speedup_ROOT_Init()

    # we create our own "tmp" directory
    # to store temporary files such as socket files (e.g., DPL FAIR-MQ sockets)
    # (This might be less relevant if running inside a Docker/singularity container)
    if not os.path.isdir("./.tmp"):
      os.mkdir("./.tmp")
    if os.environ.get('FAIRMQ_IPC_PREFIX') is None:
      socketpath = os.getcwd() + "/.tmp"
      actionlogger.info("Setting FAIRMQ socket path to " + socketpath)
      os.environ['FAIRMQ_IPC_PREFIX'] = socketpath

    # some initialization tasks
    if args.list_tasks:
      print ('List of tasks in this workflow:')
      for i, t in enumerate(self.workflowspec['stages'], 0):
        print (t['name'] + '  (' + str(t['labels']) + ')' + ' ToDo: ' + str(not self.ok_to_skip(i)))
      exit (0)

    if args.produce_script is not None:
      self.produce_script(args.produce_script)
      exit (0)

---

def add_task_resources(self, name, related_tasks_name, cpu, cpu_relative, mem, semaphore_string=None):
    """
    Construct and add a new TaskResources object
    """
    resources = TaskResources(len(self.resources), name, cpu, cpu_relative, mem, self.resource_boundaries)
    if not resources.is_within_limits() and not self.resource_boundaries.optimistic_resources:
        # exit if we can't proceed
        print(f"Resources for task {name} exceed the boundaries.\nCPU: {cpu} (estimate) vs. {self.resource_boundaries.cpu_limit} (boundary)\nMEM: {mem} (estimate) vs. {self.resource_boundaries.mem_limit} (boundary).")
        print("Use the --optimistic-resources flag with the runner to proceed despite these constraints.")
        exit(1)
    # if we reach this point, the resources are within limits or the user has opted for an optimistic run, in which case we limit the resources accordingly.
    resources.limit_resources()

---

# Logging
parser.add_argument('--action-logfile', help='Specify a log filename for action logs. If not specified, the default will be pipeline_action_#PID.log')
parser.add_argument('--metric-logfile', help='Specify a log filename for metric logs. If not specified, the default will be pipeline_metric_#PID.log')
parser.add_argument('--production-mode', action='store_true', help='Enable production mode')
# This will activate special features beneficial for non-interactive or production processing, such as automatic file cleanup.
args = parser.parse_args()

def setup_logger(name, log_file, level=logging.INFO):
    """Function to configure loggers for various needs"""

    handler = logging.FileHandler(log_file, mode='w')
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(level)
    logger.addHandler(handler)

    return logger

# Initialize the action logger
actionlogger = setup_logger('pipeline_action_logger', ('pipeline_action_' + str(os.getpid()) + '.log', args.action_logfile)[args.action_logfile is not None], level=logging.DEBUG)

---

# start by setting the base directory
os.system(tarcommand)

# subsequently, include specifics for the given task ids if applicable
for tid in taskids:
  taskspec = self.workflowspec['stages'][tid]
  directory = taskspec['cwd']
  if directory != "./":
    tarcommand = get_tar_command(dir=directory, flags='rf', filename=fn)
    actionlogger.info("Tar command is " + tarcommand)
    os.system(tarcommand)
    # similar action for symbolic links
    tarcommand = get_tar_command(dir=directory, flags='rf', findtype='l', filename=fn)
    actionlogger.info("Tar command is " + tarcommand)
    os.system(tarcommand)

# prepend file:/// to indicate a local file
fn = "file://" + fn
actionlogger.info("Local checkpoint file is " + fn)

---

# CPU section
# retrieve existing process or add new one
cachedproc = self.pid_to_psutilsproc.get(p.pid)
if cachedproc is not None:
    try:
        thiscpu = cachedproc.cpu_percent(interval=None)
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        thiscpu = 0.
    totalCPU += thiscpu
    # thisresource = {'iter': self.internalmonitorid, 'pid': p.pid, 'cpu': thiscpu, 'uss': thisuss/1024./1024., 'pss': thispss/1024./1024.}
    # metriclogger.info(thisresource)
else:
    self.pid_to_psutilsproc[p.pid] = p
    try:
        self.pid_to_psutilsproc[p.pid].cpu_percent()
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

---

# filters the original workflowspec based on specified targets or labels
# returns a new workflowspec
def filter_workflow(workflowspec, targets=[], targetlabels=[]):
    if len(targets) == 0:
        return workflowspec
    if len(targetlabels) == 0 and len(targets) == 1 and targets[0] == "*":
        return workflowspec

    transformedworkflowspec = workflowspec

    def task_matches(t):
        for filt in targets:
            if filt == "*":
                return True
            if re.match(filt, t) is not None:
                return True
        return False

    def task_matches_labels(t):
        # it's acceptable if no labels are provided at all
        if len(targetlabels) == 0:
            return True

        for l in t['labels']:
            if l in targetlabels:
                return True
        return False

---

# return False


    def cat_logfiles_tostdout(self, taskids):
        # In case of errors, we can display the contents of the logfiles for each taskname on stdout. Assuming the taskname follows the convention of having a corresponding "taskname.log" file.
        for tid in taskids:
            logfile = self.get_logfile(tid)
            if os.path.exists(logfile):
                print(' ----> START OF LOGFILE ', logfile, ' -----')
                os.system('cat ' + logfile)
                print(' <---- END OF LOGFILE ', logfile, ' -----')

    def send_checkpoint(self, taskids, location):
        # Creates a tarball that includes all base directory files (timeframe-independent) and the directory containing corrupted timeframes, then copies it to a specified ALIEN location. This is not a core function but rather a utility for managing error conditions on the GRID.

---

DOCUMENT:
    finished_from_started = [] # to keep track of finished tasks that were started
    failing = []
    while self.waitforany(self.process_list, finished_from_started, failing):
        if not args.dry_run:
            self.monitor(self.process_list) #  ---> consider making this asynchronous
            time.sleep(1) # <--- adjust to an incremental wait (short initial delay)
        else:
            time.sleep(0.001)

    finished = finished + finished_from_started
    actionlogger.debug("finished tasks now :" + str(finished_from_started))
    finishedtasks = finishedtasks + finished

---

# the software environment is now in the evnstring
# split it on semicolon
envstring = envstring.decode()
tokens = envstring.split(";")
# build envmap
envmap = {}
for t in tokens:
    # check for assignment
    if t.count("=") > 0:
        assignment = t.rstrip().split("=")
        envmap[assignment[0]] = assignment[1]
    elif t.count("export") > 0:
        # the scenario when we export or deal with a simple variable
        # account for the case where this hasn't been assigned before
        variable = t.split()[1]
        if variable not in envmap:
            envmap[variable] = ""

return envmap

#
# functions for execution; encapsulated within a WorkflowExecutor class
#

class Semaphore:
    """
    An object that can be used as a semaphore
    """
    def __init__(self):
        self.locked = False
    def lock(self):
        self.locked = True
    def unlock(self):
        self.locked = False

---

for p in list(process_list):
    pid = p[1].pid
    tid = p[0]  # the task id of this process
    returncode = 0
    if not self.args.dry_run:
        returncode = p[1].poll()
    if returncode is not None:
        actionlogger.info('Task ' + str(pid) + ' ' + str(tid) + ':' + str(self.idtotask[tid]) + ' finished with status ' + str(returncode))
        # account for cleared resources
        self.resource_manager.unbook(tid)
        self.procstatus[tid] = 'Done'
        finished.append(tid)
        #self.validate_resources_running(tid)
        process_list.remove(p)
        if returncode != 0:
            print(str(self.idtotask[tid]) + ' failed ... checking retry')
            # we inspect if this is something "unlucky" which could be resolved by a simple resubmit

---

dependency_cache = {}
# The weight affects the scheduling order and can be anything defined by the user. For now, we prioritize tasks within a timeframe and then consider the number of tasks that depend on a task as another weight factor.
# TODO: Incorporate resource estimates like CPU and MEM from the runtime.
# TODO: Develop this as a policy for the runner to explore various strategies.
def getweight(tid):
    return (globaltaskuniverse[tid][0]['timeframe'], len(find_all_dependent_tasks(global_next_tasks, tid, dependency_cache)))

task_weights = [ getweight(tid) for tid in range(len(globaltaskuniverse)) ]

for tid in range(len(globaltaskuniverse)):
    actionlogger.info("Score for " + str(globaltaskuniverse[tid][0]['name']) + " is " + str(task_weights[tid]))

# print (global_next_tasks)
return { 'nexttasks' : global_next_tasks, 'weights' : task_weights, 'topological_ordering' : tup[0] }

---

DOCUMENT:
    self.retry_counter = [0 for tid in range(len(self.taskuniverse))] # we keep track of how many times each task has been retried
    self.task_retries = [self.workflowspec['stages'][tid].get('retry_count',0) for tid in range(len(self.taskuniverse))] # the specific "retry" count for each task, which needs to be extracted from the JSON