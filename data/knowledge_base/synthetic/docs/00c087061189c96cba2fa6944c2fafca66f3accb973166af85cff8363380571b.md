## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2dpg_workflow_runner.py

**Start chunk id:** 00c087061189c96cba2fa6944c2fafca66f3accb973166af85cff8363380571b

## Content

# carry out the user-defined global initialization command for this workflow
    globalinitcmd = self.globalinit.get("cmd", None)
    if globalinitcmd is not None:
       if not self.execute_globalinit_cmd(globalinitcmd):
          exit (1)

    if args.rerun_from:
      reruntaskfound=False
      for task in self.workflowspec['stages']:
          taskname=task['name']
          if re.match(args.rerun_from, taskname):
            reruntaskfound=True
            taskid=self.tasktoid[taskname]
            self.remove_done_flag(find_all_dependent_tasks(self.possiblenexttask, taskid))
      if not reruntaskfound:
          print('No task matching ' + args.rerun_from + ' found; will not proceed ')
          exit (1)

    # *************************
    # main control loop
    # *************************
    candidates = [ tid for tid in self.possiblenexttask[-1] ]

---

parser.add_argument('--produce-script', help='Generates a shell script to execute the workflow sequentially and then terminates.')
parser.add_argument('--rerun-from', help='Re-executes the workflow starting from specified task (or pattern), rerunning all dependent jobs.')
parser.add_argument('--list-tasks', help='Displays the names of all tasks and then exits.', action='store_true')

---

speedup_ROOT_Init()

    # we create a "tmp" directory here
    # for temporary files such as sockets (especially for DPL FAIR-MQ sockets)
    # this might not be necessary if running inside a Docker/Singularity container
    if not os.path.isdir("./.tmp"):
      os.mkdir("./.tmp")
    if os.environ.get('FAIRMQ_IPC_PREFIX') is None:
      socketpath = os.getcwd() + "/.tmp"
      actionlogger.info("Setting FAIRMQ socket path to " + socketpath)
      os.environ['FAIRMQ_IPC_PREFIX'] = socketpath

    # perform initial setup tasks
    if args.list_tasks:
      print('List of tasks in this workflow:')
      for i, t in enumerate(self.workflowspec['stages'], 0):
        print(t['name'] + '  (' + str(t['labels']) + ')' + ' ToDo: ' + str(not self.ok_to_skip(i)))
      exit(0)

    if args.produce_script != None:
      self.produce_script(args.produce_script)
      exit(0)

---

# This sequence of operations functions and has a somewhat structured approach.
# However, it creates lookups utilized elsewhere, so there might be CPU savings by reusing
# some structures across functions or by processing the data in fewer passes.

# helper lookup
tasknametoid = { t['name']:i for i, t in enumerate(workflowspec['stages'],0) }

# check if a task can be executed at all
# or if it is prevented due to missing requirements
def canBeDone(t, cache={}):
    ok = True
    c = cache.get(t['name'])
    if c is not None:
        return c
    for r in t['needs']:
        taskid = tasknametoid.get(r)
        if taskid is not None:
            if not canBeDone(workflowspec['stages'][taskid], cache):
                ok = False
                break
        else:
            ok = False
            break
    cache[t['name']] = ok
    return ok

---

class ResourceBoundaries:
    """
    Holds global resource properties
    """
    def __init__(self, cpu_limit, mem_limit, dynamic_resources=False, optimistic_resources=False):
        self.cpu_limit = cpu_limit
        self.mem_limit = mem_limit
        self.dynamic_resources = dynamic_resources
        # if set, tasks exceeding resource limits will be attempted to run regardless
        self.optimistic_resources = optimistic_resources

---

# Here are the steps to ensure each TaskResources has a list of associated tasks to avoid extra lookups:
    if related_tasks_name is not empty:
        if related_tasks_name is not in self.resources_related_tasks_dict:
            # the list includes: [valid to be used, list of CPU, list of MEM, list of walltimes for each related task, list of parallel processes on average, list of CPUs taken, list of assigned CPUs, list of tasks completed in the meantime]
            self.resources_related_tasks_dict[related_tasks_name] = []
        add resources to self.resources_related_tasks_dict[related_tasks_name]
        set resources.related_tasks to self.resources_related_tasks_dict[related_tasks_name]

def add_monitored_resources(self, tid, time_delta_since_start, cpu, mem):
    self.resources[tid].add(time_delta_since_start, cpu, mem)

def book(self, tid, nice_value):
    """
    Reserve the resources for this task with the specified nice value

---

self.retry_counter = [0 for tid in range(len(self.taskuniverse))] # we monitor the number of retries for each task
self.task_retries = [self.workflowspec['stages'][tid].get('retry_count', 0) for tid in range(len(self.taskuniverse))] # retrieves the specific "retry" count for each task from the JSON specification

---

def generate_code_for_task(self, tid, lines):
    actionlogger.debug("Submitting task " + str(self.idtotask[tid]))
    taskspec = self.workflowspec['stages'][tid]
    command = taskspec['cmd']
    working_directory = taskspec['cwd']
    environment = taskspec.get('env')
    # typically:
    # create directory if it doesn't exist
    lines.append('[ ! -d ' + working_directory + ' ] && mkdir ' + working_directory + '\n')
    # change directory
    lines.append('cd ' + working_directory + '\n')
    # set local environment variables
    if environment:
        for key, value in environment.items():
            lines.append('export ' + key + '=' + str(value) + '\n')
    # execute command
    lines.append(command + '\n')
    # unset local environment variables
    if environment:
        for key, value in environment.items():
            lines.append('unset ' + key + '\n')

    # return to previous directory
    lines.append('cd $OLDPWD\n')

---

for p in list(process_list):
    pid = p[1].pid
    tid = p[0]  # the task id of this process
    returncode = 0
    if not self.args.dry_run:
        returncode = p[1].poll()
    if returncode is not None:
        actionlogger.info('Task ' + str(pid) + ' ' + str(tid)+':'+str(self.idtotask[tid]) + ' completed with status ' + str(returncode))
        # account for released resources
        self.resource_manager.unbook(tid)
        self.procstatus[tid]='Completed'
        finished.append(tid)
        #self.validate_resources_running(tid)
        process_list.remove(p)
        if returncode != 0:
            print (str(self.idtotask[tid]) + ' failed ... checking for resubmission')
            # we check if this is due to an "unlucky" circumstance that could be resolved by resubmitting

---

# retain only the tasks required to run based on user's filters
    self.workflowspec = filter_workflow(self.workflowspec, args.target_tasks, args.target_labels)

    if not self.workflowspec['stages']:
        if args.target_tasks:
            print('It seems that some of the selected target tasks are missing from the workflow')
            exit(0)
        print('The workflow is empty. There is nothing to proceed with')
        exit(0)

---

# b) the PATH for compiler includes needed by Cling
cmd = "LC_ALL=C c++ -xc++ -E -v /dev/null 2>&1 | sed -n '/^#include/,${/^ \\/.*++/{p}}'"
proc = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
incpath, err = proc.communicate()
incpaths = [ line.lstrip() for line in incpath.decode().splitlines() ]
joined = ':'.join(incpaths)
if not args.no_rootinit_speedup:
    actionlogger.info("Determined ROOT_CPPSYSINCL=" + joined)
    os.environ['ROOT_CPPSYSINCL'] = joined

speedup_ROOT_Init()

---

# Logging Configuration
parser.add_argument('--action-logfile', help='Specify the log filename for action logs. If not provided, the default will be pipeline_action_#PID.log')
parser.add_argument('--metric-logfile', help='Specify the log filename for metric logs. If not provided, the default will be pipeline_metric_#PID.log')
parser.add_argument('--production-mode', action='store_true', help='Enable production mode')
# This mode activates specific features optimized for non-interactive or production environments, such as automatic file cleanup.
args = parser.parse_args()

def configure_logger(name, log_file, log_level=logging.DEBUG):
    """Function to configure multiple loggers"""
    handler = logging.FileHandler(log_file, mode='w')
    handler.setFormatter(formatter)

    logger = logging.getLogger(name)
    logger.setLevel(log_level)
    logger.addHandler(handler)

    return logger

# Initialize the action logger
actionlogger = configure_logger('pipeline_action_logger', ('pipeline_action_' + str(os.getpid()) + '.log', args.action_logfile)[args.action_logfile is not None], level=logging.DEBUG)

---

# A backup method to obtain all child processes
# when psutil encounters issues (such as PermissionError).
# It yields the same list as psutil.children(recursive=True).
def getChildProcs(basepid):
  script='''
  childprocs() {
  local parent=$1
  if [ ! "$2" ]; then
    child_pid_list=""
  fi
  if [ "$parent" ] ; then
    child_pid_list="$child_pid_list $parent"
    for childpid in $(pgrep -P ${parent}); do
      childprocs $childpid "nottoplevel"
    done;
  fi
  # return through a string list (only if toplevel)
  if [ ! "$2" ]; then
    echo "${child_pid_list}"
  fi
  }
  '''
  script = script + '\n' + 'childprocs ' + str(basepid)
  result = subprocess.check_output(script, shell=True)
  process_list = []
  for pid in result.strip().split():
     try:
         proc=psutil.Process(int(pid))
     except psutil.NoSuchProcess:
         continue

     process_list.append(proc)
  return process_list

---

def get_tar_command(dir='./', flags='cf', findtype='f', filename='checkpoint.tar'):
    return 'find ' + str(dir) + ' -maxdepth 1 -type ' + str(findtype) + ' -print0 | xargs -0 tar ' + str(flags) + ' ' + str(filename)

if location is not None:
    print('Creating a failure checkpoint')
    # let's determine a filename based on ALIEN_PROC_ID, hostname, and PID

    aliprocid = os.environ.get('ALIEN_PROC_ID')
    if aliprocid is None:
        aliprocid = 0

    fn = 'pipeline_checkpoint_ALIENPROC' + str(aliprocid) + '_PID' + str(os.getpid()) + '_HOST' + socket.gethostname() + '.tar'
    actionlogger.info("Checkpointing to file " + fn)
    tarcommand = get_tar_command(filename=fn)
    actionlogger.info("Tarring " + tarcommand)

    # generate a README file with instructions on using the checkpoint
    readmefile = open('README_CHECKPOINT_PID' + str(os.getpid()) + '.txt', 'w')

---

parser.add_argument('-f','--workflowfile', help='The name of the input workflow file', required=True)
parser.add_argument('-jmax','--maxjobs', type=int, help='The maximum number of parallel tasks to run.', default=100)
parser.add_argument('-k','--keep-going', action='store_true', help='Continue executing the pipeline even if errors occur.')
parser.add_argument('--dry-run', action='store_true', help='Display what the pipeline would do without executing it.')
parser.add_argument('--visualize-workflow', action='store_true', help='Generates a visual representation of the workflow.')
parser.add_argument('--target-labels', nargs='+', help='Run the pipeline based on target labels such as "TPC" or "DIGI". This condition is applied in conjunction with --target-tasks using a logical AND.', default=[])
parser.add_argument('-tt','--target-tasks', nargs='+', help='Run the pipeline based on target tasks such as "tpcdigi". By default, all tasks in the graph are executed. Regular expressions are supported.', default=["*"])

---

# empty finished means we need to wait more
    return len(finished)==0

def is_worth_retrying(self, tid):
    # This function checks for specific signatures in logfiles that suggest a retry might be successful.
    # Ideally, this should be customizable by the user, allowing them to define a lambda function or a regex. For now, we have a hardcoded list.
    logfile = self.get_logfile(tid)

    return True #! --> currently, we retry tasks a few times

    # 1) ZMQ_EVENT + interrupted system calls (DPL bug during shutdown)
    # It's unclear if using `grep` is more efficient than native Python text search ...
    # status = os.system('grep "failed setting ZMQ_EVENTS" ' + logfile + ' &> /dev/null')
    # if os.WEXITSTATUS(status) == 0:
    #   return True

    # return False

---

# Collected resources from this
    self.cpu_sampled = None
    self.mem_sampled = None
    # Set these after a task finishes to calculate new estimates for related tasks
    self.walltime = None
    self.cpu_taken = None
    self.mem_taken = None
    # Gathered during monitoring
    self.time_collect = []
    self.cpu_collect = []
    self.mem_collect = []
    # Linked to other tasks of the same type
    self.related_tasks = None
    # Can assign a semaphore
    self.semaphore = None
    # The task's nice value
    self.nice_value = None
    # Indicates if the task's resources are currently booked
    self.booked = False

---

The processes must exist and the tasks file must be writable by the current user.

---

_, alive = psutil.wait_procs(procs, timeout=3)
for p in alive:
    try:
        actionlogger.info("Terminating " + str(p))
        p.terminate()
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

exit(1)

def extract_initial_environment(workflowspec):
    """
    Verifies if the workflow has a specific initial task
    that sets a global environment. Extracts the environment details and removes them from workflowspec.
    """
    init_index = 0 # this must be the first task in the workflow
    globalenv = {}
    initcmd = None
    if workflowspec['stages'][init_index]['name'] == '__global_init_task__':
        env = workflowspec['stages'][init_index].get('env', None)
        if env is not None:
            globalenv = { e : env[e] for e in env }
        cmd = workflowspec['stages'][init_index].get('cmd', None)
        if cmd != 'NO-COMMAND':
            initcmd = cmd

---

for tid in taskids:
             taskspec = self.workflowspec['stages'][tid]
             name = taskspec['name']
             readmefile.write('A checkpoint was created due to a failure in task ' + name + '\n')
             readmefile.write('To reproduce using this checkpoint, follow these steps:\n')
             readmefile.write('a) Set up the necessary O2sim environment with alienv\n')
             readmefile.write('b) Execute: $O2DPG_ROOT/MC/bin/o2_dpg_workflow_runner.py -f workflow.json -tt ' + name + '$ --retry-on-failure 0\n')
           readmefile.close()

           # starting with the base directory
           os.system(tarcommand)

---

# establish the entity responsible for managing resources...
    self.resource_manager = ResourceManager(args.cpu_limit, args.mem_limit, args.maxjobs, args.dynamic_resources, args.optimistic_resources)
    for task in self.workflowspec['stages']:
        # then, include all initial resource estimates
        global_task_name = self.get_global_task_name(task["name"])
        try:
            cpu_relative = float(task["resources"]["relative_cpu"])
        except TypeError:
            cpu_relative = 1
        self.resource_manager.add_task_resources(task["name"], global_task_name, float(task["resources"]["cpu"]), cpu_relative, float(task["resources"]["mem"]), task.get("semaphore"))

    self.procstatus = { tid: 'ToDo' for tid in range(len(self.workflowspec['stages'])) }
    self.taskneeds = { t: set(self.getallrequirements(t)) for t in self.taskuniverse }
    self.stoponfailure = not args.keep_going
    print("Stop on failure ", self.stoponfailure)

---

DOCUMENT:
    thispss = getattr(fullmem, 'pss', 0)  # <--- pss not available on MacOS
    totalPSS += thispss
    totalSWAP += fullmem.swap
    thisuss = fullmem.uss
    totalUSS += thisuss
except (psutil.NoSuchProcess, psutil.AccessDenied):
    pass

---

# <--- end code section for topological sorts

# locate all tasks that rely on a specified task (id); if a cache
# dictionary is provided, we can populate the entire graph in a single pass...
def locate_all_dependent_tasks(possiblenexttask, tid, cache=None):
    c=cache.get(tid) if cache else None
    if c!=None:
        return c

    daughterlist=[tid]
    # potentially recurse
    for n in possiblenexttask[tid]:
        c = cache.get(n) if cache else None
        if c == None:
            c = locate_all_dependent_tasks(possiblenexttask, n, cache)
        daughterlist += c
        if cache is not None:
            cache[n]=c

    if cache is not None:
        cache[tid]=daughterlist
    return list(set(daughterlist))


# a wrapper that accepts some edges, constructs the graph,
# retrieves all topological orderings and some supporting data structures
def analyseGraph(edges, nodes):
    # total number of nodes in the graph
    N = len(nodes)

---

@Property
def is_done(self):
    return self.time_collect and not self.booked

def check_within_limits(self):
    """
    Verify if allocated resources comply with established limits
    """
    cpu_within_limits = True
    mem_within_limits = True
    if self.cpu_assigned > self.resource_boundaries.cpu_limit:
        cpu_within_limits = False
        actionlogger.warning("CPU of task %s exceeds limits %d > %d", self.name, self.cpu_assigned, self.resource_boundaries.cpu_limit)
    if self.cpu_assigned > self.resource_boundaries.mem_limit:
        mem_within_limits = False
        actionlogger.warning("MEM of task %s exceeds limits %d > %d", self.name, self.cpu_assigned, self.resource_boundaries.cpu_limit)
    return cpu_within_limits and mem_within_limits

---

# create the DAG and calculate task weights
workflow = build_dag_properties(self.workflowspec)
if args.visualize_workflow:
    draw_workflow(self.workflowspec)
self.possiblenexttask = workflow['nexttasks']
self.taskweights = workflow['weights']
self.topological_orderings = workflow['topological_ordering']
self.taskuniverse = [ l['name'] for l in self.workflowspec['stages'] ]
# establish a mapping from task ID to task name
self.idtotask = [ 0 for _ in self.taskuniverse ]
self.tasktoid = {}
for i, name in enumerate(self.taskuniverse):
    self.tasktoid[name]=i
    self.idtotask[i]=name

if args.update_resources:
    update_resource_estimates(self.workflowspec, args.update_resources)

---

DOCUMENT:
    plist.append(proc)
    return plist

#
# Code section to identify all topological orderings
# of a Directed Acyclic Graph (DAG). This helps in understanding
# when tasks can be scheduled in parallel.
# Adapted from https://www.geeksforgeeks.org/all-topological-sorts-of-a-directed-acyclic-graph/

# class to represent a graph object
class Graph:

    # Constructor
    def __init__(self, edges, N):

        # A List of Lists to represent an adjacency list
        self.adjList = [[] for _ in range(N)]

        # stores in-degree of a vertex
        # initialize in-degree of each vertex to 0
        self.indegree = [0] * N

        # add edges to the graph
        for (src, dest) in edges:

            # add an edge from source to destination
            self.adjList[src].append(dest)

            # increment in-degree of destination vertex by 1
            self.indegree[dest] = self.indegree[dest] + 1

---

#
# functions for execution; encapsulated in a WorkflowExecutor class
#

class Semaphore:
    """
    Object that can be used as a semaphore
    """
    def __init__(self):
        self.locked = False
    def lock(self):
        self.locked = True
    def unlock(self):
        self.locked = False

---

if os.environ.get('ROOT_LDSYSPATH') != None and os.environ.get('ROOT_CPPSYSINCL') != None:
                  # do nothing if already defined
                  return

               # a) the PATH for system libraries
               # search taken from ROOT TUnixSystem
               cmd='LD_DEBUG=libs LD_PRELOAD=DOESNOTEXIST ls /tmp/DOESNOTEXIST 2>&1 | grep -m 1 "system search path" | sed \'s/.*=//g\' | awk \'//{print $1}\''
               proc = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
               libpath, err = proc.communicate()
               if not args.no_rootinit_speedup:
                  print("setting up ROOT system")
                  os.environ['ROOT_LDSYSPATH'] = libpath.decode()

---

# remove original file
          os.remove(logf)
          os.remove(donef)
          os.remove(timef)

    # print error message when no progress can be made
    def noprogress_errormsg(self):
        # TODO: instead of repeating this, link to the documentation for further details.
        msg = """Scheduler runtime error: The scheduler is unable to proceed even though there is a non-empty candidate set.

    """

---

Additionally, it could be beneficial to run the workflow without the resource-aware, dynamic scheduler.
This can be achieved by converting the JSON workflow into a linearized shell script and directly executing it.
To do this, use the `--produce-script myscript.sh` option.
"""
        print (msg, file=sys.stderr)

    def execute(self):
        self.start_time = time.perf_counter()
        psutil.cpu_percent(interval=None)
        os.environ['JOBUTILS_SKIPDONE'] = "ON"
        errorencountered = False

        def speedup_ROOT_Init():
               """initialize some environment variables that speed up ROOT initialization
               and prevent ROOT from spawning numerous short-lived child processes"""

               # only perform this on Linux
               if platform.system() != 'Linux':
                  return

---

DOCUMENT:
    outF.writelines(lines)
    outF.close()

    def production_endoftask_hook(self, tid):
        # This function runs at the end of a successful task, designed for GRID productions.
        # Currently, it archives log files, done files, and time files from jobutils.
        # In the future, it could perform more generic tasks such as dynamic cleanup of intermediate files.
        # Special attention is needed with the continue feature as `_done` files are stored elsewhere now.
        actionlogger.info("Cleaning up log files for task " + str(tid))
        logf = self.get_logfile(tid)
        donef = self.get_done_filename(tid)
        timef = logf + "_time"

        # Add files to the tar archive
        tf = tarfile.open(name="pipeline_log_archive.log.tar", mode='a')
        if tf is not None:
            tf.add(logf)
            tf.add(donef)
            tf.add(timef)
            tf.close()

---

# CPU part
# Retrieve or add a process
cachedproc = self.pid_to_psutilsproc.get(p.pid)
if cachedproc is not None:
    try:
        thiscpu = cachedproc.cpu_percent(interval=None)
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        thiscpu = 0.
    totalCPU += thiscpu
    # thisresource = {'iter':self.internalmonitorid, 'pid': p.pid, 'cpu':thiscpu, 'uss':thisuss/1024./1024., 'pss':thispss/1024./1024.}
    # metriclogger.info(thisresource)
else:
    self.pid_to_psutilsproc[p.pid] = p
    try:
        self.pid_to_psutilsproc[p.pid].cpu_percent()
    except (psutil.NoSuchProcess, psutil.AccessDenied):
        pass

---

# edges
for node in workflowspec['stages']:
    toindex = nametoindex[node['name']]
    for req in node['needs']:
        fromindex = nametoindex[req]
        dot.edge(str(fromindex), str(toindex))

dot.render('workflow.gv')

# constructs the graph from a "taskuniverse" list and builds accompanying structures tasktoid and idtotask
def build_graph(taskuniverse, workflowspec):
    tasktoid = { t[0]['name']: i for i, t in enumerate(taskuniverse, 0) }
    # print (tasktoid)

    nodes = []
    edges = []
    for t in taskuniverse:
        nodes.append(tasktoid[t[0]['name']])
        for n in t[0]['needs']:
            edges.append((tasktoid[n], tasktoid[t[0]['name']]))

    return (edges, nodes)


# reads JSON file into a dictionary, e.g., for workflow specification
def load_json(workflowfile):
    with open(workflowfile) as fp:
        workflowspec = json.load(fp)
    return workflowspec

---

DOCUMENT:
    self.resources.append(resources)
        # perform these steps to ensure all related TaskResources share the same Semaphore object, eliminating the need for a lookup
        if semaphore_string:
            if semaphore_string not in self.semaphore_dict:
                self.semaphore_dict[semaphore_string] = Semaphore()
            resources.semaphore = self.semaphore_dict[semaphore_string]

---

if globalPSS exceeds self.resource_manager.resource_boundaries.mem_limit:
            metriclogger.info('*** MEMORY LIMIT PASSED !! ***')
            # --> This could be utilized for corrective actions like terminating ongoing jobs
            # (or more effectively, hibernating them)

    def waitforany(self, process_list, finished, failingtasks):
       """
       Iterate through all submitted tasks and verify their completion status.

       1. If the process is still running, take no action.
       2. If the process has completed, retrieve its return value, update the finished and failingtasks lists.
       2.1 Free the utilized resources.
       2.2 Release the taken resources and hand them over to the ResourceManager.
       """
       failuredetected = False
       failingpids = []
       if len(process_list) == 0:
           return False

---

for ok_to_submit_impl, should_break in ((ok_to_submit_default, True), (ok_to_submit_backfill, False)):
    tid_index = 0
    while tid_index < len(tids_copy):
        tid = tids_copy[tid_index]
        res = self.resources[tid]

        actionlogger.info("Setup resources for task %s, cpu: %f, mem: %f", res.name, res.cpu_assigned, res.mem_assigned)
        tid_index += 1

        if (res.semaphore is not None and res.semaphore.locked) or res.booked:
            continue

        nice_value = ok_to_submit_impl(res)
        if nice_value is not None:
            # if we receive a non-None nice value, this indicates that the task is ready to be submitted
            res.nice_value = nice_value
            # provide the tid and its assigned nice value
            yield tid, nice_value

---

DOCUMENT:
    res.nice_value = nice_value
    res.booked = True
    if res.semaphore is not None:
        res.semaphore.lock()
    if nice_value != self.nice_default:
        self.n_procs_backfill += 1
        self.cpu_booked_backfill += res.cpu_assigned
        self.mem_booked_backfill += res.mem_assigned
        return
    self.n_procs += 1
    self.cpu_booked += res.cpu_assigned
    self.mem_booked += res.mem_assigned

---

Explanation: This usually occurs when the estimated resource needs for certain tasks in the workflow surpass the available CPU cores or memory (either explicitly or implicitly set using the --cpu-limit and --mem-limit options). It is often observed on laptops with ≤16GB of RAM if a task requires approximately 16GB. To address this, one can instruct the scheduler to use a slightly higher memory limit with a specific --mem-limit option (for example, `--mem-limit 20000` to allocate 20GB). This adjustment may be effective if the actual resource usage of the tasks is lower than anticipated (due to running smaller test cases).

---

# If a task was marked for retry, we reinsert it into the candidate list.
    if len(self.tids_marked_toretry) > 0:
        # First, we remove these tasks from those marked as finished.
        for t in self.tids_marked_toretry:
            finished = [ x for x in finished if x != t ]
            finishedtasks = [ x for x in finishedtasks if x != t ]

        candidates = candidates + self.tids_marked_toretry
        self.tids_marked_toretry = []

---

DOCUMENT:
    self.alternative_envs = {} # a dictionary to map task IDs to alternative software environments (to be applied per task)
      # initialize alternative software environments
      self.init_alternative_software_environments()

    def SIGHandler(self, signum, frame):
       """
       essentially forcing the shutdown of all child processes
       """
       actionlogger.info("Signal " + str(signum) + " received")
       try:
           procs = psutil.Process().children(recursive=True)
       except (psutil.NoSuchProcess):
           pass
       except (psutil.AccessDenied, PermissionError):
           procs = getChildProcs(os.getpid())

       for p in procs:
           actionlogger.info("Terminating " + str(p))
           try:
             p.terminate()
           except (psutil.NoSuchProcess, psutil.AccessDenied):
             pass

---

#!/usr/bin/env python3

# initiated in February 2021, by sandro.wenzel@cern.ch

import re
import subprocess
import time
import json
import logging
import os
import signal
import socket
import sys
import traceback
import platform
import tarfile
try:
    from graphviz import Digraph
    have_graphviz = True
except ImportError:
    have_graphviz = False

formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')

sys.setrecursionlimit(100000)

import argparse
import psutil
max_system_mem = psutil.virtual_memory().total

sys.path.append(os.path.join(os.path.dirname(__file__), '.', 'o2dpg_workflow_utils'))
from o2dpg_workflow_utils import read_workflow

# specifying command line options
parser = argparse.ArgumentParser(description='Parallel execution of an (O2-DPG) DAG data/job pipeline under resource constraints.',
                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)

---

def analyse_files_and_connections(self):
    for process_id, files in self.pid_to_files.items():
        for file in files:
            print("F" + str(file) + " : " + str(process_id))
    for process_id, connections in self.pid_to_connections.items():
        for connection in connections:
            print("C" + str(connection) + " : " + str(process_id))
        #print(str(process_id) + " CONS " + str(connection))
    try:
        # check for intersections
        for p1, files1 in self.pid_to_files.items():
            for p2, files2 in self.pid_to_files.items():
                if p1 != p2:
                    if isinstance(files1, set) and isinstance(files2, set):
                        if len(files1) > 0 and len(files2) > 0:
                            try:
                                inters = files1.intersection(files2)
                            except Exception:
                                print('Exception during intersect inner')
                                pass
                            if len(inters) > 0:

---

if mem_sampled > self.resource_boundaries.mem_limit:
            actionlogger.warning("Sampled memory (%.2f) exceeds assigned memory limit (%.2f)", mem_sampled, self.resource_boundaries.mem_limit)
        elif mem_sampled <= 0:
            actionlogger.debug("Sampled memory for %s is %.2f which is less than or equal to zero, setting it to the previously assigned value %.2f", self.name, mem_sampled, self.mem_assigned)
            mem_sampled = self.mem_assigned

        for res in self.related_tasks:
            if res.is_done or res.booked:
                continue
            res.cpu_assigned = cpu_sampled * res.cpu_relative
            res.mem_assigned = mem_sampled
            # This task has been run before, so we stay optimistic and limit the resources if the sampled ones exceed the limits
            res.limit_resources()

class ResourceManager:
    """
    Central class for managing resources

    - CPU limits
    - MEM limits
    - Semaphores

    Provides an entry point to set and query resources for updates.

    """

---

if there are no candidates and the process list is empty:
    break
except any errors:
    exc_type, exc_obj, exc_tb = sys.exc_info()
    fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
    print(exc_type, fname, exc_tb.tb_lineno)
    traceback.print_exc()
    print ('Cleaning up ')

    self.SIGHandler(0,0)

endtime = time.perf_counter()
statusmsg = "success"
if errors occurred:
    statusmsg = "with failures"

print ('\n**** Pipeline done ' + statusmsg + ' (global_runtime : {:.3f}s) *****\n'.format(endtime-self.start_time))
actionlogger.debug("global_runtime : {:.3f}s".format(endtime-self.start_time))
return errors occurred

---

# verify for overlaps
            #for p1, s1 in self.pid_to_files.items():
            #    for p2, s2 in self.pid_to_files.items():
            #        if p1 != p2 and len(s1.intersection(s2)) != 0:
            #            print('Overlap detected between files ' + str(p1) + ' ' + str(p2) + ' ' + str(s1.intersection(s2)))
        except Exception as e:
            exc_type, exc_obj, exc_tb = sys.exc_info()
            fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
            print(exc_type, fname, exc_tb.tb_lineno)
            print('Error during overlap check')
            pass

    def check_valid_candidate(self, candid, finishedtasks):
        if self.procstatus[candid] != 'ToDo':
            return False
        required_tasks = set([self.tasktoid[t] for t in self.taskneeds[self.idtotask[candid]]])
        if set(finishedtasks).intersection(required_tasks) == required_tasks:
            return True
        return False

---

else:
    failuredetected = True
    failingpids.append(pid)
    failingtasks.append(tid)

if failuredetected and self.stoponfailure:
    actionlogger.info('Stopping pipeline due to failure in stages with PID ' + str(failingpids))
    # self.analyse_files_and_connections()
    if self.args.stdout_on_failure:
        self.cat_logfiles_tostdout(failingtasks)
    self.send_checkpoint(failingtasks, self.args.checkpoint_on_failure)
    self.stop_pipeline_and_exit(process_list)

# an empty finished list means we need to wait
return len(finished)==0

---

elif should_break:
    # halt execution if resources required by the subsequent task are insufficient
    break


class WorkflowExecutor:
    # Constructor
    def __init__(self, workflowfile, args, jmax=100):
        self.args = args
        self.is_productionmode = args.production_mode == True  # os.getenv("ALIEN_PROC_ID") != None
        self.workflowfile = workflowfile
        self.workflowspec = load_json(workflowfile)
        self.globalinit = self.extract_global_environment(self.workflowspec)  # set up global environment settings
        for e in self.globalinit['env']:
            if os.environ.get(e, None) is None:
                value = self.globalinit['env'][e]
                actionlogger.info("Applying global environment from init section " + str(e) + " : " + str(value))
                os.environ[e] = str(value)

---

ENTRPOINT TO MANAGE AND QUERY RESOURCES FOR UPDATES.

ABLE TO DETERMINE IF A TASK CAN BE EXECUTED BASED ON CURRENT RESOURCE USAGE.
ALLOWS RESOURCES TO BE RESERVED AND RELEASED.
"""
INITIALIZE MEMBERS WITH DEFAULT VALUES
"""
# STORE ALL TASK RESOURCES
self.resources = []

# STORE COMMON OBJECTS TO BE SHARED AMONG TASK RESOURCES FOR EFFICIENCY
self.resources_related_tasks_dict = {}
self.semaphore_dict = {}

# MANAGE GLOBAL RESOURCE SETTINGS SUCH AS CPU AND MEMORY LIMITS
self.resource_boundaries = ResourceBoundaries(cpu_limit, mem_limit, dynamic_resources, optimistic_resources)

---

def add_task_resources(self, name, related_tasks_name, cpu, cpu_relative, mem, semaphore_string=None):
    """
    Create and append a new TaskResources object
    """
    resources = TaskResources(len(self.resources), name, cpu, cpu_relative, mem, self.resource_boundaries)
    if not resources.is_within_limits() and not self.resource_boundaries.optimistic_resources:
        # exit if we cannot proceed
        print(f"Resources for task {name} exceed the boundaries.\nCPU: {cpu} (estimate) vs. {self.resource_boundaries.cpu_limit} (boundary)\nMEM: {mem} (estimated) vs. {self.resource_boundaries.mem_limit} (boundary).")
        print("Use --optimistic-resources with the runner to proceed anyway.")
        exit(1)
    # if we reach this point, either the resources are within limits or the user opted for an optimistic run, in which case we limit the resources, typically to the specified CPU and memory limits.

---

DOCUMENT:
    # return False


    def cat_logfiles_tostdout(self, taskids):
        # In the event of errors, we can display the logfiles for each taskname
        # directly on the standard output. Assuming the taskname corresponds to a "taskname.log" file.
        for tid in taskids:
            logfile = self.get_logfile(tid)
            if os.path.exists(logfile):
                print(' ----> START OF LOGFILE ', logfile, ' -----')
                os.system('cat ' + logfile)
                print(' <---- END OF LOGFILE ', logfile, ' -----')

    def send_checkpoint(self, taskids, location):
        # Creates a tarball that includes all base directory files (timeframe-independent) and directories containing corrupted timeframes.
        # Then, it copies this tarball to a specified ALIEN location. This is not a core function, but rather a tool for managing error conditions on the GRID.

---

def unbook(self, tid):
    """
    Release the resources allocated to this task.
    """
    res = self.resources[tid]
    res.booked = False
    if self.resource_boundaries.dynamic_resources:
        res.sample_resources()
    if res.semaphore is not None:
        res.semaphore.unlock()
    if res.nice_value != self.nice_default:
        self.cpu_booked_backfill -= res.cpu_assigned
        self.mem_booked_backfill -= res.mem_assigned
        self.n_procs_backfill -= 1
        if self.n_procs_backfill <= 0:
            self.cpu_booked_backfill = 0
            self.mem_booked_backfill = 0
        return
    self.n_procs -= 1
    self.cpu_booked -= res.cpu_assigned
    self.mem_booked -= res.mem_assigned
    if self.n_procs <= 0:
        self.cpu_booked = 0
        self.mem_booked = 0

---

# backtrack: remove the current node from the path and
# mark it as undiscovered
path.pop()
discovered[v] = False

# record the valid sequence
if len(path) == N:
    allpaths.append(path.copy())


# obtain all topological sequences of a given directed acyclic graph as a list
def printAllTopologicalOrders(graph, maxnumber=1):
    # determine the number of nodes in the graph
    N = len(graph.adjList)

    # create an auxiliary array to track if a vertex is discovered
    discovered = [False] * N

    # list to hold the topological sequence
    path = []
    allpaths = []
    # find all topological sequences and print them
    findAllTopologicalOrders(graph, path, discovered, N, allpaths, maxnumber=maxnumber)
    return allpaths

# <--- end code section for topological sorts

---

if the directory does not exist in workdir:
                  create it using os.makedirs()

      set self.procstatus[tid] to 'Running'
      if a dry_run is specified:
          construct drycommand as "echo \( " + str(self.scheduling_iteration) + " : would do " + str(self.workflowspec['stages'][tid]['name']) + "\)"
          return subprocess.Popen(['/bin/bash','-c',drycommand], working_directory=workdir)

---

# second file logger setup
metriclogger = setup_logger('pipeline_metric_logger', ('pipeline_metric_' + str(os.getpid()) + '.log', args.action_logfile)[args.action_logfile!=None])

# Log the imposed memory and CPU limits, along with other relevant metadata immediately
_ , meta = read_workflow(args.workflowfile)
meta["cpu_limit"] = args.cpu_limit
meta["mem_limit"] = args.mem_limit
meta["workflow_file"] = os.path.abspath(args.workflowfile)
meta["target_task"] = args.target_tasks
meta["rerun_from"] = args.rerun_from
meta["target_labels"] = args.target_labels
metriclogger.info(meta)

# For debugging purposes when terminal access is unavailable
# TODO: Integrate into the standard logger
def send_webhook(hook, t):
    if hook!=None:
      command="curl -X POST -H 'Content-type: application/json' --data '{\"text\":\" " + str(t) + "\"}' " + str(hook) + " &> /dev/null"
      os.system(command)

---

# candidate list trivial
nextjobtrivial = { n:[] for n in nodes }
# initial nodes
nextjobtrivial[-1] = nodes
for edge in edges:
    nextjobtrivial[edge[0]].append(edge[1])
    if nextjobtrivial[-1].count(edge[1]):
        nextjobtrivial[-1].remove(edge[1])

# determine topological orderings of the graph
# construct a graph from the edges
graph = Graph(edges, N)
orderings = printAllTopologicalOrders(graph)

return (orderings, nextjobtrivial)


def visualize_workflow(workflowspec):
    if not havegraphviz:
        print('graphviz not installed, unable to visualize workflow')
        return

    dot = Digraph(comment='MC workflow')
    nametoindex={}
    index=0
    # node creation
    for stage in workflowspec['stages']:
        name=stage['name']
        nametoindex[name]=index
        dot.node(str(index), name)
        index=index+1

---

# resets the done status for tasks that need to be re-run
    def remove_done_flag(self, listoftaskids):
       """
       Remove <task>.log_done files for the specified task IDs
       """
       for tid in listoftaskids:
          done_filename = self.get_done_filename(tid)
          task_name=self.workflowspec['stages'][tid]['name']
          if args.dry_run:
              print ("Would reset task " + task_name + " to be re-run")
          else:
              print ("Resetting task " + task_name + " to be re-run")
              if os.path.exists(done_filename) and os.path.isfile(done_filename):
                  os.remove(done_filename)

    # launches a task as a subprocess and logs the Popen instance
    def submit(self, tid, nice):
      """
      Submit a task for execution
      """

---

# envfilename = "taskenv_" + str(tid) + ".json"
      # with open(envfilename, "w") as file:
      #    json.dump(taskenv, file, indent=2)

      p = psutil.Popen(['/bin/bash','-c',c], cwd=workdir, env=taskenv)
      try:
          p.nice(nice)
      except (psutil.NoSuchProcess, psutil.AccessDenied):
          actionlogger.error('Failed to set nice value of ' + str(p.pid) + ' to ' + str(nice))

      return p

    def ok_to_skip(self, tid):
        """
        Determine if the task can be skipped based on the existence of <task>.log_done
        """
        done_filename = self.get_done_filename(tid)
        if os.path.exists(done_filename) and os.path.isfile(done_filename):
          return True
        return False

    def try_job_from_candidates(self, taskcandidates, finished):
       """
       Attempt to schedule the next tasks
       """

---

okcache = {}
# create the complete list of targets
full_target_list = [ t for t in workflowspec['stages'] if task_matches(t['name']) and task_matches_labels(t) and canBeDone(t,okcache) ]
full_target_name_list = [ t['name'] for t in full_target_list ]

# generate the full dependency list for a task t
def getallrequirements(t):
    _l=[]
    for r in t['needs']:
        fulltask = workflowspec['stages'][tasknametoid[r]]
        _l.append(fulltask)
        _l=_l+getallrequirements(fulltask)
    return _l

full_requirements_list = [ getallrequirements(t) for t in full_target_list ]

# flatten and retrieve the names only
full_requirements_name_list = list(set([ item['name'] for sublist in full_requirements_list for item in sublist ]))

---

# filters the original workflowspec based on specified targets or labels
# returns a new workflowspec
def filter_workflow(workflowspec, targets=[], targetlabels=[]):
    if len(targets) == 0:
        return workflowspec
    if len(targetlabels) == 0 and len(targets) == 1 and targets[0] == "*":
        return workflowspec

    transformedworkflowspec = workflowspec

    def task_matches(t):
        for filt in targets:
            if filt == "*":
                return True
            if re.match(filt, t) is not None:
                return True
        return False

    def task_matches_labels(t):
        # it's acceptable if no labels are provided at all
        if len(targetlabels) == 0:
            return True

        for l in t['labels']:
            if l in targetlabels:
                return True
        return False

---

mem_sampled = 0
cpu_sampled = []
for res in self.related_tasks:
    if res.is_done:
        mem_sampled = max(mem_sampled, res.mem_sampled)
        cpu_sampled.append(res.cpu_sampled)
cpu_sampled = sum(cpu_sampled) / len(cpu_sampled)

# This task has already executed with the given resources, so we'll set it to the limit
if cpu_sampled > self.resource_boundaries.cpu_limit:
    actionlogger.warning("Sampled CPU (%.2f) is greater than assigned CPU limit (%.2f)", cpu_sampled, self.resource_boundaries.cpu_limit)
elif cpu_sampled < 0:
    actionlogger.debug("Sampled CPU for %s is %.2f < 0, setting to previously assigned value %.2f", self.name, cpu_sampled, self.cpu_assigned)
    cpu_sampled = self.cpu_assigned

---

DOCUMENT:
    taskenv = os.environ.copy()
      # apply specific (non-default) software version, if available
      # (this was configured earlier)
      alternative_env = self.alternative_envs.get(tid, None)
      if alternative_env is not None and len(alternative_env) > 0:
          actionlogger.info('Applying alternative software environment to task ' + self.idtotask[tid])
          if alternative_env.get('TERM') is not None:
              # the environment is a complete environment
              taskenv = {}
              taskenv = alternative_env
          else:
            for entry in alternative_env:
              # overwrite existing values
              taskenv[entry] = alternative_env[entry]

      # add task-specific environment settings
      if self.workflowspec['stages'][tid].get('env') != None:
          taskenv.update(self.workflowspec['stages'][tid]['env'])

---

=document=
    self.scheduling_iteration = 0 # track the number of attempts to schedule new tasks
      self.process_list = []  # list of currently active processes with standard priority
      self.backfill_process_list = [] # list of currently active processes with low backfill priority (uncertain if necessary)
      self.pid_to_psutilsproc = {}  # cache of psutil processes for resource monitoring
      self.pid_to_files = {} # can automatically detect which files are produced by each task (to a certain extent)
      self.pid_to_connections = {} # can automatically detect which connections are opened by each task (to a certain extent)
      signal.signal(signal.SIGINT, self.SIGHandler)
      signal.siginterrupt(signal.SIGINT, False)
      self.internalmonitorcounter = 0 # internal use
      self.internalmonitorid = 0 # internal use
      self.tids_marked_toretry = [] # sometimes tasks may need to be retried (if they were "unlucky") and they are listed here

---

# Resources
parser.add_argument('--update-resources', dest="update_resources", help='Read resource estimates from a JSON and apply where possible.')
parser.add_argument("--dynamic-resources", dest="dynamic_resources", action="store_true", help="Adjust task resource estimates based on completed related tasks.") # dynamically update resources
parser.add_argument('--optimistic-resources', dest="optimistic_resources", action="store_true", help="Attempt to run the workflow despite potential underestimation of resource needs for some tasks.")
parser.add_argument("--n-backfill", dest="n_backfill", type=int, default=1)
parser.add_argument('--mem-limit', help='Define memory limit as a scheduling constraint (in MB)', default=0.9*max_system_mem/1024./1024, type=float)
parser.add_argument('--cpu-limit', help='Specify CPU limit (core count)', default=8, type=float)
parser.add_argument('--cgroup', help='Run the pipeline under a specified cgroup (e.g., 8coregrid) to emulate resource constraints.')

---

DOCUMENT:
    def restrict_resources(self, cpu_limit=None, mem_limit=None):
        """
        Restrict the resources for this task
        """
        if cpu_limit is None:
            cpu_limit = self.resource_boundaries.cpu_limit
        if mem_limit is None:
            mem_limit = self.resource_boundaries.mem_limit
        self.cpu_assigned = min(self.cpu_assigned, cpu_limit)
        self.mem_assigned = min(self.mem_assigned, mem_limit)

    def record_resources(self, time_passed, cpu, mem):
        """
        A brief interface to record measured resources after a certain time
        """
        self.time_collect.append(time_passed)
        self.cpu_collect.append(cpu)
        self.mem_collect.append(mem)

    def capture_resources(self):
        """
        If this task has concluded, sample CPU and MEM for all tasks that have yet to start
        """
        if not self.is_done:
            return

---

# adjust the resource requirements for a workflow using JSON-specified resources
def update_resource_estimates(workflow, resource_json):
    resource_dict = load_json(resource_json)
    stages = workflow["stages"]

    for task in stages:
        if task["timeframe"] >= 1:
            name = "_".join(task["name"].split("_")[:-1])
        else:
            name = task["name"]

        if name not in resource_dict:
            continue

        new_resources = resource_dict[name]

        # memory
        newmem = new_resources.get("mem", None)
        if newmem is not None:
            oldmem = task["resources"]["mem"]
            actionlogger.info("Updating the mem estimate for " + task["name"] + " from " + str(oldmem) + " to " + str(newmem))
            task["resources"]["mem"] = newmem
        newcpu = new_resources.get("cpu", None)

---

if self.is_productionmode:
    # we can perform some general cleanup of completed tasks in non-interactive/GRID mode
    # TODO: this process can be run asynchronously
    for _t in finished_from_started:
        self.production_endoftask_hook(_t)

    # if a task was flagged as "failed" and we reach this point (due to using --keep-going) ...
    # we need to remove the pid from the list of finished tasks to avoid processing their children
    if len(failing) > 0:
        # remove these from the list of finished tasks to prevent further processing of their children
        errorencountered = True
        for t in failing:
            finished = [ x for x in finished if x != t ]
            finishedtasks = [ x for x in finishedtasks if x != t ]

---

DOCUMENT:
    self.process_list=[] # list of tuples containing node IDs and Popen subprocess instances

        finishedtasks=[] # global list tracking completed tasks

        try:

            while True:
                # arrange candidate list based on task weights
                candidates = [ (tid, self.taskweights[tid]) for tid in candidates ]
                candidates.sort(key=lambda tup: (tup[1][0],-tup[1][1])) # prioritize tasks with smaller weights and matching timeframes, then prioritize important tasks within the same timeframe
                # eliminate weights from the list
                candidates = [ tid for tid,_ in candidates ]

---

class TaskResources:
    """
    Container holding the resources allocated to a single task
    """
    def __init__(self, tid, name, cpu, cpu_relative, mem, resource_boundaries):
        # the task ID associated with these resources
        self.tid = tid
        self.name = name
        # initial CPU and memory allocations (permanent)
        self.cpu_assigned_original = cpu
        self.mem_assigned_original = mem
        # relative CPU adjustment, to be applied to sampled CPU; set by the user, for example, to enable backfilling tasks
        # only applies when sampling resources; permanent
        self.cpu_relative = cpu_relative if cpu_relative else 1
        # current CPU and memory allocations (temporary)
        self.cpu_assigned = cpu
        self.mem_assigned = mem
        # overall resource constraints
        self.resource_boundaries = resource_boundaries
        # sampled resources for this task
        self.cpu_sampled = None
        self.mem_sampled = None

---

finished = []  # --> to account for finished tasks that are already done or skipped
actionlogger.debug('Sorted current candidates: ' + str([(c, self.idtotask[c]) for c in candidates]))
self.try_job_from_candidates(candidates, finished)
if len(candidates) > 0 and self.process_list == []:
    self.noprogress_errormsg()
    send_webhook(self.args.webhook, "Unable to make further progress: Quitting")
    errorencountered = True
    break

---

DOCUMENT:
    finished_from_started = []  # to track tasks that finished even if they were started later
    failing = []
    while self.waitforany(self.process_list, finished_from_started, failing):
        if not args.dry_run:
            self.monitor(self.process_list)  # should this be asynchronous?
            time.sleep(1)  # consider a smaller initial wait time
        else:
            time.sleep(0.001)

    finished = finished + finished_from_started
    actionlogger.debug("finished now: " + str(finished_from_started))
    finishedtasks = finishedtasks + finished

---

for tid, proc in process_list:

            # proc is a Popen object
            pid = proc.pid
            if self.pid_to_files.get(pid) is None:
                self.pid_to_files[pid] = set()
                self.pid_to_connections[pid] = set()
            try:
                psutilProcs = [proc]
                # utilize psutil for CPU measurement
                psutilProcs += proc.children(recursive=True)
            except psutil.NoSuchProcess:
                continue

            except (psutil.AccessDenied, PermissionError):
                psutilProcs += getChildProcs(pid)

---

DOCUMENT:
    if len(inters) > 0:
        print('FILE Intersection ' + str(p1) + ' ' + str(p2) + ' ' + str(inters))
    # check for intersections
    for p1, s1 in self.pid_to_connections.items():
        for p2, s2 in self.pid_to_connections.items():
            if p1 != p2:
                if type(s1) is set and type(s2) is set:
                    if len(s1) > 0 and len(s2) > 0:
                        try:
                            inters = s1.intersection(s2)
                        except Exception:
                            print('Exception during intersect inner')
                            pass
                        if len(inters) > 0:
                            print('CON Intersection ' + str(p1) + ' ' + str(p2) + ' ' + str(inters))

---

# The location must be an alien path such as alien:///foo/bar/
           copycommand='alien.py cp ' + fn + ' ' + str(location) + '@disk:1'
           actionlogger.info("Copying to alien " + copycommand)
           os.system(copycommand)

def initialize_alternative_software_environments(self):
    """
    Initializes alternative software environments for specific tasks if there
    is an annotation in the workflow specification.
    """

    environment_cache = {}
    # Iterate through all tasks and set up the environment
    for taskid in range(len(self.workflowspec['stages'])):
        packagestr = self.workflowspec['stages'][taskid].get("alternative_alienv_package")
        if packagestr is None:
            continue

        if environment_cache.get(packagestr) is None:
            environment_cache[packagestr] = get_alienv_software_environment(packagestr)

        self.alternative_envs[taskid] = environment_cache[packagestr]

---

if the task is worth retrying (tid) and either the retry counter is less than the specified retry limit or the retry counter is less than the task-specific retry count:
    print ('Task ' + str(self.idtotask[tid]) + ' will be retried')
    actionlogger.info ('Task ' + str(self.idtotask[tid]) + ' failed but is marked for retry')
    self.tids_marked_toretry.append(tid)
    self.retry_counter[tid] += 1

---

# inner "lambda" helper determining if a task "name" is required by specified targets
def needed_by_targets(name):
    if name in full_target_name_list:
        return True
    if name in full_requirements_name_list:
        return True
    return False

# we then replicate all entries that match the targets along with their requirements
transformedworkflowspec['stages'] = [l for l in workflowspec['stages'] if needed_by_targets(l['name'])]
return transformedworkflowspec


# constructs topological orderings (for each timeframe)
def build_dag_properties(workflowspec):
    globaltaskuniverse = [(l, i) for i, l in enumerate(workflowspec['stages'], 1)]
    timeframeset = set(l['timeframe'] for l in workflowspec['stages'])

    edges, nodes = build_graph(globaltaskuniverse, workflowspec)
    tup = analyseGraph(edges, nodes.copy())
    #
    global_next_tasks = tup[1]

---

# evaluate CPU
    okcpu = (self.cpu_booked_backfill + res.cpu_assigned <= self.resource_boundaries.cpu_limit)
    okcpu = okcpu and (self.cpu_booked + self.cpu_booked_backfill + res.cpu_assigned <= backfill_cpu_factor * self.resource_boundaries.cpu_limit)
    # evaluate MEM
    okmem = (self.mem_booked + self.mem_booked_backfill + res.mem_assigned <= backfill_mem_factor * self.resource_boundaries.mem_limit)
    actionlogger.debug ('Condition check --backfill-- for  ' + str(res.tid) + ':' + res.name + ' CPU ' + str(okcpu) + ' MEM ' + str(okmem))

    return self.nice_backfill if (okcpu and okmem) else None

if self.n_procs + self.n_procs_backfill >= self.procs_parallel_max:
    # in this scenario, no actions can be taken
    return

---

DOCUMENT:
    def get_global_task_name(self, name):
        """
        Obtain the global task name

        Tasks are considered related if only the suffix _<i> differs
        """
        tokens = name.split("_")
        try:
            int(tokens[-1])
            return "_".join(tokens[:-1])
        except ValueError:
            pass
        return name

    def getallrequirements(self, task_name):
        """
        Retrieve all requirements for a task based on its name
        """
        l = []
        for required_task_name in self.workflowspec['stages'][self.tasktoid[task_name]]['needs']:
            l.append(required_task_name)
            l += self.getallrequirements(required_task_name)
        return l

---

# identifying new candidates
for tid in finished:
    if self.possiblenexttask.get(tid) is not None:
        potential_candidates = list(self.possiblenexttask[tid])
        for candidate in potential_candidates:
            # check if it is a valid candidate:
            if self.is_good_candidate(candidate, finishedtasks) and candidates.count(candidate) == 0:
                candidates.append(candidate)

actionlogger.debug("New candidates identified as: " + str(candidates))
send_webhook(self.args.webhook, "New candidates identified as: " + str(candidates))

---

# cpu
    if newcpu is not None:
        oldcpu = task["resources"]["cpu"]
        rel_cpu = task["resources"]["relative_cpu"]
        if rel_cpu is not None:
            # honor the relative CPU settings
            # By default, the CPU value in the workflow is already adjusted if relative_cpu is specified.
            # The new estimate, however, is not yet adjusted so it must be done here.
            newcpu *= rel_cpu
        actionlogger.info("Modifying cpu estimate for " + task["name"] + " from " + str(oldcpu) + " to " + str(newcpu))
        task["resources"]["cpu"] = newcpu

# a function to read a software environment from alienv into a python dictionary
def get_alienv_software_environment(packagestring):
    """
    packagestring can be a string like O2::v202298081-1,O2Physics::xxx representing packages
    published on CVMFS ... or ... a file containing the software environment to apply directly
    """

---

def stop_pipeline_and_exit(self, process_list):
    # terminate all remaining jobs
    for p in process_list:
       p[1].terminate()

    exit(1)


def monitor(self, process_list):
    """
    Iterate through all active tasks and retrieve their current resource usage.

    Resource usage is aggregated for each task and its offspring.

    Pass the CPU, PSS, USS, niceness, and current time to the metriclogger.

    Issue a warning if the total PSS surpasses the allocated memory limit.
    """
    self.internalmonitorcounter+=1
    if self.internalmonitorcounter % 5 != 0:
        return

    self.internalmonitorid+=1

    globalCPU=0.
    globalPSS=0.
    resources_per_task = {}

    for tid, proc in process_list:

---

# Recursive function to discover all possible topological orderings of a given DAG
def findAllTopologicalOrders(graph, path, discovered, N, allpaths, maxnumber=1):
    if len(allpaths) >= maxnumber:
        return

    # process each vertex
    for v in range(N):

        # proceed only if the in-degree of the current node is 0 and
        # the current node has not been processed yet
        if graph.indegree[v] == 0 and not discovered[v]:

            # for each adjacent vertex u of v, decrease the in-degree of u by 1
            for u in graph.adjList[v]:
                graph.indegree[u] -= 1

            # add the current node to the path and mark it as discovered
            path.append(v)
            discovered[v] = True

            # recursively call the function
            findAllTopologicalOrders(graph, path, discovered, N, allpaths)

            # backtrack: restore in-degree information for the current node
            for u in graph.adjList[v]:
                graph.indegree[u] += 1

---

# Initially, the base directory is handled
os.system(tarcommand)

# Subsequently, we append elements for the specific timeframes IDs if applicable
for tid in taskids:
    taskspec = self.workflowspec['stages'][tid]
    directory = taskspec['cwd']
    if directory != "./":
        tarcommand = get_tar_command(dir=directory, flags='rf', filename=fn)
        actionlogger.info("Tar command is " + tarcommand)
        os.system(tarcommand)
        # Similarly for symbolic links
        tarcommand = get_tar_command(dir=directory, flags='rf', findtype='l', filename=fn)
        actionlogger.info("Tar command is " + tarcommand)
        os.system(tarcommand)

# Add file:/// to indicate a local file
fn = "file://" + fn
actionlogger.info("Local checkpoint file is " + fn)

---

time_delta = int((time.perf_counter() - self.start_time) * 1000)
totalUSS /= 1024 / 1024
totalPSS /= 1024 / 1024
nice_value = proc.nice()
resources_per_task[tid]={'iter':self.internalmonitorid, 'name':self.idtotask[tid], 'cpu':totalCPU, 'uss':totalUSS, 'pss':totalPSS, 'nice':nice_value, 'swap':totalSWAP, 'label':self.workflowspec['stages'][tid]['labels']}
self.resource_manager.add_monitored_resources(tid, time_delta, totalCPU / 100, totalPSS)
if nice_value == self.resource_manager.nice_default:
    globalCPU += totalCPU
    globalPSS += totalPSS

metriclogger.info(resources_per_task[tid])
send_webhook(self.args.webhook, resources_per_task)

---

# navigate back
    lines.append('cd $OLDPWD\n')


# generate a standalone bash script for the workflow
    def generate_script(self, filename):
        # select the initial task order
        taskorder = self.topological_orderings[0]
        outF = open(filename, "w")

        lines=[]
        # header
        lines.append('#!/usr/bin/env bash\n')
        lines.append('#THIS FILE IS AUTOGENERATED\n')
        lines.append('export JOBUTILS_SKIPDONE=ON\n')

        # document the global environment settings
        # which include the workflow's initialization
        lines.append('#-- GLOBAL INIT SECTION FROM WORKFLOW --\n')
        for e in self.globalinit['env']:
            lines.append('export ' + str(e) + '=' + str(self.globalinit['env'][e]) + '\n')
        lines.append('#-- TASKS FROM WORKFLOW --\n')
        for tid in taskorder:
            print ('Processing task ' + self.idtotask[tid])
            self.emit_code_for_task(tid, lines)

---

DOCUMENT:
    env_vars[key.strip()] = value
        return env_vars

    # check if it's a file
    if os.path.exists(packagestring) and os.path.isfile(packagestring):
       actionlogger.info("Using software environment from file " + packagestring)
       return load_env_file(packagestring)

    # alienv printenv packagestring --> dictionary
    # currently works only with CVMFS
    cmd="/cvmfs/alice.cern.ch/bin/alienv printenv " + packagestring
    proc = subprocess.Popen([cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)

    envstring, err = proc.communicate()
    # check if the printenv command was successful
    if len(err.decode()) > 0:
       print (err.decode())
       raise Exception

---

DOCUMENT:
    def get_logfile(self, tid):
        """
        The O2 taskwrapper records the task's stdout and stderr in a logfile named <task>.log.
        Retrieve the precise path of this logfile using the task ID.
        """
        # calculates the name of the logfile for this specific task
        name = self.workflowspec['stages'][tid]['name']
        workdir = self.workflowspec['stages'][tid]['cwd']
        return os.path.join(workdir, f"{name}.log")

    def get_done_filename(self, tid):
        """
        After a task successfully completes, the O2 taskwrapper creates a file named <task>.log_done.
        Obtain the exact path of this file using the task ID.
        """
        return f"{self.get_logfile(tid)}_done"

    def get_resources_filename(self, tid):
        """
        Upon completion, the O2 taskwrapper generates a file named <task>.log_time.
        Retrieve the exact path of this file using the task ID.
        """
        return f"{self.get_logfile(tid)}_time"

---

if args.cgroup is not None:
    myPID = os.getpid()
    # cgroups like /sys/fs/cgroup/cpuset/<cgroup-name>/tasks
    # or /sys/fs/cgroup/cpu/<cgroup-name>/tasks
    command = f"echo {myPID} > {args.cgroup}"
    actionlogger.info(f"Attempting to run in cgroup {args.cgroup}")
    waitstatus = os.system(command)
    if code := os.waitstatus_to_exitcode(waitstatus):
        actionlogger.error(f"Failed to apply cgroup")
        exit(code)
    actionlogger.info("Running in cgroup")

executor = WorkflowExecutor(args.workflowfile, jmax=int(args.maxjobs), args=args)
exit(executor.execute())

---

if len(self.time_collect) < 3:
    # Require at least 3 points for resource sampling
    self.cpu_sampled = self.cpu_assigned
    self.mem_sampled = self.mem_assigned
    actionlogger.debug("Task %s does not have enough points (< 3) to sample resources; using previously assigned values.", self.name)
else:
    # Calculate time deltas excluding the initial CPU measurement, which may be less meaningful
    time_deltas = [self.time_collect[i+1] - self.time_collect[i] for i in range(len(self.time_collect) - 1)]
    cpu = sum([cpu * time_delta for cpu, time_delta in zip(self.cpu_collect[1:], time_deltas) if cpu >= 0])
    self.cpu_sampled = cpu / sum(time_deltas)
    self.mem_sampled = max(self.mem_collect)

---

# the simple cases take no action
if packagestring is None or packagestring == "" or packagestring == "None":
    return {}

def convert_env_file(env_file):
    """Convert an environment file created with 'export > env.txt' into a python dictionary."""
    env_vars = {}
    with open(env_file, "r") as f:
        for line in f:
            line = line.strip()

            # Ignore blank lines or comments
            if not line or line.startswith("#"):
                continue

            # Remove 'declare -x ' if it exists
            if line.startswith("declare -x "):
                line = line.replace("declare -x ", "", 1)

            # Handle the case: "FOO" without "=" (assign an empty string)
            if "=" not in line:
                key, value = line.strip(), ""
            else:
                key, value = line.split("=", 1)
                value = value.strip('"')  # Remove surrounding quotes if present

---

1. if necessary, create the working directory if it doesn't already exist
2. mark the task as running by updating the lookup structures
3. configure the environment if specified for the task
4. build a psutil.Process from the command line
4.1 modify the priority of that process if specified
5. return the psutil.Process object
"""
actionlogger.debug("Submitting task " + str(self.idtotask[tid]) + " with niceness " + str(nice))
c = self.workflowspec['stages'][tid]['cmd']
workdir = self.workflowspec['stages'][tid]['cwd']
if workdir:
    if os.path.exists(workdir) and not os.path.isdir(workdir):
        actionlogger.error('Cannot create working directory ... another resource already exists there')
        return None

    if not os.path.isdir(workdir):
        os.makedirs(workdir)

---

DOCUMENT:
        del workflowspec['stages'][init_index]

        return {"env" : globalenv, "cmd" : initcmd }

    def execute_globalinit_cmd(self, cmd):
        actionlogger.info("Executing global initialization command " + str(cmd))
        # run the global initialization command (think of cleanup or setup tasks that need to be performed in any case)
        p = subprocess.Popen(['/bin/bash','-c', cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = p.communicate()

        # verify if the command was successful (return code 0)
        if p.returncode == 0:
            actionlogger.info(stdout.decode())
        else:
            # this should be treated as an error
            actionlogger.error("Error executing global initialization function")
            return False
        return True

    def get_global_task_name(self, name):
        """
        Obtain the global task name

    PARAPHRASED DOCUMENT:

---

dependency_cache = {}
# The weight that affects the scheduling order can be anything as defined by the user. For now, we prioritize staying within a timeframe, and then consider the number of tasks that depend on a specific task as an additional factor.
# TODO: Incorporate resource estimates like CPU and MEM from runtime.
# TODO: Turn this into a policy of the runner to explore various strategies.
def getweight(tid):
    return (globaltaskuniverse[tid][0]['timeframe'], len(find_all_dependent_tasks(global_next_tasks, tid, dependency_cache)))

task_weights = [ getweight(tid) for tid in range(len(globaltaskuniverse)) ]

for tid in range(len(globaltaskuniverse)):
    actionlogger.info("The score for " + str(globaltaskuniverse[tid][0]['name']) + " is " + str(task_weights[tid]))

# print (global_next_tasks)
return { 'nexttasks' : global_next_tasks, 'weights' : task_weights, 'topological_ordering' : tup[0] }

---

# accumulate total metrics (CPU, memory)
            totalCPU = 0.
            totalPSS = 0.
            totalSWAP = 0.
            totalUSS = 0.
            for p in psutilProcs:
                """
                try:
                    for f in p.open_files():
                        self.pid_to_files[pid].add(str(f.path)+'_'+str(f.mode))
                    for f in p.connections(kind="all"):
                        remote=f.raddr
                        if remote==None:
                            remote='none'
                        self.pid_to_connections[pid].add(str(f.type)+"_"+str(f.laddr)+"_"+str(remote))
                except Exception:
                    pass
                """
                thispss=0
                thisuss=0
                # MEMORY part
                try:
                    fullmem=p.memory_full_info()
                    thispss=getattr(fullmem,'pss',0) #<-- pss not available on MacOS

---

def ok_to_submit(self, tids):
    """
    This generator yields the tid and nice value tuple from the list of task ids to be verified.
    """
    tids_copy = tids.copy()

    def ok_to_submit_default(res):
        """
        Returns the default nice value if the conditions are satisfied, or None otherwise.
        """
        # check CPU usage
        okcpu = (self.cpu_booked + res.cpu_assigned <= self.resource_boundaries.cpu_limit)
        # check MEM usage
        okmem = (self.mem_booked + res.mem_assigned <= self.resource_boundaries.mem_limit)
        actionlogger.debug('Condition check --normal-- for ' + str(res.tid) + ':' + res.name + ' CPU ' + str(okcpu) + ' MEM ' + str(okmem))
        return self.nice_default if (okcpu and okmem) else None

---

The final nice value is set during the final submission and may differ. This can occur if the nice value was supposed to change but the system doesn't permit it.
```
res = self.resources[tid]
# use the previously assigned nice value from the last resource check
previous_nice_value = res.nice_value

if previous_nice_value is None:
    # this task hasn't been checked before, treating it as backfill
    actionlogger.warning("Task ID %d has never been checked for resources. Treating as backfill", tid)
    nice_value = self.nice_backfill
elif res.nice_value != nice_value:
    actionlogger.warning("Task ID %d was last checked for a different nice value (%d) but is now submitted with (%d).", tid, res.nice_value, nice_value)
```

---

def ok_to_submit_backfill(res, backfill_cpu_factor=1.5, backfill_mem_factor=1.5):
    """
    Return the backfill nice value if the conditions are satisfied, otherwise return None
    """
    if self.n_procs_backfill >= args.n_backfill:
        return None

    if res.cpu_assigned > 0.9 * self.resource_boundaries.cpu_limit or res.mem_assigned / self.resource_boundaries.cpu_limit >= 1900:
        return None

---

# run control, webhooks
parser.add_argument('--stdout-on-failure', action='store_true', help='Redirect log files from failed tasks to standard output,')
parser.add_argument('--webhook', help=argparse.SUPPRESS) # log relevant information to this webhook channel
parser.add_argument('--checkpoint-on-failure', help=argparse.SUPPRESS) # a debugging option that creates a debug-tarball and sends it to a specified address
                                                                       # the argument is an alien-path
parser.add_argument('--retry-on-failure', help=argparse.SUPPRESS, default=0) # specify the number of times a failed task should be retried
parser.add_argument('--no-rootinit-speedup', help=argparse.SUPPRESS, action='store_true') # disable the initialization of ROOT environment variables to speed up initialization/startup

---

# track resources reserved with default priority level
self.cpu_booked = 0
self.mem_booked = 0
# count tasks reserved with default priority
self.n_procs = 0

# track resources reserved with higher priority level
self.cpu_booked_backfill = 0
self.mem_booked_backfill = 0
# count tasks reserved with higher priority
self.n_procs_backfill = 0

# define the maximum number of tasks running concurrently
self.procs_parallel_max = procs_parallel_max

# determine the default priority level of this script
self.nice_default = os.nice(0)
# increase by 19 to get the priority level for lower-priority tasks
self.nice_backfill = self.nice_default + 19

---

# if tasks_skipped:
   # return # ---> we return early to maintain job order (the next candidate should be the daughters of skipped jobs)
   # retrieve task ID and suggested niceness from the generator
   for (tid, nice_value) in self.resource_manager.ok_to_submit(taskcandidates):
      actionlogger.debug ("attempting to submit " + str(tid) + ':' + str(self.idtotask[tid]))
      if p := self.submit(tid, nice_value):
        # re-set the niceness explicitly from the process, as the submit operation might not have changed it
        # inform the ResourceManager of the final niceness
        self.resource_manager.book(tid, p.nice())
        self.process_list.append((tid,p))
        taskcandidates.remove(tid)
        # short delay
        time.sleep(0.1)

---

```python
Args:
    taskcandidates: list
        list of potential tasks to be submitted
    finished: list
        empty list that will be populated with IDs of tasks that were completed during this iteration
"""

self.scheduling_iteration = self.scheduling_iteration + 1

# immediately remove "completed or skippable" tasks
for tid in taskcandidates.copy():  # <--- the copy is crucial !! otherwise this loop does not function as intended
    if self.ok_to_skip(tid):
        finished.append(tid)
        taskcandidates.remove(tid)
        actionlogger.info("Skipping task " + str(self.idtotask[tid]))
```