## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/GRID/utils/grid_submit.sh

**Start chunk id:** 06240870f382acfb0e134404fcd3c4c55ad6701551dde8cdad9691080b04d8bf

## Content

# this is the global job status (a D indicates the production is complete)
JOBSTATUS=$(alien.py ps -j ${MY_JOBID} | awk '//{print $3}') # this is the global job status
# we can also query individual job splits
if [ -n "${WAITFORALIENANY}" ]; then
  DETAILED_STATUS_JSON=$(ALIENPY_JSON=true alien.py ps -a -m "${MY_JOBID}")
  # verify if any job is already marked as DONE
  if jq -e '.results | any(.status == "DONE")' <<<"${DETAILED_STATUS_JSON}" >/dev/null; then
    JOBSTATUS="D"
    echo "At least one job has completed successfully"
  else
    # check if there are still jobs running or waiting; if not, also mark as done
    # this might occur if all jobs are in a zombie state (in which case we also finish)
    if ! jq -e '.results | any(.status == "WAITING" or .status == "RUNNING" or .status == "SAVING" or .status == "INSERTING")' \
        <<<"${DETAILED_STATUS_JSON}" >/dev/null; then
      JOBSTATUS="D"  # some job finished successfully

---

DOCUMENT:
  REQUIRESPEC=$(grep "^#JDL_REQUIRE=" ${SCRIPT} | sed 's/#JDL_REQUIRE=//')
  if [ ! "${REQUIRESPEC}" ]; then
    echo "No Requirement setting detected; Setting to default"
    REQUIRESPEC="{member(other.GridPartitions,"${GRIDPARTITION:-multicore_8}")};"
    echo "Requirement is ${REQUIRESPEC}"
  fi

  echo "Requirements JDL entry is ${REQUIRESPEC}"

---

DOCUMENT:
    cat /proc/cpuinfo > alien_cpuinfo.log 
cat /proc/meminfo > alien_meminfo.log

# ----------- PREPARE SOME ALIEN ENV -- useful for the job -----------

if [ "${ONGRID}" = "1" ]; then
  notify_mattermost "STARTING GRID ${ALIEN_PROC_ID} CHECK $(which alien.py)"
  alien.py ps --jdl ${ALIEN_PROC_ID} > this_jdl.jdl
  ALIEN_JOB_OUTPUTDIR=$(grep "OutputDir" this_jdl.jdl | awk '{print $3}' | sed 's/"//g' | sed 's/;//')
  ALIEN_DRIVER_SCRIPT=$0

  # determine subjob id from the structure of the outputfolder
  # can use basename tool since this is like a path
  SUBJOBID=$(echo $(basename "${ALIEN_JOB_OUTPUTDIR}") | sed 's/^0*//')  # Remove leading zeros if present

  # expose some information about prodsplit and subjob id to the jobs
  # so that they can adjust/contextualise the payload
  export ALIEN_O2DPG_GRIDSUBMIT_PRODSPLIT=${PRODSPLIT}
  export ALIEN_O2DPG_GRIDSUBMIT_SUBJOBID=${SUBJOBID}

  #notify_mattermost "ALIEN JOB OUTDIR IS ${ALIEN_JOB_OUTPUTDIR}"

---

--dry) DRYRUN="ON"; shift 1 ;; # perform a dry run without interacting with the GRID (only generate local jdl file)
        --o2tag) O2TAG=$2; shift 2 ;; #
        --packagespec) PACKAGESPEC=$2; shift 2 ;; # specify the alisw and cvmfs package list (separated by command - example: '"VO_ALICE@FLUKA_VMC::4-1.1-vmc3-1","VO_ALICE@O2::daily-20230628-0200-1"')
        --asuser) ASUSER=$2; shift 2 ;; #
        --label) JOBLABEL=$2; shift 2 ;; # a label to identify the production (e.g., as a production identifier)
        --mattermost) MATTERMOSTHOOK=$2; shift 2 ;; # if provided, job status and metrics will be sent to this hook via Mattermost
        --controlserver) CONTROLSERVER=$2; shift 2 ;; # allows specifying a SERVER ADDRESS/IP to act as a controller for GRID jobs
        --prodsplit) PRODSPLIT=$2; shift 2 ;; # set the JDL production split level (useful for easily replicating workflows)
        --singularity) SINGULARITY=ON; shift 1 ;; # run all operations within a singularity container

---

# A hook that can be attached at the end of "taskwrapper" tasks.
# This hook carries out a few actions:
# a) sends task metrics to a Mattermost channel
# b) assesses whether we are nearing the TTL and acts accordingly, such as:
#     - performing a checkpoint
#     - uploading files to ALIEN
#     - halting the workflow to avoid a hard external timeout
checkpoint_hook_ttlbased() {
  RC=$3 # return code of the stage
  timepassedsincestart=$SECONDS
  walltime=`cat $2_time`
  text1="${ALIEN_PROC_ID} checkpoint check for $2; ${SECONDS} passed since job start out of ${JOBTTL}"
  notify_mattermost "${text1}"
  cpumodel=`grep -m 1 "model name" /proc/cpuinfo | sed 's/.*://' | tr ' ' '_'`

  # analyze CPU utilization
  corecount=$(grep "processor" /proc/cpuinfo | wc -l)
  path=$PWD
  cpuusage=$(analyse_CPU.py $PWD/$2_cpuusage ${corecount} 2>/dev/null)

  # analyze memory utilization
  maxmem=$(grep "PROCESS MAX MEM" ${path}/$2 | awk '//{print $5}')
  avgmem=$(grep "PROCESS AVG MEM" ${path}/$2 | awk '//{print $5}')
}

---

fi
  # -) Special singularity / Apptainer image
  [[ ! ${IMAGESPEC} ]] && IMAGESPEC=$(grep "^#JDL_IMAGE=" ${SCRIPT} | sed 's/#JDL_IMAGE=//')
  echo "Identified Container Image as ${IMAGESPEC}"

---

if [ "${JOBSTATUS}" == "D" ]; then
  echo "${WAITFORALIEN:+At least one }job(s) completed"
  WAITFORALIEN=""  # ensures exiting the outer while loop

---

sanitize_tokens_with_quotes() {
  string=$1
  result=""
  # Use comma (,) as the delimiter for tokenizing the string
  IFS=',' read -ra tokens <<< "$string"
  for token in "${tokens[@]}"; do
    [[ -n $result ]] && result=${result}","
    # Check if the token is enclosed in double quotes
    if [[ $token =~ ^\".*\"$ ]]; then
      result=$result$token
    else
      result=$result"\"${token}\""
    fi
  done
  echo ${result}
}

# Determine if this script is running on a GRID environment
# This requires the presence of the JALIEN_TOKEN_CERT environment variable
ONGRID=0
[ -n "${JALIEN_TOKEN_CERT}" ] && ONGRID=1

---

DOCUMENT:
    function Usage() { echo "$0 --script scriptname | -c WORKDIR_RELATIVE_TO_TOP [ --jobname JOBNAME ] [ --topworkdir WORKDIR (ON TOP OF HOME) ] "; }

notify_mattermost() {
  set +x
  if [ "$MATTERMOSTHOOK" ]; then
    text=$1
    command="curl -X POST -H 'Content-type: application/json' --data '{\"text\":\""${text}"\"}' "${MATTERMOSTHOOK}" &> /dev/null"
    eval "${command}"
  fi
}

start_hook() {
  notify_mattermost "${ALIEN_PROC_ID}: Initiating stage $2"
}

upload_logs() {
  # FOR NOW, WE ZIP ALL LOG FILES
  zip logs_PROCID${ALIEN_PROC_ID:-0}_failure.zip *.log* *mergerlog* *serverlog* *workerlog* alien_log_${ALIEN_PROC_ID:-0}_failure.txt
  [ "${ALIEN_JOB_OUTPUTDIR}" ] && upload_to_Alien logs_PROCID${ALIEN_PROC_ID:-0}_failure.zip  ${ALIEN_JOB_OUTPUTDIR}/
}
export -f upload_logs
failure_hook() {
  notify_mattermost "${ALIEN_PROC_ID}: **Failure** in stage $2"
  cp alien_log_${ALIEN_PROC_ID:-0}.txt logtmp_${ALIEN_PROC_ID:-0}_failure.txt

  # FOR NOW, WE ZIP ALL LOG FILES
  upload_logs
}

---

pok "Submitting job \"${MY_JOBNAMEDATE}\" from $PWD"
(
  echo "submit ${MY_JOBWORKDIR}/${MY_JOBNAMEDATE}.jdl" >> ${command_file}

  # we now execute a single command to alien:
  alien.py < ${command_file}
) >> alienlog.txt 2>&1

MY_JOBID=$( (grep 'Your new job ID is' alienlog.txt | grep -oE '[0-9]+' || true) | sort -n | tail -n1)
if [[ $MY_JOBID ]]; then
  pok "Job submission successful, monitor progress at https://alimonitor.cern.ch/agent/jobs/details.jsp?pid=$MY_JOBID"
else
  per "Job submission failed, error details below"
  cat alienlog.txt
  exit 1
fi

---

#notify_mattermost "ALIEN JOB OUTDIR IS ${ALIEN_JOB_OUTPUTDIR}" 

export ALIEN_JOB_OUTPUTDIR
export ALIEN_DRIVER_SCRIPT
export O2_PACKAGE_LATEST

# ----------- RETRIEVE PREVIOUS CHECKPOINT IF CONTINUING THE JOB ----
if [ "${CONTINUE_WORKDIR}" ]; then
  alien.py cp alien://${ALIEN_JOB_OUTPUTDIR}/checkpoint.tar file:/.
  if [ -f checkpoint.tar ]; then
    tar -xf checkpoint.tar
    rm checkpoint.tar
  else
    notify_mattermost "Could not download checkpoint; Quitting"
    exit 0
  fi
fi

# export JOBUTILS_MONITORCPU=ON
# export JOBUTILS_WRAPPER_SLEEP=5
# export JOBUTILS_JOB_KILLINACTIVE=180 # kill inactive jobs after 3 minutes --> will be the task of pipeline runner? (or make it optional)
# export JOBUTILS_MONITORMEM=ON 

# ----------- RUN THE ACTUAL JOB  ------------------------------------ 
# make the job script executable and run it
chmod +x ./alien_jobscript.sh
./alien_jobscript.sh
# capture the return code
RC=$?

---

SPLITOUTDIR=$(printf "%03d" ${splitcounter})
if [ ! -f ${SPLITOUTDIR} ]; then
  mkdir ${SPLITOUTDIR}
  echo "Fetching result files for subjob ${splitcounter} into ${PWD}"
  CPCMD="alien.py cp ${MY_JOBWORKDIR}/${SPLITOUTDIR}/* file:./${SPLITOUTDIR}"
  eval "${CPCMD}" 2> /dev/null
else
  echo "Not fetching files for subjob ${splitcounter} since job code is ${THIS_STATUS}"
fi
done
fi
fi
done
# get the job data products locally if requested

---

JOBSTATUS="D"  # some job completed successfully
              echo "No further good job"
        fi
      fi
    fi

---

# General job configuration
MY_USER=${ALIEN_USER:-`whoami`}

alien.py whois -a ${MY_USER}

if [[ ! $MY_USER ]]; then
  per "Issues fetching the current AliEn user. Have you executed alien-token-init?"
  exit 1
fi

[ "${ASUSER}" ] && MY_USER=${ASUSER}

MY_HOMEDIR="/alice/cern.ch/user/${MY_USER:0:1}/${MY_USER}"
MY_JOBPREFIX="$MY_HOMEDIR/${ALIEN_TOPWORKDIR:-selfjobs}"
MY_JOBSCRIPT="$(cd "$(dirname "${SCRIPT}")" && pwd -P)/$(basename "${SCRIPT}")" # the job script with full path
MY_JOBNAME=${JOBNAME:-$(basename ${MY_JOBSCRIPT})}
MY_JOBNAMEDATE="${MY_JOBNAME}-$(date -u +%Y%m%d-%H%M%S)"
MY_JOBWORKDIR="${MY_JOBPREFIX}/${MY_JOBNAMEDATE}"  # ISO-8601 UTC
[ "${CONTINUE_WORKDIR}" ] && MY_JOBWORKDIR="${MY_JOBPREFIX}/${CONTINUE_WORKDIR}"
MY_BINDIR="$MY_JOBWORKDIR"

pok "The job's working directory will be $MY_JOBWORKDIR"
pok "Specify the job name by running $0 <scriptname> <jobname>"

---

exec &> >(tee -a alien_log_${ALIEN_PROC_ID:-0}.txt)

# ----------- START JOB PREAMBLE  ----------------------------- 
check_env | grep "SINGULARITY" &> /dev/null
if [ "$?" = "0" ]; then
  echo "Detected execution within a Singularity container"
fi

display_banner "Environment"
print_env

display_banner "Limits"
show_ulimit -a

display_banner "Operating System"
cat /etc/os-release || true
cat /etc/redhat-release || true

---

# FOR THE TIME BEING, ALL LOG FILES ARE ZIPPED
  uploadlogs
}

export -f starthook
export -f failhook
export JOBUTILS_JOB_STARTHOOK="starthook"
export JOBUTILS_JOB_FAILUREHOOK="failhook"

# this function transfers a local file to an alien path and includes error checks
upload_to_Alien() {
  set -x
  SOURCEFILE=$1  # should be a local file --> verify
  DEST=$2  # should be a target path
  notify_mattermost "BEGINNING TRANSFER TO ALIEN: alien.py cp -f file:$SOURCEFILE ${DEST}"
  alien.py cp -f file:$SOURCEFILE ${DEST}
  RC=$?
  [ "${RC}" != "0" ] && notify_mattermost "TRANSFER OF FILE ${SOURCEFILE} TO ${DEST} RESULTED IN RC: ${RC}"
  # perform a check
  alien.py ls ${DEST}/${SOURCEFILE}
  RC=$?
  [ "${RC}" != "0" ] && notify_mattermost "LISTING OF FILE ${DEST}/${SOURCEFILE} YIELDED RC: ${RC}"

---

# Establish a temporary working directory for file assembly, and proceed from there (or run locally)
  cd "$(dirname "$0")"
  THIS_SCRIPT="$PWD/$(basename "$0")"

  cd "${GRID_SUBMIT_WORKDIR}"

---

# upload tarball
#  [ "${ALIEN_JOB_OUTPUTDIR}" ] && upload_to_Alien checkpoint.tar ${ALIEN_JOB_OUTPUTDIR}/

# resubmit
# if [ "$?" = "0" ]; then
#   notify_mattermost "RESUBMITTING"
#    [ "${ALIEN_JOB_OUTPUTDIR}" ] && ${ALIEN_DRIVER_SCRIPT} -c `basename ${ALIEN_JOB_OUTPUTDIR}` --jobname CONTINUE_ID${ALIEN_PROC_ID} --topworkdir foo --o2tag ${O2_PACKAGE_LATEST} --asuser aliperf --ttl ${JOBTTL}
# fi

# exit current workflow
exit 0
  fi
}
export -f checkpoint_hook_ttlbased
export -f notify_mattermost
export JOBUTILS_JOB_ENDHOOK=checkpoint_hook_ttlbased

---

pok "Preparing job \"$MY_JOBNAMEDATE\""
(
  # combine all GRID interactions into one script / transaction
  [ -f "${command_file}" ] && rm ${command_file}
  echo "user ${MY_USER}" >> ${command_file}
  echo "whoami" >> ${command_file}
  [ ! "${CONTINUE_WORKDIR}" ] && echo "rmdir ${MY_JOBWORKDIR}" >> ${command_file}    # remove existing job directory
  # echo "mkdir ${MY_BINDIR}" >> ${command_file}                      # create bindir
  echo "mkdir ${MY_JOBPREFIX}" >> ${command_file}                   # create job output prefix
  [ ! "${CONTINUE_WORKDIR}" ] && echo "mkdir ${MY_JOBWORKDIR}" >> ${command_file}
  [ ! "${CONTINUE_WORKDIR}" ] && echo "mkdir ${MY_JOBWORKDIR}/output" >> ${command_file}
  echo "rm ${MY_BINDIR}/${MY_JOBNAMEDATE}.sh" >> ${command_file}    # remove current job script
  echo "cp file:${PWD}/${MY_JOBNAMEDATE}.jdl alien://${MY_JOBWORKDIR}/${MY_JOBNAMEDATE}.jdl@DISK=1" >> ${command_file}  # copy the jdl

)

---

DOCUMENT:
    echo "Requirements JDL entry is ${REQUIRESPEC}"

  # -) PackageSpec
  if [ -z ${PACKAGESPEC} ]; then
    PACKAGESPEC=$(grep "^#JDL_PACKAGE=" ${SCRIPT} | sed 's/#JDL_PACKAGE=//')
    echo "Found PackagesSpec to be ${PACKAGESPEC}"
    ## sanitize package spec
    ## "no package" defaults to O2sim
    if [ -z ${PACKAGESPEC} ]; then
      PACKAGESPEC="O2sim"
      O2SIM_LATEST=`find /cvmfs/alice.cern.ch/el7-x86_64/Modules/modulefiles/O2sim -type f -printf "%f\n" | tail -n1`
      if [ -z ${O2SIM_LATEST} ]; then
        echo "Cannot lookup latest version of implicit package O2sim from CVFMS"
        exit 1
      else
        PACKAGESPEC="${PACKAGESPEC}::${O2SIM_LATEST}"
        echo "Autosetting Package to ${PACKAGESPEC}"
      fi
    fi
  fi
  ## *) add VO_ALICE@ in case not present
  if [ ! ${PACKAGESPEC} == VO_ALICE@* ]; then
    PACKAGESPEC="VO_ALICE@"${PACKAGESPEC}
  fi
  ## *) apply quotes
  PACKAGESPEC=$(sanitize_tokens_with_quotes ${PACKAGESPEC})

---

if [ "${FETCHOUTPUT}" ]; then
    SUBJOBIDS=()
    SUBJOBSTATUSES=()
    echo "Retrieving subjob information"
    while [ "${#SUBJOBIDS[@]}" -eq "0" ]; do
        QUERYRESULT=$(ALIENPY_JSON=true alien.py ps -a -m ${MY_JOBID})
        SUBJOBIDS=($(echo ${QUERYRESULT} | jq -r '.results[].id' | tr '\n' ' '))
        SUBJOBSTATUSES=($(echo ${QUERYRESULT} | jq -r '.results[].status' | tr '\n' ' '))
        # echo "LENGTH SUBJOBS ${#SUBJOBIDS[@]}"
        sleep 1
    done
    # TODO: implement parallel copying
    echo "Retrieving results for ${PRODSPLIT} sub-jobs"
    for splitcounter in $(seq 1 ${PRODSPLIT}); do
        let jobindex=splitcounter-1
        THIS_STATUS=${SUBJOBSTATUSES[jobindex]}
        THIS_JOB=${SUBJOBIDS[jobindex]}
        echo "Processing job ${THIS_JOB}"
        if [ "${THIS_STATUS}" == "DONE" ]; then
            SPLITOUTDIR=$(printf "%03d" ${splitcounter})
    done

---

exit 0
fi  # <---- end if ALIEN_JOB_SUBMITTER

####################################################################################################
# This section runs on the worker node or locally
####################################################################################################
if [[ ${SINGULARITY} ]]; then
  # if Singularity is required, we rerun this script inside a container
  # this is similar to the GRID mode, so we set JALIEN_TOKEN_CERT
  set -x
  cp $0 ${GRID_SUBMIT_WORKDIR}

  # determine the hardware architecture (ARM or X86)
  ARCH=$(uname -i)
  if [ "$ARCH" == "aarch64" ] || [ "$ARCH" == "x86_64" ]; then
    echo "Detected hardware architecture : $ARCH"
  else
    echo "Invalid architecture ${ARCH} detected. Exiting"
    exit 1
  fi
  if [ "$ARCH" == "aarch64" ]; then
    ISAARCH64="1"
  fi

---

DOCUMENT:
    alien.py xrdstat ${DEST}/${SOURCEFILE} | awk 'BEGIN{count=0}/root:/{if($3=="OK"){count=count+1}} END {if(count>=2) {exit 0}else{ exit 1}}'
  RC=$?
  notify_mattermost "FINISHED UPLOADING TO ALIEN: alien.py cp -f file:$SOURCEFILE ${DEST} ${RC}"
  set +x
  return ${RC}
}
export -f upload_to_Alien

    PARAPHRASED DOCUMENT:

---

BANNER "DETECTING OPERATING SYSTEM"
cat /etc/os-release || true
cat /etc/redhat-release || true

# loading the requested package list (this should now be handled by JDL)
#if [ -z "$O2_ROOT" ]; then
#  O2_PACKAGE_LATEST=`find /cvmfs/alice.cern.ch/el7-x86_64/Modules/modulefiles/O2 -name "*nightl*" -type f -printf "%f\n" | tail -n1`
#  banner "LOADING O2 PACKAGE $O2_PACKAGE_LATEST"
#  [ "${O2TAG}" ] && O2_PACKAGE_LATEST=${O2TAG}
#  #eval "$(/cvmfs/alice.cern.ch/bin/alienv printenv O2::"$O2_PACKAGE_LATEST")"
#fi
#if [ -z "$O2DPG_ROOT" ]; then
#  O2DPG_LATEST=`find /cvmfs/alice.cern.ch/el7-x86_64/Modules/modulefiles/O2DPG -type f -printf "%f\n" | tail -n1`
#  banner "LOADING O2DPG PACKAGE $O2DPG_LATEST"
#  #eval "$(/cvmfs/alice.cern.ch/bin/alienv printenv O2DPG::"$O2DPG_LATEST")"
#fi

BANNER "EXECUTING WORKFLOW"

# gathering some general information
echo "CONTINUE_WORKDIR ${CONTINUE_WORKDIR}"

cat /proc/cpuinfo > alien_cpuinfo.log 
cat /proc/meminfo > alien_meminfo.log

---

This hook is registered and periodically runs within the taskwrapper control loop (assuming each stage of GRID workflows is managed by taskwrapper). We utilize it to remotely control GRID jobs. For example, one could instruct a GRID job to upload its current log files to ALIEN for prompt inspection, which can be helpful during debugging.

control_hook() {
  # We interact with a remote webserver to query control commands for this GRID process. The webserver should return a single command word.
  command=$(curl ${CONTROLSERVER}/?procid=${ALIEN_PROC_ID} 2> /dev/null)
  if [ "$command" = "uploadlogs" ]; then
    notify_mattermost "Control command **uploadlogs** for ${ALIEN_PROC_ID}"
    uploadlogs & # -> run in the background to return immediately
  elif [ "$command" = "kill" ]; then
    echo "killing job"
    taskwrapper_cleanup $PID SIGKILL
    exit 1
  fi
}
export -f control_hook

---

#
# Create a local work directory
#
if [[ "${ONGRID}" == "0" ]]; then
  GRID_SUBMIT_WORKDIR=${GRID_SUBMIT_WORKDIR:-${TMPDIR:-/tmp}/alien_work/$(basename "$MY_JOBWORKDIR")}
  echo "THE WORK DIRECTORY FOR THIS JOB IS ${GRID_SUBMIT_WORKDIR}"
  [ ! -d "${GRID_SUBMIT_WORKDIR}" ] && mkdir -p ${GRID_SUBMIT_WORKDIR}
  [ ! "${CONTINUE_WORKDIR}" ] && cp "${MY_JOBSCRIPT}" "${GRID_SUBMIT_WORKDIR}/alien_jobscript.sh"
fi

#
# Submitting logic (we need to submit the job when a script is provided as input and we are not in local mode)
#
[[ ( ! "${LOCAL_MODE}" ) && ( "${SCRIPT}" || "${CONTINUE_WORKDIR}" ) ]] && IS_ALIEN_JOB_SUBMITTER=ON

---

# To ensure we capture the logs (temporarily disabled due to occasional hanging during copying)
#cp alien_log_${ALIEN_PROC_ID:-0}.txt logtmp_${ALIEN_PROC_ID:-0}.txt
#[ "${ALIEN_JOB_OUTPUTDIR}" ] && upload_to_Alien logtmp_${ALIEN_PROC_ID:-0}.txt ${ALIEN_JOB_OUTPUTDIR}/

echo "Job completed ... exiting with ${RC}"

# Necessary to exit for the ALIEN JOB HANDLER!
exit ${RC}

---

DOCUMENT:
    metrictext="#pdpmetric:${JOBLABEL},procid:${ALIEN_PROC_ID},CPU:${cpumodel},stage:$2,RC:${RC:-1},walltime:${walltime},${cpuusage},MAXMEM:${maxmem},AVGMEM:${avgmem}"
  notify_mattermost "${metrictext}"

  # perform calculation using AWK
  CHECKPOINT=$(awk -v S="${SECONDS}" -v T="${JOBTTL}" '//{} END{if(S/T>0.8){print "OK"}}' /dev/null);
  if [ "$CHECKPOINT" = "OK" ]; then
    echo "**** TTL NEAR - CHECKPOINTING *****"
    # upload
    text="CHECKPOINTING NOW"
    # notify
    notify_mattermost "${text}"

    # delete unnecessary files (pipes, sockets, etc)
    find ./ -size 0 -delete

    # create tarball (no compression to be quick and ROOT files are already compact)
    tar --exclude "output" -cf checkpoint.tar *

    text="TAR ARCHIVE CREATED WITH STATUS $?, SIZE: $(ls -al checkpoint.tar)"
    notify_mattermost "${text}"

    
    # This part is experimental: It aims to restart a new job immediately, resuming work from the generated checkpoint
    #

---

if [[ "${IS_ALIEN_JOB_SUBMITTER}" ]]; then
  #  --> verify if alien is installed?
  which alien.py &> /dev/null
  # check exit status
  if [[ ! "$?" == "0"  ]]; then
    XJALIEN_LATEST=`find /cvmfs/alice.cern.ch/el7-x86_64/Modules/modulefiles/xjalienfs -type f -printf "%f\n" | tail -n1`
    banner "Loading xjalienfs package $XJALIEN_LATEST as it is not loaded"
    eval "$(/cvmfs/alice.cern.ch/bin/alienv printenv xjalienfs::"$XJALIEN_LATEST")"
  fi

---

DOCUMENT:
    CONTAINER="/cvmfs/alice.cern.ch/containers/fs/apptainer/compat_el9-${ARCH}"
  APPTAINER_EXEC="/cvmfs/alice.cern.ch/containers/bin/apptainer/${ARCH}/current/bin/apptainer"

  # we can actually analyze the local JDL to determine the package and configure it for the container
  ${APPTAINER_EXEC} exec -C -B /cvmfs:/cvmfs,${GRID_SUBMIT_WORKDIR}:/workdir --pwd /workdir -C ${CONTAINER} /workdir/grid_submit.sh \
  ${CONTINUE_WORKDIR:+"-c ${CONTINUE_WORKDIR}"} --local ${O2TAG:+--o2tag ${O2TAG}} --ttl ${JOBTTL} --label ${JOBLABEL:-label} ${MATTERMOSTHOOK:+--mattermost ${MATTERMOSTHOOK}} ${CONTROLSERVER:+--controlserver ${CONTROLSERVER}}
  set +x
  exit $?
fi

if [[ "${ONGRID}" == 0 ]]; then
  banner "Executing job in directory ${GRID_SUBMIT_WORKDIR}"
  cd "${GRID_SUBMIT_WORKDIR}" 2> /dev/null
fi

exec &> >(tee -a alien_log_${ALIEN_PROC_ID:-0}.txt)

---

# wait here until all ALIEN jobs have completed
  spin[3]="-"
  spin[2]="/"
  spin[1]="|"
  spin[0]="\\"
  JOBSTATUS="I"
  if [ "${WAITFORALIEN}" ]; then
    echo -n "Waiting for jobs to complete ... Last status : ${spin[0]} ${JOBSTATUS}"
  fi
  counter=0
  while [ "${WAITFORALIEN}" ]; do
    sleep 0.5
    echo -ne "\b\b\b${spin[$((counter%4))]} ${JOBSTATUS}"
    let counter=counter+1
    if [ ! "${counter}" == "100" ]; then
      # ensures the spinner is visible but checks for new job status every 50 seconds
      continue
    fi
    let counter=0 # reset counter

---

DOCUMENT:
    JOBTTL=82000
CPUCORES=8
PRODSPLIT=${PRODSPLIT:-1}
# this indicates that we are continuing an existing job, which means we do not create a new work directory
while [ $# -gt 0 ] ; do
    case $1 in
	      -c) CONTINUE_WORKDIR=$2;  shift 2 ;;   # this should point to the work directory of the job to continue (excluding HOME and ALIEN_TOPWORKDIR)
        --local) LOCAL_MODE="ON"; shift 1 ;;   # if we want to emulate local execution in the work directory without GRID interaction
        --script) SCRIPT=$2; shift 2 ;;  # the job script to submit
        --jobname) JOBNAME=$2; shift 2 ;; # the name of the job, which determines the directory name on the GRID
        --topworkdir) ALIEN_TOPWORKDIR=$2; shift 2 ;; # the top work directory relative to the GRID home
        --ttl) JOBTTL=$2; shift 2 ;; # allows specifying the time-to-live for the job
        --partition) GRIDPARTITION=$2; shift 2 ;; # allows specifying a GRID partition for the job
        --cores) CPUCORES=$2; shift 2 ;; # allows specifying the number of CPU cores (ensure compatibility with the partition)
    esac
esac

---

echo "Copy the current job script to AliEn using: cp file:${THIS_SCRIPT} alien://${MY_BINDIR}/${MY_JOBNAMEDATE}.sh@DISK=1" >> ${command_file}  # copy the current job script to AliEn
      [ ! "${CONTINUE_WORKDIR}" ] && echo "Copy the job script file to AliEn using: cp file:${MY_JOBSCRIPT} alien://${MY_JOBWORKDIR}/alien_jobscript.sh" >> ${command_file}
    ) > alienlog.txt 2>&1

---

# "output_arch.zip:output/*@disk=2",
# "checkpoint*.tar@disk=2"

  pok "The local working directory is $PWD"
  if [ ! "${DRYRUN}" ]; then
    command_file="alien_commands.txt"

---

taskwrapper_cleanup $PID SIGKILL
exit 1
fi
}
export -f control_hook
export JOBUTILS_JOB_PERIODICCONTROLHOOK="control_hook"

---

--singularity) SINGULARITY=ON; shift 1 ;; # execute all commands within a singularity environment
--wait) WAITFORALIEN=ON; shift 1 ;; # wait for alien jobs to complete
--wait-any) WAITFORALIENANY=ON; WAITFORALIEN=ON; shift 1 ;; # wait for any successfully completed alien jobs to return
--outputspec) OUTPUTSPEC=$2; shift 2 ;; # specify a comma-separated list of JDL file specifications to be included in the JDL Output field (e.g., '"*.log@disk=1","*.root@disk=2"')
-h) Usage ; exit ;;
--help) Usage ; exit ;;
--fetch-output) FETCHOUTPUT=ON; shift 1 ;; # option to fetch all job outputs locally to simulate local execution; only effective if blocking until all jobs exit
*) break ;;
esac
done
export JOBTTL
export JOBLABEL
export MATTERMOSTHOOK
export CONTROLSERVER

---

# read preamble from job file if command line is not provided
  # -) OutputSpec
  if [ -z ${OUTPUTSPEC} ]; then
    OUTPUTSPEC=$(grep "^#JDL_OUTPUT=" ${SCRIPT} | sed 's/#JDL_OUTPUT=//')
    echo "Found OutputSpec to be ${OUTPUTSPEC}"
    if [ -z ${OUTPUTSPEC} ]; then
      echo "No file output requested. Please add JDL_OUTPUT preamble to your script"
      echo "Example: #JDL_OUTPUT=*.dat@disk=1,result/*.root@disk=2"
      exit 1
    else
      # check if this is a list and if all parts are properly quoted
      OUTPUTSPEC=$(sanitize_tokens_with_quotes ${OUTPUTSPEC})
    fi
  fi
  # -) ErrorOutputSpec
  if [ -z ${ERROROUTPUTSPEC} ]; then
    ERROROUTPUTSPEC=$(grep "^#JDL_ERROROUTPUT=" ${SCRIPT} | sed 's/#JDL_ERROROUTPUT=//')
    echo "Found ErrorOutputSpec to be ${ERROROUTPUTSPEC}"
    if [ -n ${ERROROUTPUTSPEC} ]; then
      # check if this is a list and if all parts are properly quoted
      ERROROUTPUTSPEC=$(sanitize_tokens_with_quotes ${ERROROUTPUTSPEC})
    fi
  fi
  # -) Special singularity / Apptainer image

---

[[ $PRODSPLIT -gt 100 ]] && echo "The production split should currently be less than 100." && exit 1

# Verify the presence of jq (required for fetching output files)
[[ "$FETCHOUTPUT" ]] && { which jq &> /dev/null || { echo "jq command not found. Please ensure it is loaded or installed." && exit 1; }; }

# Ensure the script is a valid file, otherwise terminate
[[ "${SCRIPT}" ]] && [[ ! -f "${SCRIPT}" ]] && echo "The script file ${SCRIPT} does not exist. Aborting." && exit 1

# Analyze the options:
# The script should either use --script or -c
[ "${SCRIPT}" ] && [ "$CONTINUE_WORKDIR" ] && echo "Running with a script and continue mode simultaneously is not allowed." && exit 1
if [ "${ONGRID}" = 0 ]; then
  [[ ! ( "${SCRIPT}" || "$CONTINUE_WORKDIR" ) ]] && echo "Either a script or continue mode is needed." && exit 1
fi

# General job configuration
MY_USER=${ALIEN_USER:-`whoami`}

alien.py whois -a ${MY_USER}

---

#!/bin/bash

# A supporting script designed to simplify the submission of existing scripts as ALIEN GRID jobs (using the following format):
#
# grid-submit --script my_script.sh --jobname jobname
#
# This script manages all the necessary interactions with the GRID automatically. Users do not need to create JDL files or manually upload them to the GRID.
#
# Additionally, the script can simulate local execution of the job. To do so, simply use:
#
# grid-submit --script my_script.sh --jobname jobname --local
#
# Currently, it only supports a basic JDL configuration. Further enhancements could include:
#
# -) allowing JDL customization through command line arguments or JDL tags within the script
#
# author: Sandro Wenzel

# set -o pipefail

function per() { printf "\033[31m$1\033[m\n" >&2; }
function pok() { printf "\033[32m$1\033[m\n" >&2; }
function banner() { echo ; echo ==================== $1 ==================== ; }

---

DOCUMENT:
    echo "Packages = {"${PACKAGESPEC}"};" >> "${MY_JOBNAMEDATE}.jdl"   # include package specification
  [ $ERROROUTPUTSPEC ] && echo "OutputErrorE = {"${ERROROUTPUTSPEC}"};" >> "${MY_JOBNAMEDATE}.jdl"   # specify error output files
  [ $IMAGESPEC ] && echo "DebugTag = {\"${IMAGESPEC}\"};" >> "${MY_JOBNAMEDATE}.jdl"   # use a specific singularity image for the job
  # echo "Requirements = {"${REQUIREMENTSSPEC}"} >> "${MY_JOBNAMEDATE}.jdl"
  [ "$REQUIRESPEC" ] && echo "Requirements = ${REQUIRESPEC}" >> "${MY_JOBNAMEDATE}.jdl"

---

DOCUMENT:
  # ---- Create JDL File ----------------
  # TODO: Make this configurable or read from a preamble section in the jobfile
  cat > "${MY_JOBNAMEDATE}.jdl" <<EOF
Executable = "${MY_BINDIR}/${MY_JOBNAMEDATE}.sh";
Arguments = "${CONTINUE_WORKDIR:+"-c ${CONTINUE_WORKDIR}"} --local ${O2TAG:+--o2tag ${O2TAG}} --ttl ${JOBTTL} --label ${JOBLABEL:-label} --prodsplit ${PRODSPLIT} ${MATTERMOSTHOOK:+--mattermost ${MATTERMOSTHOOK}} ${CONTROLSERVER:+--controlserver ${CONTROLSERVER}}";
InputFile = "LF:${MY_JOBWORKDIR}/alien_jobscript.sh";
${PRODSPLIT:+Split = ${QUOT}production:1-${PRODSPLIT}${QUOT};}
OutputDir = "${MY_JOBWORKDIR}/${PRODSPLIT:+#alien_counter_03i#}";
Requirements = member(other.GridPartitions,"${GRIDPARTITION:-multicore_8}");
CPUCores = "${CPUCORES}";
MemorySize = "60GB";
TTL=${JOBTTL};
EOF
  echo "Output = {"${OUTPUTSPEC:-\"logs*.zip@disk=1\",\"AO2D.root@disk=1\"}"};" >> "${MY_JOBNAMEDATE}.jdl"  # add output spec