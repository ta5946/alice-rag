## Metadata

**Document link:** https://github.com/AliceO2Group/O2DPG/blob/master/MC/bin/o2dpg_workflow_utils.py

**Start chunk id:** 19549a8411fdd1b78f385325827d0104204b1e7bf276ee76cd8b61f2d1430ae1

## Content

```python
Returns:
        dict: task dictionary
    """

    # dictionary holding global environment to be passed to task
    env_dict = {}

    if set_defaults:
        if environ.get('ALICEO2_CCDB_LOCALCACHE') is None:
            print ("ALICEO2_CCDB_LOCALCACHE is not set; defaulting to " + getcwd() + '/ccdb')
            env_dict['ALICEO2_CCDB_LOCALCACHE'] = getcwd() + "/ccdb"
        else:
        # ensures the workflow uses and remembers the externally provided path
            env_dict['ALICEO2_CCDB_LOCALCACHE'] = environ.get('ALICEO2_CCDB_LOCALCACHE')
        env_dict['IGNORE_VALIDITYCHECK_OF_CCDB_LOCALCACHE'] = '${ALICEO2_CCDB_LOCALCACHE:+"ON"}'

    if keys_values:
        # keys_values takes precedence in case of overlapping keys
        env_dict |= keys_values

    t = createTask(name = '__global_init_task__')
    t['cmd'] = 'NO-COMMAND'
    t['env'] = env_dict
    return t
```

---

CACHE[taskid] = result
    RETURN result

    # populates the matches_label dictionary
    FOR taskid IN RANGE(LEN(workflowspec['stages'])):
        IF (matches_or_inherits_label(taskid, from_stage, matches_label)):
            # now we make the final adjustment (as annotation) in the workflow itself
            IF workflowspec['stages'][taskid].get('disable_alternative_reco_software', False) != True:
                IF workflowspec['stages'][taskid].get("alternative_alienv_package") == None:
                    workflowspec['stages'][taskid]["alternative_alienv_package"] = package

---

```python
Args:
    collect_warnings: list
        store any warnings that occur
    collect_errors: list
        store any errors that occur
"""

is_sane = True
duplicates = []
for step in workflow:
    if step["name"] in duplicates:
        # This is an error because adding another task won't fix this issue
        collect_errors.append(f"A task named {step['name']} is already defined")
        is_sane = False
        continue
    duplicates.append(step["name"])
return is_sane


def validate_workflow(workflow):
    """Perform sanity checks on the workflow
    """

    warning_log = []
    error_log = []
    is_sane = validate_workflow_dependencies(workflow, warning_log, error_log) and validate_workflow_unique_names(workflow, warning_log, error_log)
```

---

```python
def check_workflow_unique_names(workflow, collect_warnings, collect_errors):
    """verify that all task names are unique

    Args:
        collect_warnings: list
            collect all warnings that might come up
        collect_errors: list
            collect all errors that might come up
    """

    is_sane = True
    task_names = []

    for task in workflow:
        if task["name"] in task_names:
            collect_warnings.append(f"WARNING: Task name '{task['name']}' is not unique")
            is_sane = False
        task_names.append(task["name"])

    return is_sane
```

---

# We aim to derive the stage from the path (which can be specified using '@' separation)
# For example, O2sim::daily-xxx@DIGI ---> apply this environment at the DIGI phase
# For example, O2sim::daily-xxx@RECO ---> apply this environment at the RECO phase
# If no '@' is present, O2sim::daily-xxx ---> apply this environment at the RECO phase (default setting)
from_stage = "RECO"
if package.count('@') == 1:
    package, from_stage = package.split('@')

# Essentially, we need to traverse the graph and apply the mapping
# Start by examining the workflow spec to check if the task or any child task is labeled RECO
# Typical graph traversal with caching is required

# Helper structures
taskuniverse = [ l['name'] for l in workflowspec['stages'] ]
tasktoid = {}
for i in range(len(taskuniverse)):
    tasktoid[taskuniverse[i]] = i

---

DOCUMENT:
    def update_workflow_resource_requirements(workflow, n_workers):
    """Modify resource requirements/settings
    """
    for s in workflow:
        if s["resources"]["relative_cpu"]:
            s["resources"]["cpu"] = relativeCPU(s["resources"]["relative_cpu"], n_workers)


def createTask(name='', needs=[], tf=-1, cwd='./', lab=[], cpu=1, relative_cpu=None, mem=500, n_workers=8):
    """Generates a new task. A task is a dictionary or class with common attributes like

    PARAPHRASED DOCUMENT:

---

DOCUMENT:
    def summary_workflow(workflow):
        print("=== WORKFLOW SUMMARY ===\n")
        print(f"-> The workflow consists of {len(workflow)} tasks")


    def dump_workflow(workflow, filename, meta=None):
        """Save this workflow to a file

        Args:
            workflow: list
                stages of the workflow
            filename: str
                name of the output file
        """

        # Perform sanity checks on the list of tasks
        check_workflow(workflow)
        taskwrapper_string = "${O2_ROOT}/share/scripts/jobutils2.sh; taskwrapper"
        # Prepare for saving, use deepcopy to ensure independence from the current instance
        to_dump = deepcopy(workflow)

---

for each item in to_dump:
    if the item contains a command and its name is not '__global_init_task__' and the taskwrapper_string is not in the command:
        # add taskwrapper if it's missing and the command is not empty
        item['cmd'] = '. ' + taskwrapper_string + ' ' + item['name']+'.log \'' + item['cmd'] + '\''
    # clean up whitespace for clarity
    item['cmd'] = trimString(item['cmd'])
# prepare the final structure for dumping
to_dump = {"stages": to_dump}
filename = make_workflow_filename(filename)
to_dump["meta"] = meta if meta else {}

with open(filename, 'w') as outfile:
    json.dump(to_dump, outfile, indent=2)

print(f"Workflow saved at {filename}")

def read_workflow(filename):
    workflow = None
    filename = make_workflow_filename(filename)
    with open(filename, "r") as wf_file:
        loaded = json.load(wf_file)
        workflow = loaded["stages"]
        meta = loaded.get("meta", {})
    return workflow, meta

---

#!/usr/bin/env python3

from os import environ, getcwd
from copy import deepcopy
import json


# List of active detectors
ACTIVE_DETECTORS = ["all"]
INACTIVE_DETECTORS = []

def activate_detector(det):
    try:
        # first of all, remove "all" if a specific detector is provided
        if "all" in ACTIVE_DETECTORS:
            ACTIVE_DETECTORS.remove("all")
    except ValueError:
        pass
    ACTIVE_DETECTORS.append(det)

def deactivate_detector(det):
    INACTIVE_DETECTORS.append(det)

def isActive(det):
    return det not in INACTIVE_DETECTORS and ("all" in ACTIVE_DETECTORS or det in ACTIVE_DETECTORS)

def compute_n_workers(interaction_rate, collision_system, n_workers_user=8, n_workers_min=1, interaction_rate_linear_below=300000):
    """
    Calculate the number of workers

    n_workers = m * IR + b

    based on
    https://indico.cern.ch/event/1395900/contributions/5868567/attachments/2823967/4932440/20240320_slides_cpu_eff.pdf, slide 3

    """

---

```python
print(f"=== There are {len(collect_warnings)} warnings ===")
for w in collect_warnings:
    print(w)
print(f"=== There are {len(collect_errors)} errors ===")
for e in collect_errors:
    print(e)

if is_sane:
    print("===> The workflow looks sane")
else:
    print("===> Please check warnings and errors!")

return is_sane

# Modifies the software version for RECO (and subsequent) stages
# (if desired). The function enacts a specific request from operations to manage varying sim and reco software versions, given their differing rates of development and bug fixes.
def adjust_RECO_environment(workflowspec, package = ""):
    if len(package) == 0:
       return
```

---

DOCUMENT:
    matches_label = {}
    # internal helper for recursive graph traversal
    def check_label_or_inheritance(taskid, label, cache):
        if cache.get(taskid) is not None:
            return cache[taskid]
        result = False
        if label in workflowspec['stages'][taskid]['labels']:
            result = True
        else:
            # check mother tasks
            for mothertask in workflowspec['stages'][taskid]['needs']:
                motherid = tasktoid[mothertask]
                if check_label_or_inheritance(motherid, label, cache):
                    result = True
                    break

        cache[taskid] = result
        return result

---

def merge_dicts(dict1, dict2):
    """
    combines dict2 into dict1 (with possible value overwrites)
    """
    for key, value in dict2.items():
        if key in dict1 and isinstance(dict1[key], dict) and isinstance(value, dict):
            # If both keys point to dictionaries, merge those recursively
            merge_dicts(dict1[key], value)
        else:
            # Otherwise, replace dict1's value with dict2's value
            dict1[key] = value

---

ASSUME n_workers_in=8 IS OPTIMAL FOR pp IR > interaction_rate_linear_below

BEGIN WITH 1 WORKER AT IR=0 AND INCREASE LINEARLY UNTIL interaction_rate_linear_below
"""
IF collision_system == "PbPb" OR interaction_rate >= interaction_rate_linear_below:
    RETURN n_workers_user

SET n_workers_min TO MAX(1, n_workers_min)
m = (n_workers_user - n_workers_min) / interaction_rate_linear_below
ENSURE AT LEAST 1 WORKER
RETURN MAX(1, ROUND(m * interaction_rate + n_workers_min))

def relativeCPU(n_rel, n_workers):
    # CALCULATE NUMBER OF CPUS BASED ON A GIVEN NUMBER OF WORKERS AND A FRACTION n_rel
    # HANDLE CASES WHERE n_rel > 1 OR n_workers * n_rel
    RETURN ROUND(MIN(n_workers, n_workers * n_rel), 2)

def trimString(cmd):
    # REMOVE UNNECESSARY SPACES
    RETURN ' '.JOIN(cmd.split())

def make_workflow_filename(filename):
    IF filename.lower().rfind(".json") < 0:
        # APPEND EXTENSION IF NOT PRESENT
        RETURN filename + ".json"
    RETURN filename

---

DOCUMENT:
    Returns:
        dict representing the task
    """
    if relative_cpu is not None:
        # Re-compute, if relative number of CPUs requested
        cpu = relativeCPU(relative_cpu, n_workers)
    return { 'name': name,
             'cmd':'',
             'needs': needs,
             'resources': { 'cpu': cpu, 'relative_cpu': relative_cpu , 'mem': mem },
             'timeframe' : tf,
             'labels' : lab,
             'cwd' : cwd }


def createGlobalInitTask(keys_values=None, set_defaults=True):
    """Returns a special task recognized by the executor as needing a global environment
       section applied to all tasks in a workflow.

    Args:
        keys_values: dict or None
            a dictionary of environment variables and their values to be globally applied to all tasks
            if there are overlapping keys with defaults, the values from keys_values take precedence
        set_defaults: bool
            whether to include default values or not

---

DOCUMENT:
    Args:
        name: str
            task identifier
        needs: list
            list of task names this task relies on
        tf: int
            associated timeframe
        cwd: str
            working directory for this task, which will be created automatically
        lab: list
            list of labels to be applied
        cpu: float
            average CPU usage required by this task
        relative_cpu: float or None
            if provided, cpu usage is recalculated based on the number of available workers
        mem: int
            memory size required by this task