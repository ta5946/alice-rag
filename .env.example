# TODO create separate config.yml file

LANGFUSE_PUBLIC_KEY="pk-lf-f5c27df0-2d07-435f-88f4-0c03b365f499"
LANGFUSE_SECRET_KEY="your_langfuse_secret_key"
LANGFUSE_HOST="http://pc-alice-ph01:3000"

# HF_LLM_REPO="MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF:Q6_K"
HF_LLM_REPO="MaziyarPanahi/Qwen2.5-7B-Instruct-GGUF:Q6_K"
# HF_LLM_REPO="MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF:Q6_K"
# HF_LLM_REPO="bartowski/gemma-2-9b-it-GGUF:Q4_K_M"
# HF_LLM_REPO="MaziyarPanahi/Qwen3-8B-GGUF:Q5_K_M"
# HF_LLM_REPO="MaziyarPanahi/Phi-4-mini-instruct-GGUF:Q8_0"
# HF_LLM_REPO="bartowski/OLMo-2-1124-7B-Instruct-GGUF:Q6_K"

HF_EMBEDDINGS_REPO="BAAI/bge-base-en-v1.5"
# HF_EMBEDDINGS_REPO="BAAI/bge-m3"

HF_RERANKER_REPO="BAAI/bge-reranker-base"
# HF_RERANKER_REPO="BAAI/bge-reranker-v2-m3"

HF_CACHE_DIR="./models/huggingface"
LLAMA_CPP_CACHE_DIR="./models/llama.cpp"
N_GPU_LAYERS=100
CTX_LENGTH=16000 # in tokens, 0 means taking it from the model config

LLM_BASE_URL="http://pc-alice-ph01:8080/v1"
LLM_API_KEY="any"
LLM_MAX_TOKENS=2000 # output token limit

CHROMA_DIR="./data/chroma"
CHROMA_COLLECTION_NAME="o2_docs"
CHROMA_CHUNK_SIZE=1000 # in characters, for text splitter

# low recall config
# CHROMA_TOP_K=10 # retriever stage
# CHROMA_TOP_N=5 # reranker stage
# CHROMA_THRESHOLD=0.25 # similarity threshold

# medium recall config
CHROMA_TOP_K=25
CHROMA_TOP_N=10
CHROMA_THRESHOLD=0.2

# high recall config
# CHROMA_TOP_K=40
# CHROMA_TOP_N=15
# CHROMA_THRESHOLD=0.15

INDEXER_RESOURCE_FILE="./src/indexer/knowledge_base.yml"
INDEXER_HASHES_FILE="./data/hashes.json"
INDEXER_DATA_DIR="./data/indexed"
INDEXER_BATCH_SIZE=1000

MATTERMOST_URL="mattermost.web.cern.ch"
MATTERMOST_TOKEN="bot_mattermost_token"
MATTERMOST_PORT=443
MATTERMOST_CHANNEL_ID="nkjjfj486i87zxr1jnqdtg6z3c"

GEMINI_MODEL="gemini-2.5-flash"
# GEMINI_MODEL="gemini-2.5-flash-lite"
GEMINI_API_KEY="your_gemini_api_key"
GEMINI_MAX_TOKENS=2000
